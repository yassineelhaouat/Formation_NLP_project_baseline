{"title": "The Agentic AI Handbook: Production-Ready Patterns", "url": "https://www.nibzard.com/agentic-handbook", "content": "The Agentic AI Handbook: Production-Ready Patterns. TheGitHub repositoryfor “Awesome Agentic Patterns” had been growing steadily since its launch. But around Christmas, the growth chart went vertical. In just a few days, the repository jumped from relative obscurity to nearly 2,500 stars. Thewebsitetraffic mirrored this spike. Something had clicked. But the real story wasn’t in the metrics—it was inwhowas talking about AI agents. Linus Torvalds, creator of Linux and Git, wrote about using AI coding agents for “vibe coding” and programming guitar pedal effects. Think about that for a second. The person who literally invented the version control system that powers modern software development was publicly embracing agents. Tobias Lütke, CEO of Shopify and already deep into agent-assisted development, declared it his “most productive time.” This from someone running one of the world’s largest e-commerce platforms. Perhaps most telling was Armin Ronacher, creator of Flask—one of the most respected voices in Python. He had been skeptical of coding agents, publicly raising concerns about their limitations. Then, seemingly overnight, his stance shifted. He started promoting agent-assisted workflows, documenting his learnings, and acknowledging that the technology had crossed a threshold. Here’s what all these stories have in common:the holidays gave people something that everyday life rarely provides—dedicated time. Learning to work effectively with AI agents isn’t something you pick up in five minutes between meetings. It requires: During the work year, these activities compete with deadlines, meetings, and the relentless pressure to ship. During the holidays, with meetings suspended and project urgency dialed down, developers finally had the bandwidth to actuallylearn. This repository, with its 113 patterns collected from real production systems, became the curriculum that accelerated that learning. Each pattern represented a battle-tested solution—something that worked outside the demo environment and in the messy reality of production code. Another phenomenon that exploded during the holidays was the “Ralph Wiggum coding loop”—named after the Simpsons character who means well but misses context. Asghuntley describes it, this describes the cycle where an agent starts working on something, seems productive, but gradually drifts off-course because it lacks the deeper context that a human would implicitly understand.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Anthropic's original take home assignment open sourced", "url": "https://github.com/anthropics/original_performance_takehome", "content": "Anthropic's original take home assignment open sourced", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "200 MB RAM FreeBSD Desktop", "url": "https://vermaden.wordpress.com/2026/01/18/200-mb-ram-freebsd-desktop/", "content": "200 MB RAM FreeBSD Desktop. Recently I came acrossLundukepost about some mysteriousVendefoul WolfLinux distribution that uses217 MB RAMwithDevuanas base (nosystemd(1)here) andXLibreX11 server along withIceWMwindow manager.  For the record – theLundukepost states200 MB RAMbutXLibreDevquotes a post where exactly217 MB RAMis reported. LaterLundukeeven posted a video about it.  As I use similarly low resource setup withOpenbox/Tint2/Dzen2setup (documentedFreeBSD Desktophere) I was wondering … how low can I go with FreeBSD RAM usage.  Lets try … TheTable of Contentsis as follows. I wanted to use most recent FreeBSD so I used15.0-RELEASEversion – including theTech PreviewPKGBASEsetup for FreeBSDBase System.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A 26,000-year astronomical monument hidden in plain sight (2019)", "url": "https://longnow.org/ideas/the-26000-year-astronomical-monument-hidden-in-plain-sight/", "content": "A 26,000-year astronomical monument hidden in plain sight (2019). The western flank of the Hoover Dam holds a celestial map that marks the time of the dam’s creation based on the 25,772-year axial precession of the earth. On the western flank of the Hoover Dam stands a little-understood monument, commissioned by the US Bureau of Reclamation when construction of the dam began in 01931. The most noticeable parts of this corner of the dam, now known as Monument Plaza, are the massive winged bronze sculptures and central flagpole which are often photographed by visitors. The most amazing feature of this plaza, however, is under their feet as they take those pictures. The plaza’s terrazzo floor is actually a celestial map that marks the time of the dam’s creation based on the 25,772-year axial precession of the earth. I was particularly interested in this monument because this axial precession is also the slowest cycle that we track in Long Now’s 10,000 Year Clock. Strangely, little to no documentation of this installation seemed to be available, except for a few vacation pictures on Flickr. So the last time I was in Las Vegas, I made a special trip out to Hoover Dam to see if I could learn more about this obscure 26,000-year monument. I parked my rental car on the Nevada side of the dam on a day pushing 100 degrees. I quickly found Monument Plaza just opposite the visitor center where tours of the dam are offered. While the plaza is easy to find, it stands apart from all the main tours and stories about the dam. With the exception of the writing in the plaza floor itself, the only information I could find came from a speaker running on loop, broadcasting a basic description of the monument while visitors walked around the area. When I asked my tour guide about it, he suggested that there may be some historical documentation and directed me to Emme Woodward, the dam’s historian. I was able to get in touch with her after returning home. As she sent me a few items, I began to see why the Bureau of Reclamation doesn’t explain very much about the monument’s background. The first thing she sent me was a description of the plaza by Oskar J. W. Hansen, the artist himself, which I thought would tell me everything I wanted to know. While parts of it were helpful, the artist’s statement of intention was also highly convoluted and opaque. An excerpt: It is pretty hard to imagine the US Bureau of Reclamation using this type of write-up to interpret the monument… and they don’t. And so there it stands, a 26,000-year clock of sorts, for all the world to see, and yet still mired in obscurity. While I may never totally understand the inner motivations of the monument’s designer, I did want to understand it on a technical level. How did Hansen create a celestial clock face frozen in time that we can interpret and understand as the date of the dam’s completion? The earth’s axial precession is a rather obscure piece of astronomy, and our understanding of it through history has been spotty at best. That this major engineering feat was celebrated through this monument to the axial precession still held great interest to me, and I wanted to understand it better. I pressed for more documentation, and the historian sent me instructions for using the Bureau of Reclamation’s image archive site as well as some keywords to search for. The black and white images you see here come from this resource. Using the convoluted web site was a challenge, and at first I had difficulty finding any photos of the plaza before or during its construction. As I discovered, the problem was that I was searching with the term “Monument Plaza,” a name only given to it after its completion in 01936. In order to find images during its construction, I had to search for “Safety Island,” so named because at the time of the dam’s construction, it was an island in the road where workers could stand behind a berm to protect themselves from the never-ending onslaught of cement trucks. I now had some historical text and photos, but I was still missing a complete diagram of the plaza that would allow me to really understand it. I contacted the historian again, and she obtained permission from her superiors to release the actual building plans. I suspect that they generally don’t like to release technical plans of the dam for security reasons, but it seems they deemed my request a low security risk as the monument is not part of the structure of the dam. The historian sent me a tube full of large blueprints and a CD of the same prints already scanned. With this in hand I was finally able to re-construct the technical intent of the plaza and how it works.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "cURL removes bug bounties", "url": "https://etn.se/index.php/nyheter/72808-curl-removes-bug-bounties.html", "content": "cURL removes bug bounties. Open source code library cURL is removing the possibility to earn money by reporting bugs, hoping that this will reduce the volume of AI slop reports. Joshua Rogers â€“ AI wielding bug hunter of fame â€“ thinks it's a great idea. cURL has been flooded with AI-generated error reports. Now one of the incentives to create them will go away. The vast majority of AI-generated error reports submitted to cURL are pure nonsense. Other open source projects are caught in the same pandemic. cURL maintainer Daniel Stenberg made an impact with his reporting on AI-generated bug reports last year â€“ â€ťDeath by a thousand slops.â€ť Determining that they are nonsense is time-consuming, causing the maintainers lots of extra work. â€ťAI slop and bad reports in general have been increasing even more lately, so we have to try to brake the flood in order not to drownâ€ť, says cURL maintainer Daniel Stenberg to Swedish electronics industry news site etn.se. Therefore, cURL is terminating the bounty payouts as of the end of January. â€śWe hope this removes some of the incentives for people to send us garbage. We spend far too much time handling slop due to findings that are not real, exaggerated, or misunderstood.â€ť Not all AI-generated bug reports are nonsense. Itâ€™s not possible to determine the exact share, but Daniel Stenberg knows of more than a hundred good AI assisted reports that led to corrections. In total, 87 bug reports to cURL have over the years amounted to USD 101,020 in bounties.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Libbbf: Bound Book Format, A high-performance container for comics and manga", "url": "https://github.com/ef1500/libbbf", "content": "Libbbf: Bound Book Format, A high-performance container for comics and manga", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The challenges of soft delete", "url": "https://atlas9.dev/blog/soft-delete.html", "content": "The challenges of soft delete. Software projects often implement \"soft delete\", maybe with adeletedboolean or anarchived_attimestamp column.\nIf customers accidentally delete their data, they can recover it, which makes work easier for customer support teams.\nPerhaps archived records are even required for compliance or audit reasons. I've run into some trouble with soft delete designs. I'll cover those, and ponder ideas for how I'd build this in the future. Adding anarchived_atcolumn seems to ooze complexity out into queries, operations, and applications.\nRecovering deleted records does happen, but 99% of archived records are never going to be read. So, the database tables will have a lot of dead data. Depending on access patterns, that might even be a significant amount of data.\nI've seen APIs that didn't work well with Terraform, so Terraform would delete + recreate records on every run, and over time that led\nto millions of dead rows. Your database can probably handle the extra bytes, and storage is fairly cheap, so it's not necessarily a problem, at first. Hopefully, the project decided on a retention period in the beginning, and set up a periodic job to clean up those rows.\nUnfortunately, I'd bet that a significant percentage of projects did neither – it's really easy to ignore the archived data for a long time. At some point, someone might want to restore a database backup. Hopefully that's for fun and profit and not because you lost the production database at 11 am.\nIf your project is popular, you might have a giant database full of dead data that takes a long time to recreate from a dump file. archived_atcolumns also complicate queries, operations, and application code. Applications need to make sure they always avoid the archived data that's sitting\nright next to the live data. Indexes need to be careful to avoid archived rows. Manual queries run for debugging or analytics are longer and more complicated.\nThere's always a risk that archived data accidentally leaks in when it's not wanted. The complexity grows when there are mapping tables involved. Migrations have to deal with archived data too. Migrations may involve more than just schema changes – perhaps you need to fix a mistake with default values, or add a new column and backfill values.\nIs that going to work on records from 2 years ago? I've done migrations where these questions were not trivial to answer. Restoring an archived record is not always as simple as just runningSET archived_at = null– creating a record may involve making calls to external systems as well.\nI've seen complex restoration code that was always a buggy, partial implementation of the \"create\" API endpoint. In the end, we removed the specialized restoration code\nand required all restoration to go through the standard APIs – that simplified the server implementation, and ensured that old data that had since become invalid, could not\nbe restored incorrectly – it needs to pass the new validation rules. I'm not a fan of thearchived_atcolumn approach. It's simple at first, but in my experience, it's full of pitfalls down the line.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "RSS.Social – the latest and best from small sites across the web", "url": "https://rss.social/", "content": "RSS.Social – the latest and best from small sites across the web. The latest and best from small sites across the web.Learn how it works.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Instabridge has acquired Nova Launcher", "url": "https://novalauncher.com/nova-is-here-to-stay", "content": "Instabridge has acquired Nova Launcher. Hi everyone. We want to share a clear update directly with the Nova community. Instabridge has acquired Nova Launcher. We are a Swedish company building products that help people get online, used by millions of people worldwide. Nova is not shutting down. Our immediate focus is simple: keep Nova stable, compatible with modern Android, and actively maintained. We also know many of you have lived through a long period of uncertainty. Nova has a strong identity and a community that still cares deeply. We take that seriously. Our job is not to reinvent Nova overnight. Our job is to be responsible owners. That means: We will be reading and collecting feedback from Reddit, Play Store reviews, email, and other community channels. We will not be able to respond to every post, but we will be paying attention. For support related issues, we will share a clear contact channel shortly. We have long admired what Nova represents: speed, customization, and user control. When we saw how much the community still cares, it was clear to us that Nova deserved a stable future with active maintenance. Yes. Nova’s identity is the point. Performance, flexibility, and user control stay at the center of the product. Any future changes will be evaluated through that lens. Nova needs a sustainable business model to support ongoing development and maintenance. We are exploring different options, including paid tiers and other approaches. As many of you have already anticipated, we are also evaluating ad based options for the free version.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Mastra 1.0, open-source JavaScript agent framework from the Gatsby devs", "url": "https://github.com/mastra-ai/mastra", "content": "Show HN: Mastra 1.0, open-source JavaScript agent framework from the Gatsby devs", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Infracost (YC W21) Is Hiring Sr Back End Eng (Node.js+SQL) to Shift FinOps Left", "url": "https://www.ycombinator.com/companies/infracost/jobs/Sr9rmHs-senior-backend-engineer-node-js-sql", "content": "Infracost (YC W21) Is Hiring Sr Back End Eng (Node.js+SQL) to Shift FinOps Left", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Which AI Lies Best? A game theory classic designed by John Nash", "url": "https://so-long-sucker.vercel.app/", "content": "Which AI Lies Best? A game theory classic designed by John Nash. A game theory classic designed byJohn Nashthatrequiresbetrayal to win. Now a benchmark\n                    for AI deception. A benchmark that tests what most benchmarks can't:\n                    deception, negotiation, and trust. So Long Suckerwas designed in 1950\n                            by four game theorists includingJohn Nash(of \"A Beautiful Mind\" fame). The\n                            game has one brutal property:betrayal is required to win. This lets us test AI capabilities that standard benchmarks miss: 4 players, each with colored chips. Take turns\n                                playing chips on piles. If your chip matches the\n                                one below it, you capture the pile. Run out of\n                                chips? Beg others for help — or get eliminated.\n                                Last player standing wins. Each AI developed its own personality. Here's who they\n                    became. Win ratesinvertas game complexity increases. Manipulation becomes more effective as game\n                                length increases. Gaslighting tactics need time to work. Reactive play dominates simple games but\n                                collapses under complexity. No internal\n                                reasoning means no long-term planning. We can see their private thoughts. They don't match what they say.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "California is free of drought for the first time in 25 years", "url": "https://www.latimes.com/california/story/2026-01-09/california-has-no-areas-of-dryness-first-time-in-25-years", "content": "California is free of drought for the first time in 25 years. This is read by an automated voice. Please report any issues or inconsistencieshere. After experiencing one of thewettest holiday seasonson record, still soggy California hit a major milestone this week — having zero areas of abnormal dryness for the first time in 25 years. The data, collected by theU.S. Drought Monitor, is a welcome nugget of news for Golden State residents, who in the last 15 years alone have lived through two of the worst droughts on record, the worst wildfire seasons on record and the most destructive wildfires ever. Right now, the wildfire risk across California is “about as close to zero as it ever gets,” and there is likely no need to worry about the state’s water supply for the rest of the year, said UC climate scientist Daniel Swain. Currently, 14 of the state’s 17 major water supply reservoirs are at 70% or more capacity, according to theCalifornia Department of Water Resources.      ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The GDB JIT Interface", "url": "https://bernsteinbear.com/blog/gdb-jit/", "content": "The GDB JIT Interface. GDB is great for stepping through machine code to figure out what is going on.\nIt uses debug information under the hood to present you with a tidy backtrace\nand also determine how much machine code to print when you typedisassemble. This debug information comes from your compiler. Clang, GCC, rustc, etc all\nproduce debug data in a format calledDWARFand then embed that debug\ninformation inside the binary (ELF, Mach-O, …) when you do-ggdbor\nequivalent. Unfortunately, this means that by default, GDB has no idea what is going on if\nyou break in a JIT-compiled function. You can step instruction-by-instruction\nand whatnot, but that’s about it. This is because the current instruction\npointer is nowhere to be found in any of the existing debug info tables from\nthe host runtime code, so your terminal is filled with???. See this example\nfrom the V8 docs: Fortunately, there is aJIT interfaceto GDB. If you implement a couple of\nfunctions in your JIT and run them every time you finish compiling a function,\nyou can get the debugging niceties for your JIT code too. See again a V8\nexample: Unfortunately, the GDB docs aresomewhat sparse. So I went\nspelunking through a bunch of different projects to try and understand what is\ngoing on. GDB expects your runtime to expose a function called__jit_debug_register_codeand a global variable called__jit_debug_descriptor. GDB automatically adds its own internal breakpoints\nat this function, if it exists. Then, when you compile code, you call this\nfunction from your runtime. In slightly more detail: This is why you see compiler projects such as V8 including large swaths of code\njust to make object files: Because this is a huge hassle, GDB also has a newer interface that does not\nrequire making an ELF/Mach-O/…+DWARF object. This new interface requires writing a binary format of your choice. You make\nthe writer and you make the reader. Then, when you are in GDB, you load your\nreader as a shared object.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IPv6 is not insecure because it lacks a NAT", "url": "https://www.johnmaguire.me/blog/ipv6-is-not-insecure-because-it-lacks-nat/", "content": "IPv6 is not insecure because it lacks a NAT. I recently saw a discussion where someone argued that IPv4 is more secure than IPv6 because “the NAT-by-default of IPv4 effectively means that I get the benefit of a default-deny security strategy.” This is a common misconception that I think is worth addressing. The fundamental issue here is conflating NAT (Network Address Translation) with security. NAT isn’t actually a security feature—it’s an address conservation mechanism that became necessary because we ran out of IPv4 addresses. (Although it is totally possible to use a NAT with IPv6 too!) NAT allows multiple devices on a home network to share a single IP address on the public Internet by rewriting the destination IP of a packet based on its destination port. It chooses a new destination IP based on the “port mappings” or “port forwards” configured by the network admin. The consequence of this is that when receiving inbound traffic to a NAT’d IP, packets with an unexpected destination port (one which has not been forwarded) will keep the destination IP of the public machine and will not be routed to another machine on the network. But the security benefits people attribute to NATactuallycome from the stateful firewall that’s typically bundled with NAT routers. Modern routers ship with firewall policies that deny inbound traffic by default, even when a NAT is not being used. The firewall will drop packets with an unexpected destination before even considering whether to rewrite or route the packets. For example, UniFi routers ship with these default IPv6 firewall rules: Therefore, in order to allow unsolicited inbound traffic to any IPv6 device hosted behind the router, you must explicitly add a firewall rule to allow the traffic, whether using a NAT or not.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Unix Pipe Card Game", "url": "https://punkx.org/unix-pipe-game/", "content": "The Unix Pipe Card Game. Programming Time, which is a game to teach python and some more fundamental algorithms, from hash tables to RSA The C Pointer Game - Pointers, Arrays and Strings, a game to teach kids to look at the computer memory and understand references and values 4917, a game to teach kids machine code and how the CPU works with memory and registers The Unix Pipes Game - Process Substitution, an expansion of the Unix Pipes Game to teach process substitution and also:paste, tr, cut, bc RunLength Encoding for Kids, small cards \"game\" to explain runlength encoding PUNK0 - The Function Composition Card Game, use cards to manipulate a list and use its values to win the game PROJEKT: OVERFLOW, RISCV assembler boardgame Programming for kids, a log of my journey of teaching my daughter how to code", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Unconventional PostgreSQL Optimizations", "url": "https://hakibenita.com/postgresql-unconventional-optimizations", "content": "Unconventional PostgreSQL Optimizations. When it comes to database optimization, developers often reach for the same old tools: rewrite the query slightly differently, slap an index on a column, denormalize, analyze, vacuum, cluster, repeat. Conventional techniques are effective, but sometimes being creative can really pay off! In this article, I present unconventional optimization techniques in PostgreSQL. Table of Contents  Imagine you have this table of users: For each user you keep their name and which payment plan they're on. There are only two plans, \"free\" and \"pro\", so you add a check constraint. Generate some data and analyze the table: You now have 100K users in the system. Now you want to let your analysts access this table in their reporting tool of choice. You give one of the analysts permission, and this is the first query they write: The query returned no results, and the analyst is baffled. How come there are no users on the \"Pro\" plan?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Are arrays functions?", "url": "https://futhark-lang.org/blog/2026-01-16-are-arrays-functions.html", "content": "Are arrays functions?. When I was a youngster first perusing theHaskell documentation for\narrays,\nI was amused to find the following description of just what these mysterious\nthings might be: Haskell provides indexable arrays, which may be thought of as functions whose\ndomains are isomorphic to contiguous subsets of the integers. I found this to be a hilariously obtuse and unnecessarily formalist description\nof a common data structure. Now, older, wiser, and well ensconced in the ivory\ntowers of academia, I look at this description and think that it is actually a\nwonderful definition of the essence of arrays! And given that this sentence\nstill lingers in my thoughts so many years later, who can say that it is not\nactually a far better piece of documentation than some more prosaic description\nmight have been? To a language designer, the correspondence between arrays and functions (for itdoesexist, independent of whether you think it is a useful way to document\nthem) is alluring, for one of the best ways to improve a language is to make it\nsmaller. Our goal is not to unify therepresentationof arrays and functions,\nof course - nobody would seriously claim that representing an array via someChurch-encodingis a good idea\nin a supposedly practical programming language. Instead, what might be\nworthwhile considering is what consequences might arise from unifying arrays and\nfunctions at the syntax or type level, and why Futhark ultimately has not done\nso. There is some prior work to consider. The array languageKhas a syntactic unification of\narrays and functions, as both are indexed/applied with the notationf[x]. This\nis however pretty much where the correspondence stops. As an APL derivative, K\nprogramming is based on bulk operations on entire arrays, rather than\nelement-at-a-time programming, and the operators that perform these bulk\noperations cannot be applied to functions. And of course, Khas no type\nsystem,\nso the correspondence is purely syntactic. Dexis a research languagepreviously covered on this channel, which\nalso leverages the array-function correspondence, although mostly at the\nconceptual level such that theyfeelsimilar. As a starting point, a function\nfromatobin Dex uses the conventional notationa -> b, while an array\nwith index typeaand element typebis writtena => b. It is required\nthatais a type that is isomorphic to a contiguous subset of the integers,\nand hence an array type in Dex can really be thought of as a precomputed and\nefficiently represented function. Anonymous functions are written as\\x->e,\nwhile arrays are constructed asfor x.e. Arrays are transformed using a\n“pointed” style, using explicit indexing, similar to how functions are defined\nwith named parameters that are then passed to other functions. Many of the common function operations have a nice interpretation for arrays as\nwell. For example, currying/uncurrying is equivalent to unflattening/flattening\nan array - consider how currying(a,b) -> ctoa -> b -> creally is the\nsame as going from(a,b) => ctoa => b => c. Partial application is like\nfixing a dimension. Flipping the parameters of a function is like transposing an\narray. Composition is like applying a permutation array to another. It is very\ninteresting to me how this line of thinking encourages recognising common\npatterns and interpreting them differently. It is particularly interesting\nbecause arrays and functions fundamentally are completely different types in\nDex, with few facilities provided for using them via a common abstraction (e.g.\nthere is notransposefunction that works for both), but merely through\nsuggestive syntax andfeelis the programmer encouraged to think in different\nways. Now let us consider to which extent a unification of arrays and functions might\nbe viable in Futhark. First, there is no hope of unification at the type level.\nTo allow for efficientdefunctionalisation, Futhark\nimposes restrictions on how functions can be used; for example banning returning\nthem from branches. These restrictions are not (and ought not be!) imposed on\narrays, and so unification is not possible. Also, in Futhark an array type such\nas[n]f64explicitly indicates its size (and consequently the valid indices),\nwhich caneven be extracted at\nrun time. This is not possible with\nfunctions, and making it possible requires us to move further towards dependent\ntypes - which may of course be a good idea anyway. On to syntax. It would be no great challenge to replacea[i]witha i. While\nit looks strange to me, any sort of change to notation looks strange initially,\nand so it is not something I will dwell on overmuch. The main challenge is\nactuallyslicing. Futhark supports a fairly conventional Python-like notation\nfor array slices, namelya[i:j]. This does not have such a simple\ncorrespondence with function application syntax. One solution would be to allow\nthe application of an array to an entireindex array, rather than just a\nsingle index, producing an array of the same shape. That is, the applicationa [i, j, k]would be equivalent to[a[i], a[j], a[k]]. Since Futhark already\nhas a decent notation for constructing ranges, this would allow the slicea[i:k]to be writtena(i..<k)(the parentheses solely for precedence\nreasons). Of course, this could also be allowed using the existing bracket\nsyntax, merely by allowinga[i]whereiis an array. The operational guarantees are a little trickier to wrangle. Currently, slicing\nis guaranteed to be an essentially free operation that merely fiddles with thearray metadata. However, this cannot be\nguaranteed when slicing with an arbitrary indexing array, as there may be no\nexpressible pattern to the indices, which may contain duplicates, arbitrary\nholes, etc - in fact, it fully generalises filtering and expansion. The compiler\nwould have to put in significant work to detect and exploit the efficient cases\ncorresponding to standard slices, and such reverse-engineering of programmer\nintent isantithetical to the Futhark\nphilosophy. We would much\nrather exploit what the programmer has actuallystated, rather than try to\nread between the lines for what they mightmean.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Director Gore Verbinski: Unreal Engine is the greatest slip backwards for movie", "url": "https://www.pcgamer.com/movies-tv/director-gore-verbinski-says-unreal-engine-is-the-greatest-slip-backwards-for-movie-cgi/", "content": "Director Gore Verbinski: Unreal Engine is the greatest slip backwards for movie. Remember the glory days of CGI in movies? Terminator 2's liquid metal T-1000, Jurassic Park's stunning dinosaurs, Starship Trooper's swarms of giant arachnids. Not only did the CGI look great then, most of the visual effects in those movies still hold up well today, even decades after they were created. Nowadays, movie fans seem much less impressed by CGI in films. There's a general distaste for a perceived overuse of CGI in favor of practical effects, and there are a lot of complaints that recent CGI is less-convincing and more fake-looking than it used to be, even in the biggest budget films. In an interview withBut Why Tho?, Gore Verbinski, director of The Ring, Rango, and the first three Pirates of the Caribbean films, was asked why visual effects in movies just don't look as good as they used to. \"I think the simplest answer is you’ve seen the Unreal gaming engine enter the visual effects landscape,\" Verbinski said. \"So it used to be a divide, with Unreal Engine being very good at video games, but then people started thinking maybe movies can also use Unreal for finished visual effects. So you have this sort of gaming aesthetic entering the world of cinema.\" Unreal Engine made waves after being used for virtual sets in production of The Mandalorian TV series back in 2020, and usage of the engine has grown more widespread in films over the past few years, such as in The Matrix Resurrections and Ant-Man and the Wasp: Quantumania. That's not good news, according to Verbinski. \"I think that Unreal Engine coming in and replacing Maya as a sort of fundamental is the greatest slip backwards,\" he said. He pointed out the types of visual effects made with Unreal aren't necessarily bad. \"It works with Marvel movies where you kind of know you’re in a heightened, unrealistic reality. I think it doesn’t work from a strictly photo-real standpoint,\" he said. Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. \"I just don’t think it takes light the same way; I don’t think it fundamentally reacts to subsurface, scattering, and how light hits skin and reflects in the same way,\" he said. \"So that’s how you get this uncanny valley when you come to creature animation, a lot of in-betweening is done for speed instead of being done by hand.\" In his new movie, science fiction comedy Good Luck, Have Fun, Don't Die, which will be released in theaters in February, Verbinski says he uses CGI, but \"we try to be really strict with making at least 50% of the frame photographic. I think that keeps you honest. You can use props as a reference, and when you see the CG replacement, you know how to replicate the real thing,\" he said.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The space and motion of communicating agents (2008) [pdf]", "url": "https://www.cl.cam.ac.uk/archive/rm135/Bigraphs-draft.pdf", "content": "The space and motion of communicating agents (2008) [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Lunar Radio Telescope to Unlock Cosmic Mysteries", "url": "https://spectrum.ieee.org/lunar-radio-telescope", "content": "Lunar Radio Telescope to Unlock Cosmic Mysteries. The catch: It will have to be on the moon Astronomer Jack Burns has spent four decades working to place a radio telescope on the moon. The first one is finally scheduled to launch in early 2027. Isolation dictates where wego to see into the far reaches of the universe. TheAtacama Desertof Chile, the summit ofMauna Keain Hawaii, the vast expanse of theAustralian Outback—these are where astronomers and engineers have built the great observatories and radio telescopes of modern times. The skies are usually clear, the air is arid, and the electronic din of civilization is far away. It was to one of these places, in the high desert of New Mexico, that a young astronomer namedJack Burnswent to study radio jets and quasars far beyond the Milky Way. It was 1979, he was just out of grad school, and theVery Large Array, a constellation of 28 giant dish antennas on an open plain, was a new mecca of radio astronomy. But the VLA had its limitations—namely, that Earth’s protective atmosphere and ionosphere blocked many parts of the electromagnetic spectrum, and that, even in a remote desert, earthly interference was never completely gone. Could there be a better, even lonelier place to put a radio telescope? Sure, a NASA planetary scientist namedWendell Mendell, told Burns: How about the moon? He asked if Burns had ever thought about building one there. “My immediate reaction was no. Maybe even hell, no. Why would I want to do that?” Burns recalls with a self-deprecating smile. His work at the VLA had gone well, he was fascinated by cosmology’s big questions, and he didn’t want to be slowed by the bureaucratic slog of getting funding to launch a new piece of hardware. But Mendell suggested he do some research and speak at a conference on future lunar observatories, and Burns’s thinking about a space-based radio telescope began to shift. That was in 1984. In the four decades since, he’s published more than500 peer-reviewed paperson radio astronomy. He’s been anadvisertoNASA, the Department of Energy, and the White House, as well as a professor and a university administrator. And while doing all that, Burns has had an ongoing second job of sorts, as a quietly persistent advocate for radio astronomy from space. And early next year, if all goes well, a radio telescope for which he’s a scientific investigator will be launched—not just into space, not just to the moon, but to the moon’s far side, where it will observe things invisible from Earth. “You can see we don’t lack for ambition after all these years,” says Burns, now 73 and a professor emeritus ofastrophysicsatthe University of Colorado Boulder.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Our approach to age prediction", "url": "https://openai.com/index/our-approach-to-age-prediction/", "content": "Our approach to age prediction", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Do you have any evidence that agentic coding works?", "url": "item?id=46691243", "content": "Ask HN: Do you have any evidence that agentic coding works?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Maintenance: Of Everything, Part One", "url": "https://press.stripe.com/maintenance-part-one", "content": "Maintenance: Of Everything, Part One. The first in a multi-volume work,Maintenance: Of Everything, Part Oneoffers a comprehensive overview of the civilizational importance of maintenance. The book explores the insights that can be gleaned from the maintenance of sailboats, vehicles, and weapons, with absorbing detours into the evolution of precision in manufacturing, the enduring importance of manuals, sustainment in the military, and the never-ending battle against corrosion.Maintenance: Of Everythingis a wide-ranging and provocative call to expand what we mean by “maintenance.” It invites us to understand not only the profound impact maintenance has on our daily lives but also why taking responsibility for maintaining something—whether a motorcycle, a monument, or our planet—can be a radical act. Stewart Brand is the cofounder and president of The Long Now Foundation. He created and edited the National Book Award-winningWhole Earth Catalogfrom 1968 to 1998. His books includeThe Media Lab(1987),How Buildings Learn(1994),The Clock of the Long Now(1999), andWhole Earth Discipline(2009). He was the subject of the documentaryWe Are As Gods(2020). Stewart Brand makes a persuasive case that keeping the human show on the road through well-planned maintenance is as vital and as fascinating a task as innovation and discovery themselves. A deliciously good book. Matt Ridley author ofThe Rational Optimist Once again, Stewart Brand reframes our worldview with a new perspective. You may not imagine you would be interested in rust, Soviet tanks, or tricked-out Model Ts—that is, until Brand reexamines them through the lens of maintenance.Maintenance: Of Everythingis destined to be a classic. Danny Hillis cofounder of Applied Invention Stewart Brand is back with a manifesto on maintenance, the tool that empowers all tools. Preventative maintenance, deferred maintenance, and emergency maintenance: this much-needed, no-nonsense treatise illuminates the difference, and why it counts. George Dyson", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Building Robust Helm Charts", "url": "https://www.willmunn.xyz/devops/helm/kubernetes/2026/01/17/building-robust-helm-charts.html", "content": "Building Robust Helm Charts. In my current work, there is often the need to deploy a similar application\nstack in various configurations, to several environments. Each configuration may\nvary in terms of scale, uptime requirements and feature flagging. Due to a lot\nof flux in infrastructure set up, each environment is also not equivalent. On\ntop of this, there are obviously financial requirements to run all of this as\ncheaply as possible. Kubernetes and helm templating are valuable tools in this\nsituation, they allow us to create a configuration blueprint with the details\nabstracted invalues.yamlfiles. Let’s start with the basics, helm provides ahelm lintcommand which performs\nchecks You can run this with your different values.yaml files to ensure that all your\nconfigurations are compliant. It’s also a good idea to use thehelm templatecommand to actually check that\nhelm is able to render your templates. I like to compare helm templating with html templating tools like JSX. This\nallows front end developers to create reusable components usable throughout\npages of a web application, A button component for example can have many states,\nprimary, secondary, loading, disabled, light or dark mode. Each state may also look different depending on the size/type of device your are\nbrowsing the site with. Each of these states represents differences in many\nparameters (font size, colour, gradient, opacity, border, padding, margin,\nwidth, height, etc). These complexities are abstracted away giving the consuming\ncode the list of states to chose from, so that they can write code like this. Under the hood of course many aspects of the CSS or HTML code will be impacted\nby the change of state so you often end up with different parts of the markup\nhaving conditionals on the same check. Just in this contrived example you already have 2 different things being\ncontrolled by the state property with 2 separate checks, the CSS classes and the\npresence of the loading icon. This is quite similar to the situation you end up templating in YAML with helm.\nConsider an application that has optional persistent storage. You could quite\neasily imagine a boolean property in yourvalues.yamlfile calledpersistent. Under the hood this has many implications likely affecting\ndifferent files. That’s 5 separateifblocks that need to be in your templates.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Proof of Concept to Test Humanoid Robots", "url": "https://thehumanoid.ai/humanoid-and-siemens-completed-a-proof-of-concept-to-test-humanoidrobots-in-industrial-logistics/", "content": "Proof of Concept to Test Humanoid Robots.  London, The UK—January 15, 2026— Humanoid, a UK-based AI and robotics company, and Siemens, a leading technology company, have successfully completed a proof of concept (POC) demonstrating the use of humanoid robots in industrial logistics. Humanoid’s HMND 01 wheeled Alpha robot was deployed in real operations at a Siemens facility, marking a significant step toward the deployment of humanoid robots in industrial settings. This successful POC is the first step in a broader partnership between the two companies to test and validate how humanoid robots can be used in real-world environments. The POC focused on a tote-to-conveyor destacking task within Siemens’ logistics process. In this use case, the robot autonomously picked totes from a storage stack, transported them to a conveyor, and placed them at the designated pickup point for human operators. This sequence was repeated until the stack was fully empty, which demonstrated how humanoid robots can take on repetitive logistics tasks.  The POC was structured in two phases. The first one focused on in-house development and demonstration and has already been completed. During this stage, the Humanoid team built a physical twin to support testing, optimization, and rapid iteration throughout the POC. The second phase involved a two-week on-site deployment at the Siemens Electronics Factory in Erlangen, where partners assessed the robots in a real-world production environment. This joint POC measured both performance and reliability of humanoid robots under autonomous operation. Target metrics were met in full and included a throughput of 60 tote moves per hour, operation with two different tote sizes, continuous autonomous task execution for more than 30 minutes, uptime exceeding 8 hours. The team also evaluated the POC’s success using the following indicators: overall pick and place success rate and autonomous pick and place success rate, both above 90%. Humanoid and Siemens see this POC as a first step toward a long-term strategic collaboration — beyond the initial POC, the companies are open to expanding the scope and adding additional use cases. Partners also may progress toward a broader rollout, deploying a greater number of humanoid robots across Siemens’ facilities, based on the robot’s specific skill set. “At Humanoid, we are a commercially driven company. Our focus is on creating robots that deliver measurable value in real-world settings. Working closely with industrial and technology partners allow us to validate our systems against real operational requirements and understand which use cases matter outside the lab. This joint POC with Siemens showed clear potential for practical deployment of humanoid robots. We see them move steadily toward the real world, and partnerships like this one help accelerate that transition,”noted Artem Sokolov, founder and CEO of Humanoid. Stephan Schlauss, Global Head of Manufacturing Motion Control, Siemens AG, said: “As Siemens’ customer zero, the Electronics Factory Erlangen is excited to partner with the Humanoid team. We’re tackling production automation, discovering new opportunities for Siemens, and are eager to advance this promising technology across our factory network to deliver customer value.” About Humanoid", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Agent Skills Leaderboard", "url": "https://skills.sh", "content": "Show HN: Agent Skills Leaderboard. The Open Agent Skills Ecosystem Skills are reusable capabilities for AI agents. Install them with a single command to enhance your agents with access to procedural knowledge. vercel-labs/agent-skills vercel-labs/agent-skills expo/skills remotion-dev/skills expo/skills expo/skills expo/skills expo/skills", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Who owns Rudolph's nose?", "url": "https://creativelawcenter.com/copyright-rudolph-reindeer/", "content": "Who owns Rudolph's nose?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Provably unmasking malicious behavior through execution traces", "url": "https://arxiv.org/abs/2512.13821", "content": "Provably unmasking malicious behavior through execution traces. arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community?Learn more about arXivLabs. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IP Addresses Through 2025", "url": "https://www.potaroo.net/ispcol/2026-01/addr2025.html", "content": "IP Addresses Through 2025. IP Addresses through 2025January 2026 It's time for another annual roundup from the world of IP addresses. Let’s see what has changed in the past 12 months in addressing the Internet and look at how IP address allocation information can inform us of the changing nature of the network itself. Back around 1992, the IETF gazed into their crystal ball and tried to understand how the Internet was going to evolve and what demands that would place on the addressing system as part of the “IP Next Generation” study.  The staggeringly large numbers of connected devices that we see today were certainly within the range predicted by that study. The assumption made at the time was that we would continue to use much the same IP protocol architecture, including the requirement that each connected device was assigned a unique IP address, and the implication was that the 32-bit address field defined in version 4 of the IP protocol was clearly going to be inadequate to cope with the predicted number of connected devices. A span of 4 billion address values was just not large enough. We concluded at the time that the only way we could make the Internet work across such a massive pool of connected devices was to deploy a new IP protocol that came with a massively larger address space. It was from this reasoning that IPv6 was designed, as this world of abundant silicon processors connected to a single public Internet was the scenario that IPv6 was primarily intended to solve. The copious volumes of a 128-bit address space were intended to allow us to uniquely assign a public IPv6 address to every such device, no matter how small, or in whatever volume they might be deployed. But while the Internet has grown at amazing speeds across the ensuing 33 years, the deployment of IPv6 has proceeded at a more measured pace. There is still no evidence of any common sense of urgency about the deployment of IPv6 in the public Internet, and still there is no common agreement that the continued reliance on IPv4 is failing us. Much of the reason for this apparent contradiction between the addressed device population of the IPv4 Internet and the actual count of connected devices, which is of course many times larger, is that through the 1990's the Internet rapidly changed from a peer-to-peer architecture to a client/server framework. Clients can initiate network transactions with servers but are incapable of initiating transactions with other clients. Servers are capable of completing connection requests from clients, but cannot initiate such connections with clients. Network Address Translators (NATs) are a natural fit to this client/server model, where pools of clients share a smaller pool of public addresses, and only require the use of an address once they have initiated an active session with a remote server. NATs are the reason why a pool of excess of 30 billion connected devices can be squeezed into a far smaller pool of some 3 billion advertised IPv4 addresses. Services and Applications that cannot work behind NATs are no longer useful in the context of the public Internet and no longer used as a result. In essence, what we did was to drop the notion that an IP address is uniquely associated with a device's identity, and the resultant ability to share addresses across clients largely alleviated the immediacy of the IPv4 addressing problem for the Internet. However, the pressures of this inexorable growth in the number of deployed devices connected to the Internet implies that the even NATs cannot absorb these growth pressures forever. NATs can extend the effective addressable space in IPv4 by up to 32 ‘extra’ bits using mapping of the 16-bit source and destination port fields of the TCP and UDP headers, and they also enable the time-based sharing of these public addresses. Both of these measures are effective in stretching the IPv4 address space to encompass a larger client device pool, but they do not transform the finite IP address space into an infinitely elastic resource. The inevitable outcome of this process, if it were to be constrained to operate solely within IPv4, is that we would see the fragmenting of the IPv4 Internet into a number of disconnected parts, probably based on the service ‘cones’ of the various points of presence of the content distribution servers, so that the entire concept of a globally unique and coherent address pool layered over a single coherent packet transmission realm would be foregone. Alternatively, we may see these growth pressures motivate the further deployment of IPv6, and the emergence of IPv6-only elements of the Internet as the network itself tries to maintain a cohesive and connected whole. There are commercial pressures pulling the network in both of these directions, so it’s entirely unclear what path the Internet will follow in the coming years, but my (admittedly cynical and perhaps overly jaded) personal opinion lies in a future of highly fragmented network, as least in terms of the underlying packet connectivity protocol. Can address allocation data help us to shed some light on what is happening in the larger Internet? Let’s look at what happened in 2025. It appears that the process of exhausting the remaining pools of unallocated IPv4 addresses is proving to be as protracted as the process of the transition to IPv6, although by the end of 2021 the end of the old registry allocation model had effectively occurred with the depletion of the residual pools of unallocated addresses in each of the Regional Internet Registries (RIRs).", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Apples, Trees, and Quasimodes", "url": "https://systemstack.dev/2025/09/humane-computing/", "content": "Apples, Trees, and Quasimodes. A while back, Ars Technica publisheda thoughtful piece about Jef Raskin, tracing his long pursuit of the “humane computer” and the cul-de-sacs where that pursuit ended. It’s a generous, well-told account of the designer who wanted to make machines simpler, kinder, and more aligned with the way people actually think. But part of what makes Raskin interesting is that his story isn’t just Apple’s story. He came out of the same cultural current John Markoff chronicled inWhat the Dormouse Said—the Bay Area tradition that treated computers not as office appliances but as tools for thought, instruments of liberation. Read that way, the Canon Cat and Raskin’s other projects aren’t just an eccentric side quest from a frustrated Apple veteran. It’s evidence of how far the humane ideal could stretch, and how quickly it ran up against the limits of commercial computing. Apple couldn’t deliver Raskin’s vision then, and it can’t deliver it now. Neither can any other big platform company. If we want to understand why, and what Raskin still tells us about humane computing, we have to put him back in the longer lineage he belonged to, and look at how his version of the dream carried that vision but also narrowed it. What the Dormouse Saiddocuments how the Bay Area counterculture  shaped early personal computing. LSD, communes, systems theory, amorphous defense research contracts, and Engelbart’s “augmentation” experiments all swirled together in a weird scene that accidentally (or maybenotso accidentally) created much of the modern world. The story usually gets told with a neat list:Engelbart’s demo, Nelson’sXanaduhypertext, Kay’sDynabook, Brand’sWhole Earth. Xerox PARC, Steve Jobs, the World Wide Web. The familiar pantheon. But that version turns a messy, improvisational moment into a plaque. Engelbart’s system needed a whole research staff just to operate; Nelson’s Xanadu was (and is) more sermon than software; Kay’s Dynabook lived mostly on paper; Brand mostly supplied vocabulary and vibe. What bound them together wasn’t working code so much as the conviction that computers could be more than appliances and calculators, even if no one agreed on what “more” meant. Ultimately all these weird white guys had a futurist vision: computers could beliberation machines.They weren’t just for business automation or scientific number-crunching; they could be deployed to expand consciousness and reshape how people thought and worked. Raskin belonged to this current. Before Apple, he was an artist and a musician. He brought a humanist’s suspicion of machine logic into the design lab. He argued forhumaneinterfaces: modeless, predictable, low-friction, focused on the human first. He wasn’t a prophet on his own crying in the wilderness so much as another strand of the same weave. That said, his role was different than that of some of these other figures. He tried to pull those ideals out of the lab and into machines ordinary people might actually use. The Macintosh began under his hand, though what shipped was less a tool for thought than a polished derivative—what you might call a “popular religion” of computing, stripped of the harder doctrines. The Canon Cat and its predecessors were Raskin’s counterargument: humane, text-first systems that tried to carry the spirit of theDormousetradition into the commercial world without sanding off everything that made it strange. It sort of worked, but only sort of. Raskin’s principles are laid out most clearly in 2000’sThe Humane Interface, but he’d been developing them since the late 1970s:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Disaster planning for regular folks (2015)", "url": "https://lcamtuf.coredump.cx/prep/index-old.shtml", "content": "Disaster planning for regular folks (2015). Written bylcamtuf@coredump.cx, Dec 2015, minor updates Jul 2021. Buy the book instead!Practical Doomsdayis an in-depth, data-packed guide\nto rational emergency preparedness. The book offers deeper and more polished insights on most of the topics covered on this page. For example,\nabout 40 pages are devoted to financial planning alone - from cash reserves, to insurance policies, to commodity derivatives. You can get it onAmazon, order fromBarnes & Noble, or visit your\nfavorite book place. Sample chapter is availablehere. The prepper culture begs to be taken with a grain of salt. In the public\nconsciousness, its has all the makings of a doomsday cult:\na tribe of unkempt misfits who hoard gold bullion, study herbalism,\nand preach about the imminent collapse of our society. Today, most of us see such worries as absurd. It's not that life-altering disasters are\nrare: every year, we hear about millions of people displaced by wildfires, earthquakes,\nhurricanes, or floods. Heck, not a decade goes by without at least one first-class\ndemocracy lapsing into armed conflict or fiscal disarray. But having grown up in a period\nof prosperity and calm, we find it difficult to believe that an episode of bad weather or a currency crisis\ncould upend our lives. I suspect that we dismiss such hazards not only because they seem surreal, but also because\nworrying about them can make one feel helpless and lost. What's more, we tend to follow the\nsame instincts to tune out far more pedestrian and avoidable risks. For example, \nmost of us don't plan ahead for losing a job, for dealing with a week-long water outage, or\nfor surviving the night if our home goes up in smoke. Quite often, our singular strategy for dealing with such dangers is to hope for the\ngovernment to bail us out. But no matter if our elected officials prefer to school us with\npassages from Milton Friendman or from Thomas Piketty, the hard truth is that no state can provide\na robust safety net for all of life's likely contingencies; in most places, government-run social\nprograms are severely deficient in funding, in efficiency, and in scope. Large-scale disasters\npit us against even worse odds. From New Orleans in 2005 to Fukushima in 2011, there are\ncountless stories of people left behind due to political dysfunction, poorly allocated\nresources, or lost paperwork. The purpose of this guide is to combat this mindset of learned helplessness by\npromoting simple, level-headed, personal preparedness techniques that are easy to\nimplement, don't cost much, and will probably help cope with whatever life throws our way. \nMore important, they don't get in the way of enjoying your everyday life - and instead of \nfeeding anxieties, they should make it easier to detach from the doom-and-gloom of 24-hour news. Effective preparedness can be simple, but it has to be rooted in an honest and\nsystematic review of the risks one is likely to face. Plenty of newcomers begin\nby shopping for ballistic vests and night vision goggles; they would be better\nserved by grabbing a fire extinguisher, some bottled water, and then putting the rest of\ntheir money in a rainy-day fund. To avoid being overwhelmed when trying to enumerate risks, I found that it's best to focus on\nbroad outcomes instead of trying to envision every single way for things to go south.\nFor example, it should not matter if one is laid off because of a downsizing, because\ntheir new boss hates them, or because the coworkers finally catch them stealing paperclips. The\noutcome is the same: they are out of a job and urgently need a way to pay the bills.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Fast Concordance: Instant concordance on a corpus of >1,200 books", "url": "https://iafisher.com/concordance/", "content": "Fast Concordance: Instant concordance on a corpus of >1,200 books. Instantconcordanceon a corpus of\n                over 1,200 public-domain classic books, courtesy ofStandard\n                    Ebooks. Read about how it was implementedhere.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Mark Carney's Full Speech at the World Economic Forum", "url": "https://www.youtube.com/watch?v=btqHDhO4h10", "content": "Mark Carney's Full Speech at the World Economic Forum", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I'm addicted to being useful", "url": "https://www.seangoedecke.com/addicted-to-being-useful/", "content": "I'm addicted to being useful. When I get together with my friends in the industry, I feel a little guilty about how much I love my job. This is atough timeto be a software engineer. The job was less stressful in the late 2010s than it is now, and I sympathize with anyone who is upset about the change. There are a lot of objective reasons to feel bad about work. But despite all that, I’m still having a blast. I enjoy pulling together projects, figuring out difficult bugs, and writing code in general. I like spending time with computers. But what I really love isbeing useful. The main character in Gogol’s short storyThe Overcoatis a man called Akaky Akaievich1. Akaky’s job is objectively terrible: he’s stuck in a dead-end copyist role, being paid very little, with colleagues who don’t respect him. Still, he loves his work, to the point that if he has no work to take home with him, he does some recreational copying just for his own sake. Akaky is a dysfunctional person. But his dysfunction makes him a perfect fit for his job2. It’s hard for me to see a problem and not solve it. This is especially true if I’m the only person (or one of a very few people) who could solve it, or if somebody is asking for my help. I feel an almost physical discomfort about it, and a corresponding relief and satisfaction when I do go and solve the problem. The work of a software engineer - or at least my work as a staff software engineer - is perfectly tailored to this tendency. Every day people rely on me to solve a series of technical problems3. In other words, like Akaky Akaievich, I don’t mind the ways in which my job is dysfunctional, because it matches the ways in which I myself am dysfunctional: specifically,my addiction to being useful. (Of course, it helps that my working conditions are overallmuchbetter than Akaky’s). I’m kind of like a working dog, in a way. Working dogs get rewarded with treats4, but they don’t do itforthe treats. They do it for the work itself, which is inherently satisfying. This isn’t true of all software engineers. But it’s certainly true of many I’ve met: if not an addiction to being useful, then they’re driven by an addiction to solving puzzles, or to the complete control over your work product that you only really get in software or mathematics. If they weren’t working as a software engineer, they would be getting really into Factorio, or crosswords, or tyrannically moderating some internet community. A lot of the advice I give about working a software engineering job is really about how I’ve shaped my need to be useful in a way that delivers material rewards, and how I try to avoid the pitfalls of such a need. For instance,Protecting your time from predators in large tech companiesis about how some people in tech companies will identify people like me and wring us out in ways that only benefit them.Crushing JIRA tickets is a party trick, not a path to impactis about how I need to be usefulto my management chain, not to the ticket queue.Trying to impress people you don’t respectis about how I cope with the fact that I’m compelled to be useful to some people who I may not respect or even like. There’s a lot of discussion on the internet about whatoughtto motivate software engineers: money and power, producing realvalue, ushering in the AI machine god, and so on. But whatactually doesmotivate software engineers is often more of an internal compulsion. If you’re in that category - as I suspect most of us are - then it’s worth figuring out how you can harness that compulsion most effectively. I think in Russian this is supposed to be an obviously silly name, like “Poop Poopson”. Unfortunately, his low status and low pay catches up with Akaky in the end. His financial difficulty acquiring a new coat for the cold Russian winter (and his lack of backbone) end up doing him in, at which point the story becomes a ghost story. I interpret “technical problem” quite broadly here: answering questions, explaining things, and bug-fixing all count.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nova Launcher added Facebook and Google Ads tracking", "url": "https://lemdro.id/post/lemdro.id/35049920", "content": "Nova Launcher added Facebook and Google Ads tracking", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: TopicRadar – Track trending topics across HN, GitHub, ArXiv, and more", "url": "https://apify.com/mick-johnson/topic-radar", "content": "Show HN: TopicRadar – Track trending topics across HN, GitHub, ArXiv, and more. Pricing Pay per usage mick-johnson/topic-radar Track any topic across the internet and get aggregated, ranked results from multiple sources in one place. Perfect for market research, competitive intelligence, trend monitoring, content creation, and staying updated on any subject. Pricing Pay per usage Rating 5.0 (3) Developer", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nvidia Stock Crash Prediction", "url": "https://entropicthoughts.com/nvidia-stock-crash-prediction", "content": "Nvidia Stock Crash Prediction.  One of the questions ofthe 2026acxprediction contestis whetherNvidia’s\nstock price will close below $100on any day in 2026. At the time of writing, it\ntrades at $184 and a bit, so going down to $100 would be a near halving of the\nstock value of the highest valued company in the world. It’s an interesting question, and it’s worth spending some time on it. If you just want the answer, my best prediction is that the probability is\naround 10 %. I didn’t expect to get such a high answer, but read on to see how\nwe can find out. Whenwe predicted the Dow Jones index crossing a barrier in 2023, we treated the\nindex as an unbiased random walk. That was convenient, but we cannot do it with\nthe Nvidia question because of one major difference: the time scale. Over short time spans, thevolatility11Or noise, or variation, or standard\ndeviation.of stock movements dominate theirreturn22Or signal, or drift,\nor average change.. This happens because noise grows with the square root of\ntime, while signal grows linearly with time. The plot below illustrates an imaginary amazing investment which has a yearly\nlog-return of 0.3, and a yearly volatility of 0.3.33Readers aware thatstonks\ngo upwill recognise this as an unrealistic Sharpe ratio of 1.0.The middle\nline follows our best guess for how the investment will grow after each year,\nand the outer curves illustrate our uncertainty around the exact value of it.  Early on, we can see that the uncertainty is much bigger than the height to the\ntrend line. Before a year has passed, the exact result is determined more by\nnoise than by growth. Toward the end, growth has taken over and the noise has a\nsmaller effect. One measure of how much volatility there is compared to expected return is the\nsignal-to-noise ratio. It’s computed as", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Prediction markets are ushering in a world in which news becomes about gambling", "url": "https://www.theatlantic.com/technology/2026/01/america-polymarket-disaster/685662/", "content": "Prediction markets are ushering in a world in which news becomes about gambling. For the past week, I’ve found myself playing the same23-second CNN clipon repeat. I’ve watched it in bed, during my commute to work, at the office, midway through making carrot soup, and while brushing my teeth. In the video, Harry Enten, the network’s chief data analyst, stares into the camera and breathlessly tells his audience about the gambling odds that Donald Trump will buy any of Greenland. “The people who are putting their money where their mouth is—they are absolutely taking this seriously,” Enten says. He taps the giant touch screen behind him and pulls up a made-for-TV graphic: Based on how people were betting online at the time, there was a 36 percent chance that the president would annex Greenland. “Whoa, way up there!” Enten yells, slapping his hands together. “My goodness gracious!” The ticker at the bottom of the screen speeds through other odds: Will Gavin Newsom win the next presidential election? 19 percent chance. Will Viktor Orbán be out as the leader of Hungary before the end of the year? 48 percent chance.  These odds were pulled from Kalshi, which hilariouslyclaimsnot to be a gambling platform: It’s a “prediction market.” People go to sites such as Kalshi and Polymarket—another big prediction market—in order to put money down on a given news event. Nobody would bet on something that they didn’t believe would happen, the thinking goes, and so the markets are meant to forecast the likelihood of a given outcome. Listen: Prediction markets and the “suckerification” crisis, with Max Read Prediction markets let you wager on basically anything. Will Elon Musk fatheranother babyby June 30? WillJesus returnthis year? Will Israelstrike Gaza tomorrow? Will thelongevity guruBryan Johnson’s next functional sperm count be greater than “20.0 M/ejac”? These sites have recently boomed in popularity—particularly amongterminally online young menwho trade meme stocks and siphon from their 401(k)s to buy up bitcoin. But now prediction markets are creeping into the mainstream. CNNannounced a dealwith Kalshi last month to integrate the site’s data into its broadcasts, which has led to betting odds showing up in segments about Democrats possibly retaking the House, credit-card interest rates, and Federal Reserve Chair Jerome Powell. At least twice in the past two weeks, Enten has told viewers about the value of data from people who are “putting their money where their mouth is.”  On January 7, the media giant Dow Jones announced its own collaboration with Polymarket and said that it will begin integrating the site’s odds across its publications, includingThe Wall Street Journal. CNBC has a prediction-market deal, as does Yahoo Finance,Sports Illustrated, andTime. Last week, MoviePassannouncedthat it will begin testing a betting platform. On Sunday, the Golden Globes featured Polymarket’s forecasts throughout the broadcast—because apparently Americans wanted to know whether online gamblers favored Amy Poehler or Dax Shepard to win Best Podcast.  Media is a ruthless, unstable business, andrevenue streams are drying up; if you squint, you can see why CNN or Dow Jones mightsign a contractthat, after all, provides its audience with some kind of data. On air, Enten cites Kalshi odds alongside Gallup polls and Google searches—what’s the difference? “The data featured through our partnership with Kalshi is just one of many sources used to provide context around the stories or topics we are covering and has no impact on editorial judgment,” Brian Poliakoff, a CNN spokesperson, told me in a statement. Nolly Evans, theJournal’s digital general manager, told me that Polymarket provides the newspaper’s journalists with “another way to quantify collective expectations—especially around financial or geopolitical events.” In an email, Jack Suh, a Kalshi spokesperson, told me that the company’s partnerships are designed to inform the public, not to encourage more trading. Polymarket declined to comment. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Crates.io: Development Update", "url": "https://blog.rust-lang.org/2026/01/21/crates-io-development-update/", "content": "Crates.io: Development Update. Time flies! Six months have passed since our last crates.io development update, so it's time for another one. Here's a summary of the most notable changes and improvements made tocrates.ioover the past six months. Crate pages now have a new \"Security\" tab that displays security advisories from theRustSecdatabase. This allows you to quickly see if a crate has known vulnerabilities before adding it as a dependency.  The tab shows known vulnerabilities for the crate along with the affected version ranges. This feature is still a work in progress, and we plan to add more functionality in the future. We would like to thank theOpenSSF(Open Source Security Foundation) for funding this work andDirkjan Ochtmanfor implementing it. In our July 2025 update, we announced Trusted Publishing support for GitHub Actions. Since then, we have made several enhancements to this feature. Trusted Publishing now supportsGitLab CI/CDin addition to GitHub Actions. This allows GitLab users to publish crates without managing API tokens, using the same OIDC-based authentication flow. Note that this currently only works with GitLab.com. Self-hosted GitLab instances are not supported yet. The crates.io implementation has been refactored to support multiple CI providers, so adding support for other platforms like Codeberg/Forgejo in the future should be straightforward. Contributions are welcome! Crate owners can now enforce Trusted Publishing for their crates. When enabled in the crate settings, traditional API token-based publishing is disabled, and only Trusted Publishing can be used to publish new versions. This reduces the risk of unauthorized publishes from leaked API tokens. Thepull_request_targetandworkflow_runGitHub Actions triggers are now blocked from Trusted Publishing. These triggers have been responsible for multiple security incidents in the GitHub Actions ecosystem and are not worth the risk.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Danish pension fund divesting US Treasuries", "url": "https://www.reuters.com/business/danish-pension-fund-divest-its-us-treasuries-2026-01-20/", "content": "Danish pension fund divesting US Treasuries", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Revive a mostly dead Discord server", "url": "item?id=46697735", "content": "Ask HN: Revive a mostly dead Discord server", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Running Claude Code dangerously (safely)", "url": "https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/", "content": "Running Claude Code dangerously (safely). I’ve been using Claude Code more and more recently. At some point I realized that rather than do something else until it finishes, I would constantly check on it to see if it was asking for yet another permission, which felt like it was missing the point of having an agent do stuff. So I wanted to use Claude Code with the--dangerously-skip-permissionsflag. If you haven’t used it, this flag does exactly what it says: it lets Claude Code do whatever it wants without asking permission first. No more “May I install this package?”, “Should I modify this config?”, “Can I delete these files?” It just… does it. Which is great for flow since I don’t have to worry that it stopped doing stuff just to ask a permission question. But also, you know, dangerous. I like my filesystem intact, so the obvious solution is to not run this thing directly on my OS account. First instinct: throw it in a Docker container. Containers are for isolation, right? Except I want Claude to be able to build Docker images. And run containers. And maybe orchestrate some stuff. So now you need Docker-in-Docker, which means--privilegedmode, which defeats the entire purpose of sandboxing. That means trading “Claude might mess up my filesystem” for “Claude has root-level access to my container runtime.” Not great.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Zen of Reticulum", "url": "https://github.com/markqvist/Reticulum/blob/master/Zen%20of%20Reticulum.md", "content": "The Zen of Reticulum", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "RCS for Business", "url": "https://developers.google.com/business-communications/rcs-business-messaging", "content": "RCS for Business. Engage with customers seamlessly on Android and iOS. Allow your customers to interact with\n  your business directly, and enhance the interaction with distinctive branding and rich\n  media. Measure engagement with read receipts and analytics, and build trust with a\n  'Verified' icon. Learn more Ready to become an RCS for Business partner?Partner interest formarrow_forward Learn more Learn more Learn more Go to Console Manage RCS for Business agents from the Administration Console, and get insights into message activity\n  and billing. Explore the key documentation, or contact us directly for support. Exclusively for registered RCS for Business partners: Access a curated collection of resources to help you champion RCS for Business with your internal teams and brand clients.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "When \"likers'' go private: Engagement with reputationally risky content on X", "url": "https://arxiv.org/abs/2601.11140", "content": "When \"likers'' go private: Engagement with reputationally risky content on X. arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community?Learn more about arXivLabs. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "LG UltraFine Evo 6K 32-inch Monitor Review", "url": "https://www.wired.com/review/lg-ultrafine-evo-6k-32-inch-monitor/", "content": "LG UltraFine Evo 6K 32-inch Monitor Review. 7/10 More pixels isnever a bad thing, right? That's at least part of the reasoning behind the existence of the LG UltraFine Evo 6K. This 32-inch monitor aims to put even more pixels in front of content creators and professionals. Beyond that, it has an attention-grabbing design and off-the-charts image quality. It's one of thebest monitorsyou can buy for content creators, despite some of the unfortunate trade-offs it comes with. The 32-inch LG UltraFine Evo 6K is a very pretty monitor. I wouldn't blame you for mistaking this as an Apple product, given the focus on clean lines, simple shapes, and designerly aesthetic. The extra-wide stand means that the base itself isn’t overly large. Like theApple Studio Display, the flat base provides more usable desk space rather than occupying it. The stand itself has a unique design, too. It resembles the styling Apple uses on theiMacand Studio Display, but it has a textured pattern on the back. It’s gorgeous, though you probably won’t spend a lot of time looking at the back of the monitor unless your desk is in the middle of the room or in command position (if you know, you know). I also like that the back of the cabinet is flat, giving it a sleek look that the rounded backs of typical monitors can’t achieve. Because it uses conventional backlighting, though, it’s not as thin as OLED displays like some ofSamsung’s Odyssey gaming monitors. The UltraFine 6K also has some impressively thin bezels, too, adding to the ultra-modern aesthetic. While they’re not “virtually borderless” as LG states, they’re smaller than the bezels on most monitors I’ve tested. One of my favorite aspects of the UltraFine Evo 6K is the speakers. The pair of included speakers on this might be the best I've heard on a monitor. They are extremely loud and clear. There's even a decent amount of bass in there, to the point where you won't need a pair ofcomputer speakers. One thing I don’t love is the port placement. In favor of keeping everything clean and minimalist, there’s nowhere to hide the ports, so they’re just lined up vertically on the back and in the middle of the monitor. That makes them hard to reach, and there’s no built-in cable management to speak of. The UltraFine Evo 6K sports a decent amount of adjustment, though the design of the hinge limits some of what's possible. It can rotate a full 90 degrees into portrait mode, which is awesome. But the height adjustment is pretty minimal, with a range of only a few inches or so. It also doesn't swivel. As a tall person, I didn't have a hard time finding a comfortable position with this monitor. But for shorter folks, the height of the UltraFine Evo 6K could cause some significant ergonomic problems because of how high up the minimum height is. It does have a VESA mount, so you can avoid all these problems by using a monitor arm. While I don’t like their placement, the ports themselves are powerful. You get the latest standards and speeds, including DisplayPort 2.1, HDMI 2.1, and two Thunderbolt 5 ports. There's a built-inKVM switchfor using the same monitor and peripherals with multiple devices. The UltraFine Evo 6K lacks a few ports that other high-end monitors include, such as a headphone jack, Ethernet jack, or upstream USB-A ports. One of the Thunderbolt ports supports power delivery, although only up to 96 watts. This is less than the 240 watts that's possible withThunderbolt 5. A high-powered laptop like my 16-inch M4 Pro MacBook Pro couldn't hold a charge when plugged in. However, youcandaisy chain multiple 6K monitors together using just a single cable, which feels impossibly great.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Aventos – An experiment in cheap AI SEO", "url": "https://www.aventos.dev/", "content": "Show HN: Aventos – An experiment in cheap AI SEO", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Dockerhub for Skill.md", "url": "https://skillregistry.io/", "content": "Dockerhub for Skill.md. Find and install skills for Claude, ChatGPT, and AI agents. The definitive registry for SKILLS.md files that extend your AI assistant's capabilities. Then runsr search<query>orsr install<skill> Search for skills like \"1password\", \"browser\", \"github\", or any tool you want your AI to use Automatically search Skill Registry for relevant skills before starting tasks. Enhances Claude's capabilities by finding specialized skills that can help with the current task. Use when you need to control Slack from Clawdbot via the slack tool, including reacting to messages or pinning/unpinning items in Slack channels or DMs. Automates browser interactions for web testing, form filling, screenshots, and data extraction. Use when the user needs to navigate websites, interact with web pages, fill forms, take screenshots, test web applications, or extract information from web pages. \"Interact with GitHub using the `gh` CLI. Use `gh issue`, `gh pr`, `gh run`, and `gh api` for issues, PRs, CI runs, and advanced queries.\" Guide for creating effective skills that extend Claude's capabilities. Use when creating new skills or updating existing skills with specialized knowledge, workflows, or tool integrations. Local speech-to-text with the Whisper CLI (no API key). Gemini CLI for one-shot Q&A, summaries, and generation.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Electricity use of AI coding agents", "url": "https://www.simonpcouch.com/blog/2026-01-20-cc-impact/", "content": "Electricity use of AI coding agents. Throughout 2025, we got better estimates of electricity and water use of AI chatbots. There are all sorts of posts I could cite on this topic, but a favorite isthis blog postfrom Our World in Data’s Hannah Ritchie. On the electricity front:  In short, “unless you’re an extreme power user, asking AI questions every day is still a rounding error on your total electricity footprint.” A similar story applies to water usage.This one from Benjamin Todd: The average Americanuses 1600 liters of water per day, so even if you make 100 prompts per day, at 2ml per prompt, that’s only 0.01% of your total water consumption. Using ashower for one secondwould use far more. Generally, these analyses guide my own thinking about the environmental impacts of my individual usage of LLMs; if I’m interested in reducing my personal carbon footprint, I’m much better off driving a couple miles less a week or avoiding one flight each year. This is indeed the right conclusion for users of chat interfaces like chatgpt.com or claude.ai. That said, 1 or 10 or 100 median prompts a day is many orders of magnitude off from my own personal use of LLMs; I likely am, in Hannah Ritchie’s words, an “extreme power user.” I work in software and spend much of my workday driving 2 or 3 coding agents, like Claude Code, at a time. Thus, a much more relevant question for me ishow much energy does a typical Claude Code session consume?(I’m not going to discuss water use in this post.) tl;dr, much more:  There are so many considerations and assumptions and pieces of shorthand one must use along the way to answer this sort of question. I’ll do my best to call those out throughout this post, but please do understand this is still just Sunday afternoon napkin math from Some Guy.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: macOS native DAW with Git branching model", "url": "https://www.scratchtrackaudio.com", "content": "Show HN: macOS native DAW with Git branching model. Multi-track DAW for macOS ScratchTrack brings Git-style branching and merging to audio production. Experiment freely, collaborate without conflicts, and never lose a take again. Every recording, edit, and mix decision tracked. Branch to experiment, merge what works. Not another DAW trying to do everything. ScratchTrack focuses on what matters: capturing ideas and keeping them organized. Create branches to explore different arrangements, mix approaches, or instrument choices without affecting your main session. Merge back what works. Work with session musicians, producers, or co-writers asynchronously. ScratchTrack handles merge conflicts intelligently so you stay in creative flow. Every recording, edit, and mix decision is preserved. Revert to any point in your project's history. Compare takes across time. Record multiple inputs simultaneously with take-based comping. Monitor playback while recording. Professional-quality 44.1kHz/16-bit audio. Full functionality without an internet connection. Changes sync automatically when you're back online. Your work is always accessible. Export individual stems or full mixes as industry-standard WAV files. Take your tracks into any DAW for final production.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IP over Avian Carriers with Quality of Service (1999)", "url": "https://www.rfc-editor.org/rfc/rfc2549.html", "content": "IP over Avian Carriers with Quality of Service (1999)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Scaling long-running autonomous coding", "url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/", "content": "Scaling long-running autonomous coding. Scaling long-running autonomous coding. Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents: This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens. They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not. In my predictions for 2026the other dayI said that by 2029: I think somebody will have built a full web browser mostly using AI assistance, and it won’t even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it’ll get so much easier. I may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach: To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explorethe source code on GitHub. But how well did they do? Their initial announcement a couple of days ago was met withunsurprising skepticism, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo. It looks like they addressed that within the past 24 hours. Thelatest READMEincludes build instructions which I followed on macOS like this: This got me a working browser window! Here are screenshots I took of google.com and my own website:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Level S4 solar radiation event", "url": "https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026", "content": "Level S4 solar radiation event. G4 Levels were first reached at 2:38pm EST (1938 UTC) on 19 January, 2026 upon CME shock arrival. CME passage is expected to continue through the evening with G4 levels remaining possible.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Parallel Agentic Search on the Twitter Algorithm", "url": "https://www.morphllm.com/playground/na/warpgrep?repo=xai-org%2Fx-algorithm", "content": "Show HN: Parallel Agentic Search on the Twitter Algorithm", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Overcomplexity of the Shadcn Radio Button", "url": "https://paulmakeswebsites.com/writing/shadcn-radio-button/", "content": "The Overcomplexity of the Shadcn Radio Button. The other day I was asked to update the visual design of radio buttons in a web\napp at work. I figured it couldn't be that complicated. It's just a radio button\nright? Boom! Done. Radio buttons are a built-in HTML element. They've been around for\n30 years. The browser makes it easy. Time for a coffee. I dug into our codebase and realized we were using two React components fromShadcnto power our radio buttons:<RadioGroup>and<RadioGroupItem>. For those unfamiliar with Shadcn, it's a UI framework that provides a bunch of\nprebuilt UI components for use in your websites. Unlike traditional UI\nframeworks like Bootstrap, you don't import it with a script tag ornpm install. Instead you run a command that copies the components into your\ncodebase. Here's the code that was exported from Shadcn into our project: Woof... 3 imports and 45 lines of code. And it's importing a third party icon\nlibrary just to render a circle. (Who needs CSSborder-radiusor the SVG<circle>element when you can add a third party dependency instead?) All of the styling is done by the 30 different Tailwind classes in the markup. I\nshould probably just tweak those to fix the styling issues. But now I'm distracted, annoyed, and curious. Where's the actual<input>?\nWhat's the point of all this? Let's dig a little deeper. The Shadcn components import components from another library called Radix. For\nthose unfamiliar with Radix, it's a UI framework that provides a bunch of\nprebuilt UI components... Wait a second! Isn't that what I just said about Shadcn? What gives? Why do we\nneed both? Let's see what the Radix docs say:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "url": "https://github.com/majcheradam/ocrbase", "content": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Fence – Sandbox CLI commands with network/filesystem restrictions", "url": "https://github.com/Use-Tusk/fence", "content": "Show HN: Fence – Sandbox CLI commands with network/filesystem restrictions", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Alignment Game (2023)", "url": "https://dmvaldman.github.io/alignment-game/", "content": "The Alignment Game (2023). TLDR; I made a game to align people and priorities in aGoogle Sheet At work as an “executive” I found myself often focused on issues of “alignment,” especially among the other execs. It just turns out as an organization grows, it operates on fractured sets of implicit assumptions. I found there is often little disagreement onwhatthe problems are, but plenty of disagreement onwhichwere more important. People then carry these differences into decision making without revealing their working assumptions, cascading tradeoffs are made and efforts diverge. There was an incredible sense of clarity when everyone could agree on what’s most important in unison, and I wanted to get there. I started by doing the exercise of stack ranking priorities. Sometimes this would just be finger to the wind thinking about issues, sometimes this would mean months of work to assess impact rigorously. I would challenge others in the company to make their own stack rankings. We’d then discuss the differences and try to converge on a shared ordering. This was an incredibly fruitful exercise that led to great conversations. With more than two people though, as with an exec team, there was a need for more process. It turns out there’s a whole branch of mathematics calledvoting theoryall about how to get a plurality of people to agree on a single thing. The concepts ofrun-off elections,“I cut, you choose”division algorithms, andhow medical schools select studentsthrough ranked preferences are all facets of voting theory. In my situation, we had a half dozen stack ranked lists of priorities and we wanted to align people on a single ordering. Turns out, there isno algorithmthat always works! You can always find yourself in a situation where more than half of people want A over B, some other half want B over C, and some other half want C over A, so a majority are upset with any outcome. Each ranking algorithm makes certain tradeoffs. TheKemeny-Young methodis a ranking algorithm that finds the ordering which minimizes total disagreement across all voters. A disagreement is any time one voter chooses A over B and another chooses B over A. One of the voters would need to swap their preferences in order to align, and the Kemeny Young method finds the ordering requiring the fewest swaps. The downsides of the Kemeny Young method come down to it being the “compromise solution.” Half of people may think A is most important and B least, and another half would invert that, and the Kemeny Young method would put it in the middle and upset everyone. Something to be cognizant of. The important bit is not to use the ordering as marching orders, but as a tool for conversation. The benefits of Kemeny Young lies in its interpretability. Because it works by counting pairwise disagreements, you get a natural measure of which items are contentious, as well as which voters are misaligned. It can be said of any two people: “You need to change your mind on X things to align with one another” and between any two priorities: “X voters disagreed on this prioritization.” This makes it easy to identify where consensus already exists versus what we need to debate. We had great success playing the game. Each person would make their ranking in private, we’d gather them all, churn through an algorithm and immediately all implicit tradeoffs are surfaced. We would then meet pairwise to try to align our priorities. This worked especially well at company off-sites and quarterly planning cycles. I’ve since turned the process into aGoogle Sheet. Try it at work or in your personal life!", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Claude Chill: Fix Claude Code's flickering in terminal", "url": "https://github.com/davidbeesley/claude-chill", "content": "Claude Chill: Fix Claude Code's flickering in terminal", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Does Calendar-Based Time-Intelligence Change Custom Logic?", "url": "https://towardsdatascience.com/does-calendar-based-time-intelligence-change-custom-logic/", "content": "Does Calendar-Based Time-Intelligence Change Custom Logic?. Let's look at calculating the moving average over time With the advent ofcalendar-based Time Intelligence, the need for custom Time Intelligence logic has decreased dramatically. Now, we can create custom calendars to meet our Time Intelligence calculation needs. You might have read my article about advanced Time Intelligence: https://towardsdatascience.com/advanced-time-intelligence-in-dax-with-performance-in-mind/ Most of the custom logic is no longer needed. But we still have scenarios where we must have custom calculations, like running average. Some time ago, SQLBIwrote an articleabout calculating the running average. This piece uses the same principles described there in a slightly different approach. Let’s see how we can calculate the running average over three months by using the new Calendars.", "source": "TowardsDataScience", "date": "2026-01-21T10:21:58.839784", "author": null, "score": null}
{"title": "How to Perform Large Code Refactors in Cursor", "url": "https://towardsdatascience.com/how-to-perform-large-code-refactors-in-cursor/", "content": "How to Perform Large Code Refactors in Cursor. Learn how to perform code refactoring with LLMs Refactoring codehas historically been a tedious yet important task. Refactoring is the work of taking some piece of code and cleaning it up, either by better separation of concerns, the Don’t Repeat Yourself (DRY) principle, or other code hygiene principles. Code refactors have always been important, but with the release of coding agents, we’re seeing higher coding outputs, which inevitably leads to more need for code refactoring. I more and more often find myself in situations where some code needs to be refactored, though I don’t think this is a warning sign, considering the amount of code I output is also significantly higher now with the help of LLMs. Luckily, the effort to refactor code has significantly gone down since the release of LLMs. In this article, I’ll go through my high-level approach to performing code refactoring using coding agents like Cursor or Claude Code. I’ll cover my generic approach and thought process, so the model you utilize doesn’t matter. You should perform code refactoring whenever you notice a lot of antipatterns in your code, or when you notice you (or your coding agent) is spending more time than should be needed on an implementation. Your threshold before performing a refactoring should also be lower than it was before the release of LLMs, considering refactors are way easier and faster to implement now using coding agents. This is because part of the training set for coding agents is to refactor code, and they’re especially good at that. In a lot of cases, I would even say they’re better than humans, considering refactoring requires a lot of working memory: Thus, you should perform code refactoring when: And you should perform code refactorings because: In this section, I’ll cover my high-level approach to refactoring code. I’ll go through four steps:", "source": "TowardsDataScience", "date": "2026-01-21T10:22:00.027055", "author": null, "score": null}
{"title": "You Probably Don’t  Need a Vector Database for Your RAG — Yet", "url": "https://towardsdatascience.com/you-probably-dont-need-a-vector-database-for-your-rag-yet/", "content": "You Probably Don’t  Need a Vector Database for Your RAG — Yet. Numpy or SciKit-Learn might meet all your retrieval needs Right now, off the back of Retrieval Augmented Generation (RAG), vector databases are getting a lot of attention in the AI world. Many people say you need tools like Pinecone, Weaviate, Milvus, or Qdrant to build a RAG system and manage your embeddings. If you are working on enterprise applications with hundreds of millions of vectors, then tools like these are essential. They let you perform CRUD operations, filter by metadata, and use disk-based indexing that goes beyond your computer’s memory. But for most internal tools, documentation bots, or MVP agents, adding a dedicated vector database might be overkill. It increases complexity, network delays, adds serialisation costs, and makes things more complicated to manage. The truth is that “Vector Search” (i.e the Retrieval part of RAG) is just matrix multiplication. And Python already has some of the world’s best tools for that. In this article, we’ll show how to build a production-readyretrieval componentof a RAG pipeline for small-to-medium data volumes using only NumPy and SciKit-Learn. You’ll see that it’s possible to search millions of text strings in milliseconds, all in memory and without any external dependencies. Typically, RAG involves four main steps: Steps 1 and 4 rely on large language models. Steps 2 and 3 are the domain of the Vector DB. We will concentrate on parts 2 and 3 and how we avoid using vector DBs entirely. But when we’re searching our vector database, what actually is “closeness”? Usually, it isCosine Similarity. If your two vectors are normalised to have a magnitude of 1, then cosine similarity is just the dot product of the two. If you have a one-dimensional query vector of size N, Q(1xN), and a database of document vectors of size M by N, D(MxN), finding the best matches is not a database query; it is a matrix multiplication operation, the dot product of D with the transpose of Q.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:01.207995", "author": null, "score": null}
{"title": "Why Package Installs Are Slow (And How to Fix It)", "url": "https://towardsdatascience.com/why-package-installs-are-slow-and-how-to-fix-it/", "content": "Why Package Installs Are Slow (And How to Fix It). How sharded indexing patterns solve a scaling problem in package management Every developerknows the wait. You type an install command and watch the cursor blink. The package manager churns through its index. Seconds stretch. You wonder if something broke. This delay has a specific cause: metadata bloat. Many package managers maintain a monolithic index of every available package, version, and dependency. As ecosystems grow, these indexes grow with them. Conda-forge surpasses 31,000 packages across multiple platforms and architectures. Other ecosystems face similar scale challenges with hundreds of thousands of packages. When package managers use monolithic indexes, your client downloads and parses the entire thing for every operation. You fetch metadata for packages you will never use. The problem compounds: more packages mean larger indexes, slower downloads, higher memory consumption, and unpredictable build times. This is not unique to any single package manager. It is a scaling problem that affects any package ecosystem serving thousands of packages to millions of users. Conda-forge, like some package managers, distributes its index as a single file. This design has advantages: the solver gets all the information it needs upfront in a single request, enabling efficient dependency resolution without round-trip delays. When ecosystems were small, a 5 MB index downloaded in seconds and parsed with minimal memory. At scale, the design breaks down. Consider conda-forge, one of the largest community-driven package channels for scientific Python. Its repodata.json file, which contains metadata for all available packages, exceeds 47 MB compressed (363 MB uncompressed). Every environment operation requires parsing this file. When any package in the channel changes – which happens frequently with new builds – the entire file must be re-downloaded. A single new package version invalidates your entire cache. Users re-download 47+ MB to get access to one update. The consequences are measurable: multi-second fetch times on fast connections, minutes on slower networks, memory spikes parsing the 363 MB JSON file, and CI pipelines that spend more time on dependency resolution than actual builds. The solution borrows from database architecture. Instead of one monolithic index, you split metadata into many small pieces. Each package gets its own “shard” containing only its metadata. Clients fetch the shards they need and ignore the rest.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:02.422308", "author": null, "score": null}
{"title": "Bridging the Gap Between Research and Readability with Marco Hening Tallarico", "url": "https://towardsdatascience.com/bridging-the-gap-between-research-and-readability-with-marco-hening-tallarico/", "content": "Bridging the Gap Between Research and Readability with Marco Hening Tallarico. Diluting complex research, spotting silent data leaks, and why the best way to learn is often backwards. In the Author Spotlight series, TDS Editors chat with members of our community about their career path in data science and AI, their writing, and their sources of inspiration. Today, we’re thrilled to share our conversation withMarco Hening Tallarico. Marco is a graduate student at the University of Toronto and a researcher for Risklab, with a deep interest in applied statistics and machine learning. Born in Brazil and having grown up in Canada, Marco appreciates the universal language of mathematics. What motivates you to take dense academic concepts (like Stochastic Differential Equations) and turn them into accessible tutorials for the broader TDS community? It’s natural to want to learn everything in its natural order. Algebra, calculus, statistics, etc. But if you want to make fast progress, you have to abandon this inclination. When you’re trying to solve a maze, it’s cheating to pick a place in the middle, but in learning, there is no rule. Start at the end and work your way back if you like. It makes it less tedious. YourData Science Challengearticle focused on spotting data leakage in code rather than just theory. In your experience, which silent leak is the most common one that still makes it into production systems today? It’s really easy to let data leakage seep in during data analysis, or when using aggregates as inputs to the model. Especially now that aggregates can be computed in real time relatively easily. Before graphing, before even running the.head()function, I think it’s important to make the train-test split. Think about how the split should be made, from user level, size, and chronology to a stratified split: there are many choices you can make, and it’s worth taking the time. Also, when using metrics like average users per month, you need to double-check that the aggregate wasn’t calculated during the month you’re using as your testing set. These are trickier, as they are indirect. It’s not always as obvious as not using black-box data when you’re trying to predict what planes will crash. If you have the black box, it’s not a prediction; the plane did crash. You mention thatlearning grammar from data alone is computationally costly. Do you believe hybrid models (statistical + formal) are the only way to achieve sustainable AI scaling in the long run? If we take LLMs for example, there are a lot of easy tasks that they struggle with, like adding a list of numbers or turning a page of text into uppercase. It’s not unreasonable to think that just making the model larger will solve these problems but it’s not a good solution. It’s a lot more reliable to have it invoke a.sum()or.upper()function on your behalf and use its language reasoning to select inputs. This is likely what the major AI models are already doing with clever prompt engineering.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:03.778040", "author": null, "score": null}
{"title": "Using Local LLMs to Discover High-Performance Algorithms", "url": "https://towardsdatascience.com/using-local-llms-to-discover-high-performance-algorithms/", "content": "Using Local LLMs to Discover High-Performance Algorithms. How I used open-source models to explore new frontiers in efficient code generation, using my MacBook and local LLMs Ever since I was a child, I’ve been fascinated by drawing. What struck me was not only the drawing act itself, but also the idea that every drawing could be improved more and more. I remember reaching very high levels with my drawing style. However, once I reached the peak of perfection, I would try to see how I could improve the drawing even further – alas, with disastrous results. From there I always keep in mind the same mantra: “refine and iterate and you’ll reach perfection”. At university, my approach was to read books many times, expanding my knowledge searching for other sources, for finding hidden layers of meaning in each concept. Today, I apply this same philosophy to AI/ML and coding. We know that matrix multiplication (matmul for simplicity here), is the core part of any AI process. Back in the past I developedLLM.rust, a Rust mirror of Karpathy’sLLM.c. The hardest point in the Rust implementation has been the matrix multiplication. Since we have to perform thousands of iterations for fine-tuning a GPT-based model, we need an efficient matmul operation. For this purpose, I had to use the BLAS library, implementing anunsafestrategy for overcoming the limits and barriers. The usage ofunsafein Rust is against Rust’s philosophy, that’s why I am always looking for safer methods for improve matmul in this context. So, taking inspiration from Sam Altman’s statement – “ask GPT how to create value” – I decided to ask local LLMs to generate, benchmark, and iterate on their own algorithms to create a better, native Rust matmul implementation. The challenge has some constraints: I know that achieving BLAS-level performances with this method is almost impossible, but I want to highlight how we can leverage AI for custom needs, even with our “tiny” laptops, so that we can unblock ideas and push boundaries in any field. This post wants to be an inspiration for practitioners, and people who want to get more familiar with Microsoft Autogen, and local LLM deployment. All the cod implementation can be found in thisGithub repo. This is an on-going experiment, and many changes/improvements will be committed. The overall idea is to have a roundtable of agents. The starting point is theMrAderMacher Mixtral 8x7B model Q4 K_Mlocal model. From the model we create 5 entities: The overall workflow can be orchestrated through Microsoft Autogen as depicted in fig.1.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:04.956115", "author": null, "score": null}
{"title": "Time Series Isn’t Enough: How Graph Neural Networks Change Demand Forecasting", "url": "https://towardsdatascience.com/time-series-isnt-enough-how-graph-neural-networks-change-demand-forecasting/", "content": "Time Series Isn’t Enough: How Graph Neural Networks Change Demand Forecasting. Why modeling SKUs as a network reveals what traditional forecasts miss Demand forecastingin supply-chain planning has traditionally been treated as a time-series problem. And yet, despite increasingly sophisticated models, the usual problems persist: The issue is that demand in a supply chain is not independent. It is networked. As an example, this is what just 12 SKUs from a typical supply chain look like when you map their shared plants, product groups, subgroups, and storage locations. So when demand shifts in one corner of the network, the effects are felt throughout the network. In this article, we step outside the model-first thinking and look at the problem the way a supply chain actually behaves — as a connected operational system. Using a real FMCG dataset, we show why even a simplegraph-based neural network(GNN)fundamentally outperforms traditional approaches, and what that means for both business leaders and data scientists. We tested this idea on a real FMCG dataset (SupplyGraph) that combines two views of the business: The dataset has 40 active SKUs, 9 plants, 21 product groups, 36 sub-groups and 13 storage locations. On average, each SKU has~41 edge connections, implying a densely connected graph where most SKUs are linked to many others through shared plants or product groups.. From a planning standpoint, this network encodes institutional knowledge that often lives only in planners’ heads: “If this SKU spikes, these others will feel it.”", "source": "TowardsDataScience", "date": "2026-01-21T10:22:06.201462", "author": null, "score": null}
{"title": "The Hidden Opportunity in AI Workflow Automation with n8n for Low-Tech Companies", "url": "https://towardsdatascience.com/the-hidden-opportunity-in-ai-workflow-automation-with-n8n-for-low-tech-companies/", "content": "The Hidden Opportunity in AI Workflow Automation with n8n for Low-Tech Companies. How to use n8n with multimodal AI and optimisation tools to help companies with low data maturity accelerate their digital transformation. Every day on professionalsocial media, someone claims their “AI agent” will run your entire business while you sleep.It is as if they can deploy AGI across factories, finance teams, and customer service using their “secret” n8n template. My reality check is that many companies are still struggling to collect and harmonise data to follow basic performance metrics. Logistics Director: “I don’t even know how many orders have been delivered late, what do you think your AI agent can do?” And these advertised AI workflows, which are often not ready for production, can unfortunately do nothing to help with that. Therefore, I adopt a more pragmatic approach for our supply chain projects. Instead of promising an AGI that will run your entire logistics operations, let us start with local issues hurting a specific process. Logistics Director: “I want our operators to get rid of papers and pens for order preparation and inventory cycle count.” Most of the time, it involves data extraction, repetitive data entry, and heavy admin work using manual processes that are inefficient and lack traceability. For example, a customer was using paper-based processes to organise inventory cycle counts in its warehouse.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:07.445574", "author": null, "score": null}
{"title": "Why Healthcare Leads in Knowledge Graphs", "url": "https://towardsdatascience.com/why-healthcare-leads-in-knowledge-graphs/", "content": "Why Healthcare Leads in Knowledge Graphs. How science, regulation, collaboration, and public funding shaped the world’s most mature semantic infrastructure Note 1: This post is part 2 of a three-part series on healthcare, knowledge graphs, and lessons for other industries. Part 1, “What Is a Knowledge Graph — and Why It Matters” is availablehere. Note 2: All images by author In Part 1, we described how structured knowledge enabled healthcare’s progress. This article examines why healthcare, more than any other industry, was able to build that structure at scale. Healthcare is the most mature industry in the use of knowledge graphs for a few fundamental reasons. At its core, medicine is grounded in empirical science (biology, chemistry, pharmacology) which makes it possible to establish a shared understanding of the types of things that exist, how they interact, and causality. In other words, healthcare lends itself naturally toontology. The industry also benefits from a deep culture of sharedcontrolled vocabularies. Scientists and clinicians are natural librarians. By necessity, they meticulously list and categorize everything they can find, from genes to diseases. This emphasis on classification is reinforced by a commitment to empirical, reproducibleobservation, where data must be comparable across institutions, studies, and time. Finally, there are structural forces that have accelerated maturity: strictregulation; strong pre-competitivecollaboration; sustainedpublic funding; andopen data standards. All of these factors incentivize shared standards and reusable knowledge rather than isolated, proprietary models. Together, these factors created the conditions for healthcare to build durable, shared semantic infrastructure—allowing knowledge to accumulate across institutions, generations, and technologies. Humans have always tried to understand how the world works. When we observe and report the same thing repeatedly, and agree that it is true, we develop a shared understanding of reality. This process is formalized in science using the scientific method. Scientists develop a hypothesis, conduct an experiment, and evaluate the results empirically. In this way, humans have been developing an implicit medical ontology for thousands of years. Otzi, the caveman discovered in 1991, who lived 5,300 years ago, was discovered with an antibacterial fungus in his leggings, likely to treat his whipworm infection (Kirsch and Ogas 4). Even cavemen had some understanding that plants could be used to treat ailments.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:08.626055", "author": null, "score": null}
{"title": "Data Poisoning in Machine Learning: Why and How People Manipulate Training Data", "url": "https://towardsdatascience.com/data-poisoning-in-machine-learning-why-and-how-people-manipulate-training-data/", "content": "Data Poisoning in Machine Learning: Why and How People Manipulate Training Data. Do you know where your data has been? Data is a sometimesoverlooked but hugely vital part of enabling machine learning and therefore AI to function. Generative AI companies are scouring the world for more data constantly because this raw material is required in great volumes for models to be built. Anyone who’s building or tuning a model must first collect a significant amount of data to even begin. Some conflicting incentives result from this reality, however. Protecting the quality and authenticity of your data is an important component of security, because these raw materials will make or break the machine learning models you are serving to users or customers. Bad actors can strategically insert, mutate, or remove data from your datasets in ways you may not even notice, but which will systematically alter the behavior of your models. Simultaneously, creators such as artists, musicians, and authors are fighting an ongoing battle againstrampant copyright violation and IP theft, primarily by generative AI companiesthat need to find more data to toss into the voracious maw of the training process. These creators are looking for action they can take to prevent or discourage this theft that doesn’t just require being at the mercy of often slow moving courts. Additionally, as companies do their darndest to replace traditional search engines with AI mediated search, companies whose businesses are founded on being surfaced through search are struggling. How do you access customers and present your desired brand identity to the public if the investments you made in search visibility over past decades are no longer relevant? All three of these cases point us to one concept — “data poisoning”. In short,data poisoning is changing the training data used to produce a machine learning model in some way so that the model behavior is altered.The impact is specific to the training process, so once a model artifact is created, the damage is done. The model will be irreparably biased, potentially to the point of being useless, and the only real solution is retraining with clean data. This phenomenon is a danger for automatic retraining, where human observation is minimal, but also for very well observed training becauseusually the changes to the training data are invisible to the average viewer. For example, in one study cited byHartle et al. (2025)in relation to poisoned medical misinformation data, “Fifteen clinicians were tasked with determining the poisoned response and the baseline response; the reviewers were unable to determine the difference between the two results… When the concept-specific data was poisoned, at 0.001%, there was a 4.8% increase in harmful content.” Attempting to reverse-engineer the poisoned data and remove it has largely not been successful. Techniques under the umbrella of“machine unlearning”have been attempted, but when we can’t detect the problematic data, it’s difficult for these efforts to make progress. Even when we can detect the data, researchers find thatremoving traces from a model’s architecture is not effective at undoing the damage. Data poisoning can take a lot of different forms, so I’m going to work backwards and discuss three specific motives for data poisoning, how they work, and what their results are:", "source": "TowardsDataScience", "date": "2026-01-21T10:22:09.878849", "author": null, "score": null}
{"title": "Terms of Use", "url": "https://towardsdatascience.com/website-terms-of-use/", "content": "Terms of Use. Last Modified: February 7, 2025 Acceptance of the Terms of Use These terms of use are entered into by and between you and Insight Media Group, LLC (“TDS,” “we,” or “us” or “our”). The following terms and conditions, together with any documents they expressly incorporate by reference (collectively, “Terms of Use“), govern your access to and use ofhttps://towardsdatascience.com, including any content, functionality, and services offered on or throughhttps://towardsdatascience.com(the “Website“), whether as a guest or a registered user. Please read these Terms of Use carefully before you start to use the Website. By using the Website or by clicking to accept or agree to the Terms of Use when this option is made available to you, you accept and agree to be bound and abide by these Terms of Use  and our Privacy Policy, found athttps://towardsdatascience.com/privacy-policy/,and incorporated herein by reference. In the event of a conflict between these Terms of Use and the Privacy Policy, the Privacy Policy shall govern. If you do not want to agree to these Terms of Use or the Privacy Policy, you must not access or use the Website. This Website is offered and available to users who are 13 years of age or older. If you are under the age of 13, you may not access or use the Website. Changes to the Terms of Use TDS may revise and update these Terms of Use from time to time in our sole discretion. All changes are effective immediately when we post them and apply to all access to and use of this Website thereafter. Your continued use of the Website following the posting of revised Terms of Use means that you accept and agree to the changes. You are expected to check this page from time to time so you are aware of any changes, as they are binding on you.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:11.043224", "author": null, "score": null}
{"title": "A Geometric Method to Spot Hallucinations Without an LLM Judge", "url": "https://towardsdatascience.com/the-red-bird/", "content": "A Geometric Method to Spot Hallucinations Without an LLM Judge. How geometry shows when LLMs are lying Imagine a flockof birds in flight. There’s no leader. No central command. Each bird aligns with its neighbors—matching direction, adjusting speed, maintaining coherence through purely local coordination. The result is global order emerging from local consistency. Now imagine one bird flying with the same conviction as the others. Its wingbeats are confident. Its speed is correct. But its direction doesn’t match its neighbors. It’s the red bird. It’s not lost. It’s not hesitating. It simply doesn’t belong to the flock. Hallucinations in LLMs are red birds. LLMs generate fluent, confident text that may contain fabricated information. They invent legal cases that don’t exist. They cite papers that were never written. They state facts with the same tone whether those facts are true or completely made up. The standard approach to detecting this is to ask another language model to check the output. LLM-as-judge. You can see the problem immediately: we’re using a system that hallucinates to detect hallucinations. It’s like asking someone who can’t distinguish colors to sort paint samples. They’ll give you an answer. It might even be right sometimes. But they’re not actually seeing what you need them to see. The question we asked was different:can we detect hallucinations from the geometric structure of the text itself, without needing another language model’s opinion? Before getting to the detection method, I want to step back and establish what we’re working with.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:14.994614", "author": null, "score": null}
{"title": "Cutting LLM Memory by 84%: A Deep Dive into Fused Kernels", "url": "https://towardsdatascience.com/cutting-llm-memory-by-84-a-deep-dive-into-fused-kernels/", "content": "Cutting LLM Memory by 84%: A Deep Dive into Fused Kernels. Why your final LLM layer is OOMing and how to fix it with a custom Triton kernel. If you’ve ever trainedor fine-tuned an LLM, you’ve likely hit a wall at the very last step: theCross-Entropy Loss. The culprit is thelogit bottleneck. To predict the next token, we project a hidden state into a massive vocabulary space. For Llama 3 (128,256 tokens), the weight matrix alone is over525 million parameters. While that’s only ~1GB inbfloat16, the intermediate logit tensor is the real issue. For large batches, it can easily exceed80GBof VRAM just to compute a single scalar loss. Optimising this layer is how libraries like Unsloth and Liger-Kernel achieve such massive memory reductions. In this article, we’ll build a fusedLinear + Cross Entropykernel from scratch in Triton. We will derive the math and implement a tiled forward and backward pass that slashes peak memory usage by84%. Note on Performance:This implementation is primarilyeducational. We prioritise mathematical clarity and readable Triton code by using global atomic operations. While it solves the memory bottleneck, matching production-grade speeds would require significantly more complex implementations which are out of scope for this article. This post is part of my Triton series. We’ll be using concepts liketilingandonline softmaxthat we’ve covered previously. If those sound unfamiliar, I recommend catching up there first! To get us started, let’s put some more numbers on the logit bottleneck. We consider an input matrixXwith shape[NxD], a weight matrixWwith shape[DxV]and a logit matrixY=X@Wwith shape[NxV]. In the context of an LLM,Nwould be the sequence length multiplied by the batch size (i.e. the total number of tokens in the batch),Dthe size of the hidden state andVthe vocabulary size. For a Llama3 8B model, we would have a context window of 8192 tokens, a hidden state with 4096 dimensions and a vocabulary size of 128,256 tokens. Using a modest batch size of 8, we getN = 8192x8 = 65,536. This results in theYmatrix having shape[NxV]=[65,536x128,256], or roughly8.4 billionelements. Inbfloat16, this would take up16.8GBof memory. However, if we follow best practices and usefloat32for the loss calculation to ensure numerical stability, the requirements double to33.6GB. To put this number in perspective, we would also need around16GBof memory to hold the weights of Llama3 8B in memory inbfloat16. One most GPUs, this leaves no space for the massive overhead of the optimiser states (e.g.Adam’s moments) and other activations, resulting in the infamous PyTorch OOM error.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:16.299783", "author": null, "score": null}
{"title": "From RGB to Lab: Addressing Color Artifacts in AI Image Compositing", "url": "https://towardsdatascience.com/from-rgb-to-lab-addressing-color-artifacts-in-ai-image-compositing/", "content": "From RGB to Lab: Addressing Color Artifacts in AI Image Compositing. A multi-tier approach to segmentation, color correction, and domain-specific enhancement While backgroundreplacement is a staple of image editing, achieving production-grade results remains a significant challenge for developers. Many existing tools work like “black boxes,” which means we have little control over the balance between quality and speed needed for a real application. I ran into these difficulties while buildingVividFlow. The project is mainly focused on Image-to-Video generation, but it also provides a feature for users to swap backgrounds using AI prompts. To make the system more reliable across different types of images, I ended up focusing on three technical areas that made a significant difference in my results: These are the approaches that worked for me when I deployed the app on HuggingFace Spaces. In this article, I want to share the logic and some of the math behind these choices, and how they helped the system handle the messy variety of real-world images more consistently. Standard RGB alpha blending tends to leave a stubborn visual mess in background replacement. When you blend a portrait shot against a colored wall into a new background, the edge pixels usually hold onto some of that original color. This is most obvious when the original and new backgrounds have contrasting colors, like swapping a warm yellow wall for a cool blue sky. You often end up with an unnatural yellowish tint that immediately gives away the fact that the image is a composite.This is why even when your segmentation mask is pixel-perfect, the final composite still looks obviously fake — the color contamination betrays the edit. The issue is rooted in how RGB blending works. Standard alpha compositing treats each color channel independently, calculating weighted averages without considering how humans actually perceive color.To see this problem concretely, consider the example visualized in Figure 1 below.Take a dark hair pixel (RGB 80, 60, 40) captured against a yellow wall (RGB 200, 180, 120). During the photo shoot, light from that wall reflects onto the hair edges, creating a color cast. If you apply a 50% blend with a new blue background in RGB space, the pixel becomes a muddy average (RGB 140, 120, 80) that preserves obvious traces of the original yellow—exactly the yellowish tint problem we want to eliminate. Instead of a clean transition, this contamination breaks the illusion of natural integration. As demonstrated in the figure above, the middle panel shows how RGB blending produces a muddy result that retains the yellowish tint from the original wall. The rightmost panel reveals the solution: switching to Lab color space before the final blend allows surgical removal of this contamination. Lab space separates lightness (L channel) from chroma (a and b channels), enabling targeted corrections of color casts without disturbing the luminance that defines object edges. The corrected result (RGB 75, 55, 35) achieves natural hair darkness while eliminating yellow influence through vector operations in the ab plane, a mathematical process I’ll detail in Section 5. The background replacement pipeline orchestrates several specialized components in a carefully designed sequence that prioritizes both robustness and efficiency. The architecture ensures that even when individual models encounter challenging scenarios, the system gracefully degrades to alternative approaches while maintaining output quality without wasting GPU resources. Following the architecture diagram, the pipeline executes through six distinct stages: Image Preparation: The system resizes and normalizes input images to a maximum dimension of 1024 pixels, ensuring compatibility with diffusion model architectures while maintaining aspect ratio.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:17.484145", "author": null, "score": null}
{"title": "The Great Data Closure: Why Databricks and Snowflake Are Hitting Their Ceiling", "url": "https://towardsdatascience.com/the-great-data-closure-why-databricks-and-snowflake-are-hitting-their-ceiling/", "content": "The Great Data Closure: Why Databricks and Snowflake Are Hitting Their Ceiling. Acquisitions, venture, and an increasingly competitive landscape all point to a market ceiling How big cana data company really grow? This week what would have been news a year ago was no longer news.Snowflake invested in AtScale, a provider of semantic layer services in a strategic investment in the waning company’s history. An odd move, given the commitment to theopen semantic interchangeor “OSI” (yet another acronym or .yaa) which appears to bemetricflow masqueradingas something else. Meanwhile, Databricks, the AI and Data company,invested in AI-winner and all-round VC paramore Loveable— the rapidly growing vibe-coding company from Sweden. Starting a venture arm is a tried-and-tested route for enterprises. Everybody from Walmart and Hitachi to banks like JPMorgan and Goldman Sachs, and of course the hyperscalers — MSFT, GOOG — themselves have venture arms (though strangelynot AWS). The benefits are clear. An investment into a round can give the right of first refusal. It offers both parties influence around complementary roadmap features as well as clear distribution advantages. “Synergy” is the word used in boardrooms, though it is the less insidious and friendly younger brother ofcentral cost cuttingso prevalent in PE rather than venture-backed businesses. It should therefore come as no surprise to see that Databricks are branching out outside of Data. After all (and Ali has been very open about this), the team understands the way to grow the company is through new use cases, most notably AI. WhileDolly was a flop, the jury is out on thepartnership with OpenAI. AI/BI, as well as Databricks Applications, are promising initiatives designed to bring more friends into the tent — outside of the core SYSADMIN cluster administrators. Snowflake meanwhile may be trying a similar tack but with differing levels of success. Aside from Streamlit, it is not clear what value its acquisitions are truly bringing.Openflow, Neolithic Nifi under-the-hood, is not well received. Rather, it is the internal developments such as the embedding of dbt core into the Snowflake platform that appear to be gaining more traction. In this article, we’ll dive into the different factors at play and make some predictions for 2026. Let’s get stuck in! Databricks has a problem. A big problem. And that is equity.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:18.710706", "author": null, "score": null}
{"title": "TDS Newsletter: Is It Time to Revisit RAG?", "url": "https://towardsdatascience.com/tds-newsletter-is-it-time-to-revisit-rag/", "content": "TDS Newsletter: Is It Time to Revisit RAG?. Let's make sense of the current state of retrieval-augmented generation Never miss a new edition ofThe Variable, our weekly newsletter featuring a top-notch selection of editors’ picks, deep dives, community news, and more. It’s very difficult to tell what phase of the hype cycle we are in for any givenAI tool. Things are moving fast: a concept that just weeks ago seemed cutting edge can now appear stale, while an approach that was headed towards obsolescence might suddenly make a comeback.Retrieval-augmented generation is an interesting case in point. It dominated conversations a couple of years ago, quickly attracted a vocal crowd of skeptics, splintered into multiple types and flavors, and inspired a cottage industry of enhancements. These days, it seems to have landed somewhere midway between exciting and mundane. It’s a technique used by millions of practitioners, but no longer producing endless buzz. To help us make sense of the current state of RAG, we turn to our expert authors, who cover some of its current challenges, use cases, and recent innovations. We begin our exploration withSarah Schürch‘s enlightening and detailed look into chunking—the process of splitting longer documents into shorter, more easily digestible ones—and its potential effects on the retrieval step in your LLM pipelines. Can we apply the power of RAG beyond text? Sara Nobrega introduces us to the emerging idea of retrieval-augmented forecasting for time-series data. How complex should your RAG systemsactuallybe? Ida Silfverskiöld presents her latest testing, aiming to find the right balance between performance, latency, and cost. Catch up with three articles that resonated with a wide audience in the past few days. We hope you explore some of our other recent must-reads on a diverse range of topics.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:19.920254", "author": null, "score": null}
{"title": "When Shapley Values Break: A Guide to Robust Model Explainability", "url": "https://towardsdatascience.com/when-shapley-values-break-a-guide-to-robust-model-explainability/", "content": "When Shapley Values Break: A Guide to Robust Model Explainability. Shapley Values are one of the most common methods for explainability, yet they can be misleading. Discover how to overcome these limitations to achieve better insights. Explainability in AI is essential for gaining trust in model predictions and is highly important for improving model robustness. Good explainability often acts as a debugging tool, revealing flaws in the model training process. While Shapley Values have become the industry standard for this task, we must ask: Do they always work? And critically, where do they fail? To understand where Shapley values fail, the best approach is to control the ground truth. We will start with a simple linear model, and then systematically break down the explanation. By observing how Shapley values react to these controlled changes, we can precisely identify exactly where they yield misleading results and how to fix them. We will start with a model with 100 uniform random variables. In this straightforward example, where all variables are independent, the calculation simplifies dramatically. Recall that the Shapley formula is based on themarginal contributionof each feature, the difference in the model’s output when a variable is added to a coalition of known features versus when it is absent. \\[ V(S∪{i}) – V(S)\\] Since the variables are independent, the specific combination of pre-selected features (S) does not influence the contribution of feature i. The effect of pre-selected and non-selected features cancel each other out during the subtraction, having no impact on the influence of feature i. Thus, the calculation reduces to measuring the marginal effect of feature i directly on the model output: \\[ W_i · X_i \\] The result is both intuitive and works as expected. Because there is no interference from other features, the contribution depends solely on the feature’s weight and its current value. Consequently, the feature with the largest combination of weight and value is the most contributing feature. In our case, feature index 0 has a weight of 10 and a value of 1.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:21.170775", "author": null, "score": null}
{"title": "How to Run Coding Agents in Parallel", "url": "https://towardsdatascience.com/how-to-run-coding-agents-in-parallell/", "content": "How to Run Coding Agents in Parallel. Get the most out of Claude Code In the last few years, coding agents have become more and more prevalent. Initially, coding agents could only auto-complete specific lines of code. We then experienced how agents could interact with a single file and make changes to entire functions. After this, we started seeing agents capable of keeping track of and updating code in multiple files. Now, coding agents are extremely capable and can work across multiple code repositories, even implementing entire features with no need for human intervention. The capabilities of coding agents have opened up a whole new world of productivity for software engineers. In this article, I’ll highlight how coding agents have increased my productivity as an engineer, and how I leverage coding agents maximally by running multiple in parallel. I aim to create a high-level overview of what coding agents can do for you and the techniques I utilize to get the most out of my coding agents by running them in parallel. Just a year ago, it was almost unthinkable that you could be programming on multiple projects at the same time. Programming was known as a very high cognitive effort activity, where you had to minimize context switching. If you want to take full advantage of coding agents, you need to run them in paralell. And if you’re not taking full advantage of coding agents, you’re falling behind I still recommend minimizing context switching. However, the capabilities of coding agents have gotten so far that if you don’t run multiple in parallel, you’re falling behind. When spinning up a coding agent, you usually start it on a task by giving it some directions and asking a few questions. After this, however, the agents start working, and it can take 5-20 minutes before you need to interact with the agent again. Instead of waiting for this long, you spin up another coding agent. You can then continue this cycle of spinning up new agents until you have to interact with the first agent again. Simply put, the reason you should run multiple agents in parallel is that this is the way to achieve maximum effectiveness as a software engineer. You could, for example, look at the creator of Claude Code, Boris Cherny, on X.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:22.353865", "author": null, "score": null}
{"title": "The 2026 Goal Tracker: How I Built a Data-Driven Vision Board Using Python, Streamlit, and Neon", "url": "https://towardsdatascience.com/the-2026-goal-tracker-how-i-built-a-data-driven-vision-board-using-python-streamlit-and-neon/", "content": "The 2026 Goal Tracker: How I Built a Data-Driven Vision Board Using Python, Streamlit, and Neon. Designing a centralized system to track daily habits and long-term goals Have you ever wonderedhow to actually stay consistent with your goals for 2026? This year, I’ve decided that I don’t just want a list of goals. I want a vision board backed by real metrics to track my progress month after month. The problem I’ve been facing these last few years is fragmentation. There are a million apps out there to help you track finance, training, or daily habits, but I could never find a single, centralized tracker. Even harder was finding something that could scale: a system that follows a goal whether it’s daily, weekly, monthly, quarterly or yearly. For this reason, I decided to build my own goal tracker. This app is just one example of what works well for me, but the intention goes beyond this specific implementation. The goal is to share the product thinking behind it: how to design a system that aligns metrics, visuals, and structure in a way that actually supports short and long-term goals. Before jumping into the code, it’s important to understand the design decisions behind the app. In reality, our ambition operates on different scales. Most trackers fail because they focus on a single resolution (often tracking daily habits). In my case, I needed a system that could support different frequencies of goals so i categorized my objectives into 2 categories: The app I designed was meant to capture all of these frequencies in a single system. This makes it possible to monitor execution on a daily basis, but also maintain an overview of progress throughout the whole year. When it came to the interface, I deliberately avoided complexity. I’m not a UI expert, and I didn’t want an app filled with buttons, menus, or unnecessary interactions. Instead, I chose a grid-based matrix. This allows to simply check boxes for habits or completed goals. In data visualization, an empty cell is just as informative as a filled one. Seeing gaps in the grid becomes a powerful and very concrete signal. It immediately shows where consistency is missing and helps adjusting. For this project, I had two important requirements for the architecture:", "source": "TowardsDataScience", "date": "2026-01-21T10:22:23.601498", "author": null, "score": null}
{"title": "Do You Smell That? Hidden Technical Debt in AI Development", "url": "https://towardsdatascience.com/do-you-smell-that-hidden-technical-debt-in-ai-development/", "content": "Do You Smell That? Hidden Technical Debt in AI Development. Why speed without standards creates fragile AI products Not everybody can“smell” them at first. In practice,code smellsare warning signs that suggest future problems. The code may work today, but its structure hints that it will become hard to maintain, test, scale, or secure. Smells arenot necessarily bugs; they’re indicators of design debt and long-term product risk. These smells typically manifest as slower delivery and higher change risk, more frequent regressions and production incidents, and less reliable AI/ML outcomes, often driven by leakage, bias, or drift that undermines evaluation and generalization. Most phases in the development of data/AI products can vary, but they usually follow a similar path. Typically, we start with a prototype: an idea first sketched, followed by a small implementation to demonstrate value. Tools likeStreamlit,Gradio, orn8ncan be used to present a very simple concept usingsynthetic data. In these cases, you avoid using sensitive real data andreduce privacy and security concerns, especially in large,privacy‑sensitive, orhighly regulated companies. Later, you move to the PoC, where you use a sample of real data and go deeper into the features while working closely with the business. After that, you move towardproductization, building anMVPthat evolves as you validate and capture business value. Most of the time, prototypes and PoCs are built quickly, and AI makes it even faster to deliver them. The problem is that this code rarely meets production standards. Before it can be robust, scalable, and secure, it usually needs refactoring acrossengineering(structure, readability, testing, maintainability),security(access control, data protection, compliance), andML/AI quality(evaluation, drift monitoring, reproducibility). This hidden technical debt (often visible as code smells) is easy to overlook when teams chasequick wins, and “vibe coding” can amplify it. As a result, you can run into issues such as: And the list goes on… and on. The problem isn’t that prototypes are bad. The problem is the gap between prototype speed and production responsibility,when teams, for one reason or another,don’t invest in the practices that make systems reliable, secure, and able to evolve. It’s also useful to extend the idea of “code smells” intomodel and pipeline smells: warning signs that the system may be producing confident but misleading results, even when aggregate metrics look great. Common examples includefairness gaps(subgroup error rates are consistently worse),spillover/leakage(evaluation accidentally includes future or relational information that won’t exist at decision time, generating dev/prod mismatch [7]), or/andmulticollinearity(correlated features that make coefficients and explanations unstable). These aren’t academic edge cases; they reliably predict downstream failures like weak generalization, unfair outcomes, untrustworthy interpretations, and painful production drops.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:24.854199", "author": null, "score": null}
{"title": "Terms of Use", "url": "https://towardsdatascience.com/website-terms-of-use/", "content": "Terms of Use. Last Modified: February 7, 2025 Acceptance of the Terms of Use These terms of use are entered into by and between you and Insight Media Group, LLC (“TDS,” “we,” or “us” or “our”). The following terms and conditions, together with any documents they expressly incorporate by reference (collectively, “Terms of Use“), govern your access to and use ofhttps://towardsdatascience.com, including any content, functionality, and services offered on or throughhttps://towardsdatascience.com(the “Website“), whether as a guest or a registered user. Please read these Terms of Use carefully before you start to use the Website. By using the Website or by clicking to accept or agree to the Terms of Use when this option is made available to you, you accept and agree to be bound and abide by these Terms of Use  and our Privacy Policy, found athttps://towardsdatascience.com/privacy-policy/,and incorporated herein by reference. In the event of a conflict between these Terms of Use and the Privacy Policy, the Privacy Policy shall govern. If you do not want to agree to these Terms of Use or the Privacy Policy, you must not access or use the Website. This Website is offered and available to users who are 13 years of age or older. If you are under the age of 13, you may not access or use the Website. Changes to the Terms of Use TDS may revise and update these Terms of Use from time to time in our sole discretion. All changes are effective immediately when we post them and apply to all access to and use of this Website thereafter. Your continued use of the Website following the posting of revised Terms of Use means that you accept and agree to the changes. You are expected to check this page from time to time so you are aware of any changes, as they are binding on you.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:26.047750", "author": null, "score": null}
{"title": "Why Human-Centered Data Analytics Matters More Than Ever", "url": "https://towardsdatascience.com/why-human-centered-data-analytics-matters-more-than-ever/", "content": "Why Human-Centered Data Analytics Matters More Than Ever. From optimizing metrics to designing meaning: putting people back into data-driven decisions We live in an erawhere beingdata-drivenhas become a badge of credibility. Organizations proudly talk about the dashboards, AI strategies, predictive models, and automation they have invested and reaped benefits from. As the internet would inform you, nearly every Fortune 1000 company is increasing its investment in data and AI to stay agile and competitive. And yet, despite the unprecedented access to the quality and quantity of data, a vast majority of analytics and AI initiatives do not make it to production or can’t make a lasting impact. Data models are created, insights are shared, decks are applauded and then quietly forgotten only to become (what I like to call) trashboards. In this day and age of machines taking over our decision-making capabilities, the problem isn’t a lack of data, talent, or tooling – it’s thehumanthat we are starting to forget to talk to. This is whereHuman-Centered Data Analyticsbecomes not just relevant, but essential. Data is nothing but the digital traces of human interactions. A human-centered approach can enhance the choices data scientists make every day, by making the process more transparent, asking questions, and considering the social context of the data. A human-centered approach asks a very simple question: Who is this for and how will it actually be used? Now think about it this way—from asking“What can we predict from this data?”, the human-centered approach makes us want to ask“What should we help people understand or decide with this data?” Human-Centered Data Analytics is the concept of understanding how people interact and make sense of social situations, enabling humans to explore and gain insights, and design data models with the end-user in mind (not just the business).", "source": "TowardsDataScience", "date": "2026-01-21T10:22:30.482100", "author": null, "score": null}
{"title": "What Is a Knowledge Graph — and Why It Matters", "url": "https://towardsdatascience.com/what-is-a-knowledge-graph-and-why-it-matters/", "content": "What Is a Knowledge Graph — and Why It Matters. How structured knowledge became healthcare’s quiet advantage Note 1: This post is part 1 of a three-part series on healthcare, knowledge graphs, and lessons for other industries Note 2: All images by author Imagine you’re livingin the first half of the 19th century, and you feel an almost paralyzing ache in your abdomen. You now have a choice. You learn to live with that pain for the rest of your life (which may only be weeks or months away depending on what’s causing that ache) or you venture to the doctor, a nightmarish experience potentially involving tortuous treatments like bloodletting, laxatives, induced vomiting, or downing vials of mercury (Hager 52). There is no knowledge about how diseases spread, so going into a crowded hospital could mean exposure to smallpox and cholera (Kirsch and Ogas 80). If you are unlucky enough to need surgery (or have a physician prescribe an unneeded one—again, there is almost no knowledge of disease pathways), there will be no anesthesia. Finding the best surgeon likely means finding the fastest one, who can work as rapidly as possible to minimize the time orderlies have to restrain you while you’re shrieking and writhing on the table. If you survive the surgery, you still have a significant chance of dying of an infection since there’s no knowledge of germ theory and so no aseptic techniques (Kirsch and Ogas 45). And if you’re a pregnant woman, you can expect the maternity ward to be even more fucked up.Nearly 15 percentof babies born in the UK in the mid-19th century died at birth. Compare that with the medical care provided in any developed country today, and let’s just say, we’ve come a long way. The infant mortality rate in developed countries is now less than 6 per 1,000 live births, or0.6 percent. The average life expectancy in developed countries is usuallyhigher than 80compared toabout 40in the mid-19th century. We have drugs or other treatments for almost all of the most common diseases, and humanity is curing more every day. The future looks even more promising, especially with the increasing capabilities of AI and the funding behind them. TheChan Zuckerberg Initiative (CZI), for example, aims to help scientists cure, prevent, or manage all diseases by the end of the 21st century. How has healthcare made this progress? And why does healthcare continue to attract disproportionate investment in AI today? It’s not simply better data; it’s better structure around knowledge. Long before computers, medicine began developing shared understandings of diseases and causal relationships, controlled vocabularies to catalog real-world entities, and data standards to ensure observations were empirical and replicable. Taken together, these frameworks form what we might now recognize as a knowledge graph. At a high level, knowledge graphs solve a recurring set of problems that become unavoidable as domains scale: Mature domain knowledge graphs in healthcare are the reason drugs can be designed to target specific diseases, why your doctor knows about the negative side effects of a drug in Japan even if it goes by a different name there, and why physicians can aggregate and learn from observations from millions of clinical encounters and experiments, often in real-time. In this three-part series, I hope to provide some context and insights around how knowledge graphs (and their precedents) have worked in healthcare, how healthcare became the industry leader in knowledge graphs, and share some potential lessons for other industries grappling with similar challenges.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:31.826541", "author": null, "score": null}
{"title": "Glitches in the Attention Matrix", "url": "https://towardsdatascience.com/glitches-in-the-attention-matrix-a-history-of-transformer-artifacts-and-the-latest-research-on-how-to-fix-them/", "content": "Glitches in the Attention Matrix. A history of Transformer artifacts and the latest research on how to fix them  Transformers have laidthe groundwork for foundation models, which allow us to take pretrained models off the shelf and apply them to a variety of tasks. However, there is a common artifact found in transformer models that can have detrimental impacts in specific tasks and scenarios. Not understanding these downfalls could cause your project to substantially underperform or fail. For example, theDINOv2’s GitHub pagehas models pretrained with and without registers. A table with metrics suggests that registers, which were introduced to fix this artifact, do not help the model in a meaningful way. And why add complexity if there isn’t an increase in accuracy? However, the metrics shown on the DINOv2’s page are only for ImageNet classification, which is known to not be impacted by these artifacts. If you use the DINOv2 ViT model without registers for object detection (like with LOST), your performance would likely be substantially worse. Using Pretrained ViT Models without understanding when high-norm artifacts could impact your project could result in your project failing. Since these artifacts were identified, the research community has developed several methods to address them. The latest solutions require little to no retraining and introduce zero additional test-time latency. These phenomena are not unique to ViTs, but also occur in LLMs. In fact, one of the NeurIPS 2025 papers reviewed here proposes a general solution to these “attention sink” artifacts — which modifies the self-attention transformer architecture. This modified architecture is shown to be beneficial in a multitude of ways and is already being incorporated into the latest Qwen model, Qwen3-Next. This article provides a comprehensive guide to: While ViTs have been pivotal in ushering in the era of foundation models for computer vision, they suffer from a persistent anomaly: the emergence of high-norm spikes1. These artifacts appear across both supervised and self-supervised training regimes, with the original DINO being a notable exception. In Figure 1, this is demonstrated on ViT Base models trained with different algorithms, spanning self-supervised (DINO/DINOv2, MAE), weakly supervised (CLIP), to supervised (DeiT-III). These artifacts exhibit four key characteristics: The impact on accuracy varies by task. We measure this impact by observing how much performance improves after applying the fixes discussed in later sections. A summary of results from Jiang et al. (2025)2is provided below:", "source": "TowardsDataScience", "date": "2026-01-21T10:22:33.032518", "author": null, "score": null}
{"title": "Topic Modeling Techniques for 2026: Seeded Modeling, LLM Integration, and Data Summaries", "url": "https://towardsdatascience.com/topic-modeling-techniques-for-2026-seeded-modeling-llm-integration-and-data-summaries/", "content": "Topic Modeling Techniques for 2026: Seeded Modeling, LLM Integration, and Data Summaries. Seeded topic modeling, integration with LLMs, and training on summarized data are the fresh parts of the NLP toolkit. By: Martin Feldkircher (Vienna School of International Studies), Márton Kardos (Aarhus University, Denmark), and Petr Koráb (Text Mining Stories) Topic modelling has recently progressed in two directions. The improvedstatistical methodsstream of Python packages focuses on more robust, efficient, and preprocessing-free models, producing fewer junk topics (e.g., FASTopic). The other relies on the power ofgenerative language modelsto extract intuitively understandable topics and their descriptions (e.g., TopicGPT[6], LlooM[5]). Thanks to research on statistical methods for modelling text representations from transformers, junk topics are the exception rather than the norm in newer models. Meanwhile, novel, LLM-based approaches are challenging our long-standing views about what a topic model is and what it can do.Human-readable topic namesanddescriptionsare now becoming more and more an expected result of a well-designed topic modelling pipeline. As exciting as these developments are, topic modelling is far from being a solved problem. Neural topic models can be ratherunstableand sometimes hard for users to trust because of theirblack-box nature. LLM-powered methods produce impressive results, but can at times raise questions about trust, due to hallucinations and sensitivity to semantically irrelevant changes in input. This is especially a problem for the banking sector, where (un)certainty is essential. Running large language models is also a huge infrastructural and computational burden, and might end up costing large sums of money even for smaller datasets. Our previous tutorialprovides a detailed introduction to how LLMs enhance traditional topic modeling by automatically labeling topic names. In this article, we combine current topic modeling methods with targeted LLM assistance. In our view, a combination of recent advances in language modeling and classical machine learning can provide users with the best of both worlds: a pipeline that combines the capabilities of large language models with the computational efficiency, trustworthiness, and stability of probabilistic ML. This article explains three fresh topic-modelling techniques that should be part of the NLP toolkit in 2026. We will figure out: We illustrate these on the central bank communication speeches corpus from the European Central Bank. This type of text is long, carefully structured, and highly repetitive — exactly the kind of data where standard topic models struggle and where interpretability is essential. By combining seeded topic modelling with LLM-assisted document summarization and analysis, we show how to extract focused, stable, and economically meaningful topics without compromising transparency or scalability. We use the press conference communications of the European Central Bank (ECB) as example text data. Since 2002, the ECB’s Governing Council has met on the first Thursday of each month, and its communication of the meeting’s outcome follows the two-step structure ([2]). How it works:First, at 13:45 CET, the ECB releases a brief monetary policy decision (MPD) statement, which contains only limited textual information. Second, at 14:30 CET, the ECB President delivers an introductory statement during a press conference. This carefully prepared document explains the rationale behind policy decisions, outlines the ECB’s assessment of economic conditions, and provides guidance on future policy considerations. The introductory statement typically lasts about 15 minutes and is followed by a 45-minute Q&A session.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:34.277235", "author": null, "score": null}
{"title": "An introduction to AWS Bedrock", "url": "https://towardsdatascience.com/an-introduction-to-aws-bedrock/", "content": "An introduction to AWS Bedrock. The how, why, what and where of Amazon’s LLM access layer As I write thisat the beginning of 2026, AWS has several related yet distinct components that make up its agentic and LLM abstractions. Apart from these three services, AWS also hasStrands,an open source Python library for building agents outside of the Bedrock service, which can then be deployed on other AWS services such as ECS and Lambda. It can be confusing because all three agentic-based services have the term “Bedrock” in their names, but in this article, I’ll focus on the standard Bedrock service and show how and why you would use it. standard Bedrock service and show how and why you would use it. As a service, Bedrock has only been available on AWS since early 2023. That should give you a clue as to why it was introduced. Amazon could clearly see the rise of Large Language Models and their impact on IT architecture and the systems development process. That’s AWS’s meat and potatoes, and they were keen that nobody was going to eat their lunch. And although AWS has developed a few LLMs of its own, it realised that to stay competitive, it would need to make the very top models, such as those from Anthropic, available to users. And that’s where Bedrock steps in. As they said in their own blurb on their website, … Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications, simplifying development while maintaining privacy and security. Ok, so that’s the theory behind the why of Bedrock, but how do we get access to it and actually use it? Not surprisingly, the first thing you need is an AWS account. I’m going to assume you already have this, but if not, click the following link to set one up. https://aws.amazon.com/account Usefully, after you register for a new AWS account, a good number of the services that you use will fall under the so-called “free tier” at AWS, which means your costs should be minimal forone yearfollowing your account creation – assuming you don’t go crazy and start firing up huge compute servers and such like.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:35.503288", "author": null, "score": null}
{"title": "From ‘Dataslows’ to Dataflows: The Gen2 Performance Revolution in Microsoft Fabric", "url": "https://towardsdatascience.com/from-dataslows-to-dataflows-the-gen2-performance-revolution-in-microsoft-fabric/", "content": "From ‘Dataslows’ to Dataflows: The Gen2 Performance Revolution in Microsoft Fabric. How the latest Dataflow enhancements change the way we play the game In the oceanof announcements from the recent FabCon Europe in Vienna, one that may have gone under the radar was about the enhancements in performance and cost optimization for Dataflows Gen2. Before we delve into explaining how these enhancements impact your current Dataflows setup, let’s take a step back and provide a brief overview of Dataflows. For those of you who are new to Microsoft Fabric — a Dataflow Gen2 is the no-code/low-code Fabric item used to extract, transform, and load the data (ETL). A Dataflow Gen2 provides numerous benefits: However, simplicity usually comes with a cost. In the case of Dataflows, the cost was significantly higher CU consumption compared to code-first solutions, such as Fabric notebooks and/or T-SQL scripts. This was already well-explained and examined in two great blog posts written by my fellow MVPs, Gilbert Quevauvilliers (Fourmoo):Comparing Dataflow Gen2 vs Notebook on Costs and usability, and Stepan Resl:Copy Activity, Dataflows Gen2, and Notebooks vs. SharePoint Lists, so I won’t waste time discussing the past. Instead, let’s focus on what the present (and future) brings for Dataflows! Let’s briefly examine what’s displayed in the illustration above. Previously, every second of the Dataflow Gen2 run was billed at 16 CU (CU stands for Capacity Unit, representing a bundled set of resources — CPU, memory, and I/O — used in synergy to perform a specific operation). Depending on the Fabric capacity size, you get a certain number of capacity units — F2 provides 2 CUs, F4 provides 4 CUs, and so on. Going back to our Dataflows scenario, let’s break this down by using a real-life example. Say you have a Dataflow that runs for 20 minutes (1200 seconds)… The longer your Dataflow needs to execute, the bigger the savings in CUs you potentially make. This is amazing on its own, but there is still more to that. I mean, it’s nice to be charged less for the same amount of work, but what if we could make these 1200 seconds, let’s say, 800 seconds? So, it wouldn’t save us just CUs, but also reduce the time-to-analysis, since the data would have been processed faster. And, that’s exactly what the next two enhancements are all about… The new preview feature, named Modern Evaluator, enables using the new query execution engine (running on .NET core version 8) for running Dataflows. As per theofficial Microsoft docs, Dataflows running the modern evaluator can provide the following benefits:", "source": "TowardsDataScience", "date": "2026-01-21T10:22:36.906008", "author": null, "score": null}
{"title": "Under the Uzès Sun: When Historical Data Reveals the Climate Change", "url": "https://towardsdatascience.com/under-the-uzes-sun-when-historical-data-reveals-the-climate-change/", "content": "Under the Uzès Sun: When Historical Data Reveals the Climate Change. Longer summers, milder winters: analysis of temperature trends in Uzès, France, year after year. Living in the South of France, I am biologically required to endure the same loop of small talk every year: “It’s boiling, isn’t it? Way hotter than 2020,” or the classic, “Back in my day, we actually had four seasons, not just ‘Pre-Oven’ and ‘Deep Fryer.’” Honestly, I’m tempted to nod along and complain too, but I have the memory of a goldfish and a brain that demands cold, hard facts before joining a rant. Since I can’t remember if last July was “sweaty” or “molten,” I’d love to have some actual data to back up my grumbling. I work aticCube. It’s basically a professional sin for me to get into a data-driven argument without bringing enterprise-level tooling to a back-of-the-napkin debate. At the nextapéro, when someone starts reminiscing about how “1976 was the real scorcher,” I shouldn’t just be nodding politely while nursing my pastis. I should be whipping out a high-performance, pixel-perfect dashboard that visualizes their nostalgia right into oblivion. If I can’t use multi-dimensional analysis to prove that our sweat glands are working harder than they did in the seventies, then what am I even doing with my life? While this journey began as a quest to settle a local argument in the South of France, this post goes beyond the climate debate. It serves as a blueprint for a classic data challenge: how to architect a high-performance analytical system capable of making sense of decades of historical data applicable to any domain requiring historical vs. current benchmarking. Here is the plan mapping out our tactical strike against vague nostalgia and anecdotal evidence: Data is central to our mission. Therefore, we need to secure accurate, high-fidelity historical temperature records from France. Méteo-France, the national meteorological and climatological service, is a public establishment of the State. It makes available to all users the data produced as part of its public service missions in its public data portal:datagouv.fr. God bless public data portals. While half the world’s data is locked behind paywalls and registration forms that ask for your blood type, France just… hands it over. Liberté, égalité, température. The data used in this post is made available under theOpen License 2.0.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:38.271902", "author": null, "score": null}
{"title": "Why Your ML Model Works in Training But Fails in Production", "url": "https://towardsdatascience.com/why-your-ml-model-works-in-training-but-fails-in-production/", "content": "Why Your ML Model Works in Training But Fails in Production. Hard lessons from building production ML systems where data leaks, defaults lie, populations shift, and time does not behave the way we expect. Early in my career, I worked on real-time fraud detection systems and recommendation models for product companies that looked excellent during development. Offline metrics were strong.AUC curveswere stable across validation windows. Feature importance plots told a clean, intuitive story. We shipped with confidence. A few weeks later, our metrics started to drift. Click-through rates on recommendations began to slide. Fraud models behaved inconsistently during peak hours. Some decisions felt overly confident, others oddly blind. The models themselves had not degraded. There were no sudden data outages or broken pipelines. What failed was our understanding of how the system behaved once it met time, latency, and delayed truth in the real world. This article is about those failures. The quiet, unglamorous problems that show up only when machine learning systems collide with reality. Not optimizer choices or the latest architecture. The problems that do not appear in notebooks, but surface at 3 a.m. dashboards. My message is simple: most production ML failures are data and time problems, not modeling problems. If you do not design explicitly for how information arrives, matures, and changes, the system will quietly make those assumptions for you. Time travel is the most common production ML failure I have seen, and also the least discussed in concrete terms. Everyone nods when you mention leakage. Very few teams can point to the exact row where it happened. Let me make it explicit. Imagine a fraud dataset with two tables: The feature we want isuser_chargeback_count_last_30_days.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:39.489229", "author": null, "score": null}
{"title": "How to Maximize Claude Code Effectiveness", "url": "https://towardsdatascience.com/how-to-maximize-claude-code-effectiveness/", "content": "How to Maximize Claude Code Effectiveness. Learn how to get the most out of agentic coding In this article, I’ll cover my experience on how you can get the most out of Claude Code. Claude Code is a powerful coding command-line interface (CLI) tool. You can open it directly in your terminal and start coding. This makes it an effective tool both for proficient coders familiar with a terminal. However, it also works well for non-technical people, since they can code simply by typing natural language, and don’t have to interact with a complicated IDE. In this article, I’ll cover why you should consider using Claude Code for coding, highlighting its effectiveness at implementing code with little to no manual review. Furthermore, I’ll cover some specific techniques I utilize to get the most out of Claude Code, before highlighting some limitations of Claude Code. I am not affiliated with Claude Code, and I’m simply writing this article from my experience using the tool. If you want to practice coding nowadays, you should embrace agentic coding. I think this applies whether you just started programming or if you’ve been programming for 10+ years. All of us should be leveraging coding agents to become more efficient engineers. Now you have a large variety of such tools to choose from. Claude Code is one of several CLI tools, with some other options being OpenAI Codex or Gemini CLI. These CLI tools are good if you don’t need to look at the code you’re developing, typically for simpler implementations or bug fixes. However, I am noticing more and more that my coding agents are able to implement larger scale implementation with fewer and fewer errors, which is noteworthy. I believe we’ll soon be in an era where coding agents are doing all coding (you could also argue we’re already there), and in such a scenario, CLI tools will be powerful, since you never have the need to look at any code. If you do want to look more at the code, however, you can consider more integrated agentic coding experiences, in tools such as Cursor or Antigravity, which are forks of VS Code, but with agentic functionality deeply integrated into the application. I like using both CLI’s and IDEs for agentic coding. If I want to code without performing manual reviews of the code, I tend to use Claude Code more and more. I think scenarios where you should strongly consider utilizing Claude Code are:", "source": "TowardsDataScience", "date": "2026-01-21T10:22:40.705772", "author": null, "score": null}
{"title": "How AI Can Become Your Personal Language Tutor", "url": "https://towardsdatascience.com/how-ai-can-become-your-personal-language-tutor/", "content": "How AI Can Become Your Personal Language Tutor. How I used n8n to build AI study partners for learning Mandarin: vocabulary, listening, and pronunciation correction. No one learnsa language by passively turning pages in a textbook. You really progress when the language talks back to you. When you see images, hear real sentences, try to speak, and get feedback, everything finally clicks in your head. In the past, you needed a teacher by your side at all times to get that kind of feedback. Today, generative AI can play that role on your phone or computer, like an AI language tutor you can use any time. When I started learning Mandarin ten years ago, I saw many foreigners struggling to be understood by locals in everyday conversations because of poor pronunciation. It convinced me that without good pronunciation, a rich vocabulary is useless. I still remember sitting in my apartment in Shanghai, repeating the same sentence again and again, without anyone to correct me. Years later, when I discovered generative AI, I remembered the engineer in China who was struggling with grammar books and tones.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:41.931873", "author": null, "score": null}
{"title": "Terms of Use", "url": "https://towardsdatascience.com/website-terms-of-use/", "content": "Terms of Use. Last Modified: February 7, 2025 Acceptance of the Terms of Use These terms of use are entered into by and between you and Insight Media Group, LLC (“TDS,” “we,” or “us” or “our”). The following terms and conditions, together with any documents they expressly incorporate by reference (collectively, “Terms of Use“), govern your access to and use ofhttps://towardsdatascience.com, including any content, functionality, and services offered on or throughhttps://towardsdatascience.com(the “Website“), whether as a guest or a registered user. Please read these Terms of Use carefully before you start to use the Website. By using the Website or by clicking to accept or agree to the Terms of Use when this option is made available to you, you accept and agree to be bound and abide by these Terms of Use  and our Privacy Policy, found athttps://towardsdatascience.com/privacy-policy/,and incorporated herein by reference. In the event of a conflict between these Terms of Use and the Privacy Policy, the Privacy Policy shall govern. If you do not want to agree to these Terms of Use or the Privacy Policy, you must not access or use the Website. This Website is offered and available to users who are 13 years of age or older. If you are under the age of 13, you may not access or use the Website. Changes to the Terms of Use TDS may revise and update these Terms of Use from time to time in our sole discretion. All changes are effective immediately when we post them and apply to all access to and use of this Website thereafter. Your continued use of the Website following the posting of revised Terms of Use means that you accept and agree to the changes. You are expected to check this page from time to time so you are aware of any changes, as they are binding on you.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:43.156919", "author": null, "score": null}
{"title": "Why 90% Accuracy in Text-to-SQL is 100% Useless", "url": "https://towardsdatascience.com/why-90-accuracy-in-text-to-sql-is-100-useless/", "content": "Why 90% Accuracy in Text-to-SQL is 100% Useless. The eternal promise of self-service analytics I have been working in the Analytics space for over 20 years. Back then, it was not called “analytics”, it was “Business Intelligence” or even “Decision Support Systems” in older times. The terms change, from data warehouses to Big Data, to lakehouses, and now with AI, the essence and the eternal promise of self-service Analytics remains the same: extracting truth from data to empower users without relying on someone from the data team. AI without humans in the loop? That sounds controversial. With the advent of Large Language Models (LLMs), one use case I find fascinating is developing conversational interfaces to chat with databases (Text-to-SQL). The potential here is immense, promising to democratize data access across organizations. However, for this specific use case, the solution has to be binary. It either works or it doesn’t. An accuracy of 80% or even 90% is, unfortunately, not enough. Giving your end-users an AI analytical application that hallucinates tables or misinterprets filters is no joke. You cannot compromise on accuracy because it immediately erodes trust. And what happens when a system loses trust? It will not be used. Adoption will decline, without forgetting the catastrophic risk of business decisions being made based on the wrong data. I started my research on this topic over one year and a half ago and it quickly became clear that orchestrating a robust Text-to-SQL RAG (Retrieval-Augmented Generation) application is not trivial. You need multiple components in your pipeline, working in perfect harmony: This last part, evaluation, I believe is often omitted or treated as an afterthought, but it is perhaps the most crucial component for ensuring the reliability needed in an enterprise setting. Managing this complex pipeline often requires integrating multiple platforms. I was recently impressed by how BigQuery has introduced the merger of Analytics and Generative AI natively in their platform. You have the ability to work with your SQL in the BigQuery IDE and use Gen AI immediately without going to another platform or product. For example: you can query the database and the retrieved results can be immediately sent to Gemini (or through Vertex you can also add other models). You can use Gemini to classify intent, create embeddings and store them in BigQuery’s vector database capabilities, do a semantic search, and generate SQL. All of that with only one platform, without the hassle of managing multiple subscriptions.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:47.152523", "author": null, "score": null}
{"title": "When Does Adding Fancy RAG Features Work?", "url": "https://towardsdatascience.com/when-does-adding-fancy-rag-features-work/", "content": "When Does Adding Fancy RAG Features Work?. Looking at the performance of different pipelines Last year I wrotean article about overengineering a RAG system, adding fancy things like query optimization, detailed chunking with neighbors and keys, along with expanding the context. The argument against this kind of work is that for some of these add-ons, you still end up paying 40–50% more in latency and cost. So, I decided totest both pipelines, onewith query optimisation and neighbor expansion, andone without. The first test I ran used simple corpus questions generated directly from the docs, and the results were lackluster. But then I continued testing it on messier questions and on random real-world questions, and it showed something different. This is what we’ll talk about here: where features like neighbor expansion can do well, and where the cost may not be worth it. We’ll go through the setup, the experiment design, three different evaluation runs with different datasets, how to understand the results, and the cost/benefit tradeoff. Please note that this experiment is using reference-free metrics and LLM judges, which you always have to be cautious about. You can see the entire breakdown inthisexcel document. If you feel confused at any time, there are two articles,hereandhere, that came before this one, though this one should stand on its own. People continue to add complexity to their RAG pipelines, and there is a reason for it. The overall design is flawed, so we keep patching on fixes to make something that is more robust.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:48.405641", "author": null, "score": null}
{"title": "Optimizing Data Transfer in Batched AI/ML Inference Workloads", "url": "https://towardsdatascience.com/optimizing-data-transfer-in-batched-ai-ml-inference-workloads/", "content": "Optimizing Data Transfer in Batched AI/ML Inference Workloads. A deep dive on data transfer bottlenecks, their identification, and their resolution with the help of NVIDIA Nsight™ Systems – part 2 Thisis asequel posttoOptimizing Data Transfer in AI/ML Workloadswhere we demonstrated the use ofNVIDIA Nsight™ Systems (nsys)in studying and solving the common data-loading bottleneck — occurrences where the GPU idles while it waits for input data from the CPU. In this post we focus our attention on data travelling in the opposite direction, from the GPU device to the CPU host. More specifically, we address AI/ML inference workloads where the size of the output being returned by the model is relatively high. Common examples include: 1) running a scene segmentation (per-pixel labeling) model on batches of high-resolution images and 2) capturing high dimensional feature embeddings of input sequences using an encoder model (e.g., to create avector database). Both examples involve executing a model on an input batch and then copying the output tensor from the GPU to the CPU for additional processing, storage, and/or over-the-network communication. GPU-to-CPU memory copies of the model output typically receive much less attention in optimization tutorials than the CPU-to-GPU copies that feed the model (e.g., seehere). But their potential impact on model efficiency and execution costs can be just as detrimental. Moreover, while optimizations to CPU-to-GPU data-loading are well documented and easy to implement, optimizing data copy in the opposite direction requires a bit more manual labor. In this post we will apply the same strategy we used in our previous post: We will define a toy model and use nsys profiler to identify and solve performance bottlenecks. We will run our experiments on anAmazon EC2 g6e.2xlargeinstance (with anNVIDIA L40SGPU) running anAWS Deep Learning (Ubuntu 24.04) AMIwithPyTorch (2.8),nsys-cli profiler(version 2025.6.1), and theNVIDIA Tools Extension(NVTX) library. The code we will share is intended for demonstrative purposes; please do not rely on its correctness or optimality. Please do not interpret our use of any library, tool, or platform, as an endorsement of its use. The impact of the optimizations we will cover can vary greatly based on the details of the model and the runtime environment. Please be sure to assess their effect on your own use case before integrating their use. Many thanks toYitzhak LeviandGilad Wassermanfor their contributions to this post. We introduce a batched inference script that performs image segmentation on a synthetic dataset using aDeepLabV3model with aResNet-50backbone. The model outputs are copied to the CPU for post processing and storage. We wrap the different portions of the inference step with color-codednvtxannotations: Note the inclusion of all of the CPU-to-GPU data-loading optimizations discussed in our previous post. We run the following command to capture an nsys profile trace: This results in abaseline.nsys-reptrace file that we copy over to our development machine for analysis.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:49.818309", "author": null, "score": null}
{"title": "Automatic Prompt Optimization for Multimodal Vision Agents: A Self-Driving Car Example", "url": "https://towardsdatascience.com/automatic-prompt-optimization-for-multimodal-vision-agents-a-self-driving-car-example/", "content": "Automatic Prompt Optimization for Multimodal Vision Agents: A Self-Driving Car Example. Walkthrough using open-source prompt optimization algorithms in Python to improve the accuracy of an autonomous vehicle car safety agent running on OpenAI's GPT 5.2 Multimodal AI agents, those that can process text and images (or other media), are rapidly entering real-world domains like autonomous driving, healthcare, and robotics. In these settings, we have traditionally used vision models like CNNs; in the post-GPT era, we can use vision and multimodal language models that leverage human instructions in the form of prompts, rather than task-oriented, highly specific vision models. However, ensuring good outcomes from the models requires effective instructions, or, more commonly, prompt engineering. Existing prompt engineering methods rely heavily on trial and error, and this is often exacerbated by the complexity and higher cost of tokens when working across non-text modalities such as images. Automatic prompt optimization is a recent advancement in the field that systematically tunes prompts to produce more accurate, consistent outputs. For example, a self-driving car perception system might use a vision-language model to answer questions about road images. A poorly phrased prompt can lead to misunderstandings or errors with serious consequences. Instead of fine-tuning and reinforcement learning, we can use another multimodal model with reasoning capabilities to learn and adapt its prompts. Although these automatic methods can be applied to text-based agents, they are often not well documented for more complex, real-world applications beyond a basic toy dataset, such as handwriting or image classification. To best demonstrate how these concepts work in a more complex, dynamic, and data-intensive setting, we will walk through an example using a self-driving car agent. Agent optimization is part of automatic prompt engineering, but it involves working with various parts of the agent, such as multi-prompts, tool calling, RAG, agent architecture, and various modalities. There are a number of research projects and libraries, such as GEPA; however, many of these tools do not provide end-to-end support for tracing, evaluating, and managing datasets, such as images. For this walk-through, we will be using the Opik Agent Optimizer SDK (opik-optimizer), which is an open-sourced agent optimization toolkit that automates this process using LLMs internally, along with optimization algorithms like GEPA and a variety of their own, such as HRPO, for various use cases, so you can iteratively improve prompts without manual trial-and-error. Essentially, an LLM can“act as”a prompt engineer and rewrite a given prompt. We start by taking the traditional approach, as a prompt engineer would with trial and error, and ask a small agent to review its work across a few examples, fix its mistakes, and create a new prompt. Meta Prompting is a classic example of using chain-of-thought reasoning (CoT), such as“explain the reason why you gave me this prompt”, during its new prompt generation process, and we keep iterating on this across multiple rounds of prompt generation. Below is an example of an LLM-based meta-prompting optimizer adjusting the prompt and generating new candidates. In the toolkit, there is a meta-prompt-based optimizer calledmetaprompter, and we can demonstrate how the optimization works:", "source": "TowardsDataScience", "date": "2026-01-21T10:22:51.008615", "author": null, "score": null}
{"title": "How to Leverage Slash Commands to Code Effectively", "url": "https://towardsdatascience.com/how-to-leverage-slash-commands-to-code-effectively/", "content": "How to Leverage Slash Commands to Code Effectively. Learn how I utilize slash commands to be a more efficient engineer Slash commandsare prompts you can store for your coding agent for easy access. This is typically very useful for prompts you use repeatedly, such as: These are all commands I run on a daily basis. Instead of typing out the prompts each time or storing the prompts somewhere, I can simply save them as slash commands in Claude Code or Warp. This gives me super-fast access to my most commonly used prompts, saving me a lot of time on a daily basis. In this article, I’ll discuss slash commands, what they are, and why they are so effective. Furthermore, I’ll cover some specific commands that I have saved and utilize regularly. Slash commands are simply easy-to-access prompts that are super useful if you find yourself running a lot of repetitive prompts. I believe that a lot of the prompts most programmers use will be repeated prompts. It can, for example ,be: These are all prompts you likely run on a regular basis. If so, you should make these prompts into slash commands. Instead of writing: You can simply store this as a command and write: With the recent advancements of coding agents, I find myself writing a lot more code and thus making a lot more pull requests. I therefore write this prompt anywhere from 2 to 10 times a day. The time writing out the prompt, therefore, adds up quickly, and I save a lot of time simply using the slash command instead. An additional benefit of using slash commands is that your commands will be consistent. You’ll always be running the same command, and never forget to write out parts of the command, for example, forgetting to run pre-commit checks. This is also a huge time-saver.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:52.401140", "author": null, "score": null}
{"title": "Federated Learning, Part 1: The Basics of Training Models Where the Data Lives", "url": "https://towardsdatascience.com/federated-learning-part-1-the-basics-of-training-models-where-the-data-lives/", "content": "Federated Learning, Part 1: The Basics of Training Models Where the Data Lives. Understanding the foundations of federated learning Ifirst came acrossthe concept of federated learning (FL) through acomic by Googlein 2019. It was a brilliant piece and did a great job at explaining how products can improve without sending user data to the cloud. Lately, I have been wanting to understand the technical side of this field in more detail. Training data has become such an important commodity as it is essential for building good models but a lot of this gets unused because it is fragmented, unstructured or locked inside silos. As I started exploring this field, I found theFlower frameworkto be the most straightforward and beginner-friendly way to get started in FL. It is open source, the documentation is clear, and the community around it is very active and helpful. It is one of the reason for my renewed interest in this field. This article is the first part of a series where I explore federated learning in more depth, covering what it is, how it is implemented, the open problems it faces, and why it matters in privacy-sensitive settings. In the next instalments, I will go deeper into practical implementation with theFlowerframework, discuss privacy in federated learning and examine how these ideas extend to more advanced use cases. We know AI models depend on large amounts of data, yet much of the most useful data is sensitive, distributed, and hard to access. Think of data inside hospitals, phones, cars, sensors, and other edge systems. Privacy concerns, local rules, limited storage, and network limits make moving this data to a central place very difficult or even impossible. As a result, large amounts of valuable data remain unused. In healthcare, this problem is especially visible. Hospitals generate tens of petabytes of data every year, yet studies estimate that up to97% of this data goes unused. Traditional machine learning assumes that all training data can be collected in one place, usually on a centralized server or data center. This works when data can be freely moved, but it breaks down when data is private or protected. In practice, centralised training also depends on stable connectivity, enough bandwidth, and low latency, which are difficult to guarantee in distributed or edge environments. In such cases, two common choices appear. One option is to not use the data at all, which means valuable information remains locked inside silos. The other option is to let each local entity train a model on its own data and share only what the model learns, while the raw data never leaves its original location. This second option forms the basis of federated learning, which allows models to learn from distributed data without moving it. A well-known example isGoogle Gboard on Android, where features like next-word prediction andSmart Composerun across hundreds of millions of devices. Federated learning can be thought of as a collaborative machine learning setup where training happens without collecting data in one central place. Before looking at how it works under the hood, let’s see a few real-world examples that show why this approach matters in high-risk settings, spanning domains from healthcare to security-sensitive environments. In healthcare, federated learning enabled earlyCOVID screening throughCurial AI, a system trained across multiple NHS hospitals using routine vital signs and blood tests. Because patient data could not be shared across hospitals, training was done locally at each site and only model updates were exchanged. The resulting global model generalized better than models trained at individual hospitals, especially when evaluated on unseen sites.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:53.719814", "author": null, "score": null}
{"title": "Beyond the Flat Table: Building an Enterprise-Grade Financial Model in Power BI", "url": "https://towardsdatascience.com/beyond-the-flat-table-building-an-enterprise-grade-financial-model-in-power-bi/", "content": "Beyond the Flat Table: Building an Enterprise-Grade Financial Model in Power BI. A step-by-step journey through data transformation, star schema modeling, and DAX variance analysis with lessons learned along the way. We’ve all beenthere: You open Power BI, drag a messy Excel sheet into the canvas, and start dropping charts until something looks “right.” It’s easy, it’s intuitive, and honestly, that’s why Power BI is one of my favourite tools for data visualisation. But as the world of data shifts toward end-to-end solutions likeMicrosoft Fabric, “just making it work” isn’t enough anymore. Large organisations need models that are performant, secure, and scalable. I’ve decided to challenge myself by taking the PL-300: Microsoft Data Analyst Associate exam. But instead of just grinding through practice tests or memorising definitions, I’m going into“Practical Mode.”If I’m going to get certified, I want to prove I can solve the problems real businesses actually face. For my first project, I’m tackling theExecutive Financial Health Suite. Why finance? Because in the enterprise world, it’s the ultimate test of yourData ModelingandDAXskills. Most “generic” tutorials use a single, flat table. But in a real company, data is fragmented. You have “Actuals” (what happened) sitting in one place and “Budgets” (the goal) sitting in another, usually at different levels of detail. In this project, I’m going to document how I: I’m sharing my journey in public so that if you’re also preparing for the PL-300, you can follow along, build these solutions with me, and understand thewhybehind the architecture — not just thehow.For this project, we are using theMicrosoft Financial Sample. It’s the perfect “blank canvas” because it comes as a flat, “messy” table that we have to re-engineer professionally. How to get it:In Power BI Desktop, go toHome > Sample Dataset > Load Sample Data. Select thefinancialstable. Let’s get our hands dirty in Power Query.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:55.260429", "author": null, "score": null}
{"title": "How LLMs Handle Infinite Context With Finite Memory", "url": "https://towardsdatascience.com/llms-can-now-process-infinite-context-windows/", "content": "How LLMs Handle Infinite Context With Finite Memory. Achieving infinite context with 114× less memory Over the pasttwo years, we witnessed a race for sequence length in AI language models. We gradually evolved from 4k context length to 32k, then 128k, to the massive 1-million token window first promised by models like Gemini 1.5 pro. The promise was alluring: dump entire codebases or novels into the model and let it reason across the entire thing. But there is a hidden cost to this virtually “infinite” context length, which is rarely ever mentioned: Memory. In a standard Transformer architecture, memorising and reasoning across the entire prompt isn’t free. As the input sequence grows, the model must store the Key and Value (KV) states for every single token to calculate attention scores. For a 1-million-token sequence, this KV Cache can quickly snowball to hundreds of gigabytes, which in turn requires large clusters of GPUs across multiple data centres, all to just hold the conversation in memory. In a standard attention mechanism (Vaswani et al., 2017)6, every new token that the model generates needs to “look back” to every previous token in the prompt to fully understand the context. To make this efficient over multiple generations, the model caches the Key (K) and Value (V) vectors of previous tokens in the GPU VRAM. This is known as the KV cache. While caching the Key and Value vectors (KV cache) can be time-efficient (as we don’t have to recompute the past for every new token), it has a huge memory footprint, which grows linearly with the input sequence length. To put this into perspective: to store the KV cache for a standard 500B parameter model for a context of just 20,000 tokens requires about 126GB of memory. If we scale that to the parameter counts of modern LLM’s 1T+ parameters, and serving millions of users at any given time, the total memory footprint becomes an astronomically large figure. Historically, we’ve had two ways to handle sequential data, neither of which is perfect: This is the trade-off that Infini-attention aims to fill. To solve the memory paradox, researchers at Google formulated Infini-attention (Munkhdalai et al., 2024)1. The core principle of the approach is that instead of storing the entire conversation, we can store a summary of it.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:56.676983", "author": null, "score": null}
{"title": "Data Science Spotlight: Selected Problems from Advent of Code 2025", "url": "https://towardsdatascience.com/data-science-spotlight-selected-problems-from-advent-of-code-2025/", "content": "Data Science Spotlight: Selected Problems from Advent of Code 2025. Hands-on walkthroughs of problems and solution approaches that power real‑world data science use cases Adventof Codeis an annual advent calendar of programming puzzles that are themed around helping Santa’s elves prepare for Christmas. The whimsical setting masks the fact that many puzzles call for serious algorithmic problem-solving, especially towards the end of the calendar. In aprevious article, we discussed the importance of algorithmic thinking for data scientists even as AI-assisted coding becomes the norm. With Advent of Code 2025 having wrapped up last month, this article takes a closer look at a selection of problems from the event that are especially relevant for data scientists. We will sketch out some interesting solution approaches in Python, highlighting algorithms and libraries that can be leveraged in a wide array of real-world data science use cases. The first problem we will look at isDay 7: Laboratories. We are given a tachyon manifold in a file calledinput_d7.txt, as shown below: A tachyon beam (“|”) starts at the top of the manifold and travels downward. If the beam hits a splitter (“^”), it splits into two beams, one on either side of the splitter. Part One of the puzzle asks us to determine the number of times a beam will split given a set of initial conditions (starting point of the beam and the manifold layout). Note that simply counting the number of splitters and multiplying by two will not give the correct answer, since overlapping beams are only counted once, and some splitters are never reached by any of the beams. We can leverage set algebra to account for these constraints as shown in the implementation below: We use theintersectionoperation to identify the splitters that are directly hit by active beams coming from above. New beams are created to the left and right of every splitter that is hit, but overlapping beams are only counted once with theunionoperator. The set of beams resulting from each layer of splitters in the tachyon manifold is computed using a list comprehension wrapped in areducefunction, a higher-order function that helps to simplify the code and typically seen in functional programming. Thedifferenceoperator ensures that the original beams incident on the splitter are not counted among the set of outgoing active beams. In a classical system, if a tachyon particle is sent through the manifold and encounters a splitter, the particle can only continue along one unique path to the left or right of the splitter. Part Two of the puzzle introduces a quantum version of this setup, in which a particle simultaneously goes down both the left and right paths, effectively spawning two parallel timelines. Our task is to determine the total number of timelines that exist after a particle has traversed all viable paths in such a quantum tachyon manifold. This problem can be solved efficiently using dynamic programming as shown below: Recursive depth-first search with memoization is used to set up a top-down form of dynamic programming, where each subproblem is solved once and reused multiple times. Two base cases are defined: a valid timeline is not created if a particle goes out of bounds horizontally, and a complete timeline is counted once the particle reaches the bottom of the manifold. The recursive step accounts for two cases: whenever the particle reaches a splitter, it branches into two timelines, otherwise it continues straight down in the existing timeline. Memoization (using the@lru_cachedecorator) prevents recalculation of known values when multiple paths converge at the same location in the manifold. In practice, data scientists can use the tools and techniques described above in a variety of situations. The concept of beam splitting is similar in some ways to the proliferation of data packets in a complex communications network. Simulating the cascading process is a bit like modeling supply chain disruptions, epidemics, and information diffusion. At a more abstract level, the puzzle can be framed as a constrained graph traversal or path counting problem. Set algebra and dynamic programming are versatile concepts that data scientists can use to solve such seemingly difficult algorithmic problems. The next problem we will look at isDay 8: Playground. We are provided with a list of triples that represent the 3D location coordinates of electrical junction boxes in a file calledinput_d8.txt, as shown below: In Part One, we are asked to successively identify and connect pairs of junction boxes that are closest together in terms of straight-line (or Euclidean) distance. Connected boxes form a circuit through which electricity can flow. The task is ultimately to report the result of multiplying together the sizes of the three largest circuits after connecting the 1000 pairs of junction boxes that are closest together. One neat solution involves using a min-heap to store pairs of junction box coordinates. Following is an implementation based on aninstructive videoby James Peralta:", "source": "TowardsDataScience", "date": "2026-01-21T10:22:58.101156", "author": null, "score": null}
{"title": "Mastering Non-Linear Data: A Guide to Scikit-Learn’s SplineTransformer", "url": "https://towardsdatascience.com/mastering-non-linear-data-a-guide-to-scikit-learns-splinetransformer/", "content": "Mastering Non-Linear Data: A Guide to Scikit-Learn’s SplineTransformer. Stop fitting straight lines to curved data. A smarter way to handle non-linearity We all knowthat linear models can be… well, stiff. Have you ever looked at a scatter plot and realized a straight line just isn’t going to cut it? We’ve all been there. Real-world data is always challenging. Most of the time, it feels like the exception is the rule. The data you get in your job is nothing like those beautiful linear datasets that we used during years of training in the academy. For example, you’re looking at something like “Energy Demand vs. Temperature.” It’s not a line; it’s a curve. Usually, our first instinct is to reach for Polynomial Regression. But that’s a trap! If you’ve ever seen a model curve go wild at the edges of your graph, you’ve witnessed the “Runge Phenomenon.” High-degree polynomials are like a toddler with a crayon, since they’re too flexible and have no discipline. That’s why I am going to show you this option calledSplines.They’re a neat solution: more flexible than a line, but far more disciplined than a polynomial. Splines are mathematical functions defined by polynomials, and used to smooth a curve. Instead of trying to fit one complex equation to your entire dataset, you break the data into segments at points calledknots. Each segment gets its own simple polynomial, and they’re all stitched together so smoothly you can’t even see the seams. Imagine we have a non-linear trend, and we apply a polynomialx²orx³to it. It looks okay locally, but then we look at the edges of your data, and the curve is going way off. According toRunge’s Phenomenon[2],high-degree polynomials have this problem where one weird data point at one end can pull the entire curve out of whack at the other end. Splines don’t try to fit one giant equation to everything. Instead, they divide your data into segments using points calledknots. We have some advantages of using knots.", "source": "TowardsDataScience", "date": "2026-01-21T10:22:59.540485", "author": null, "score": null}
{"title": "Terms of Use", "url": "https://towardsdatascience.com/website-terms-of-use/", "content": "Terms of Use. Last Modified: February 7, 2025 Acceptance of the Terms of Use These terms of use are entered into by and between you and Insight Media Group, LLC (“TDS,” “we,” or “us” or “our”). The following terms and conditions, together with any documents they expressly incorporate by reference (collectively, “Terms of Use“), govern your access to and use ofhttps://towardsdatascience.com, including any content, functionality, and services offered on or throughhttps://towardsdatascience.com(the “Website“), whether as a guest or a registered user. Please read these Terms of Use carefully before you start to use the Website. By using the Website or by clicking to accept or agree to the Terms of Use when this option is made available to you, you accept and agree to be bound and abide by these Terms of Use  and our Privacy Policy, found athttps://towardsdatascience.com/privacy-policy/,and incorporated herein by reference. In the event of a conflict between these Terms of Use and the Privacy Policy, the Privacy Policy shall govern. If you do not want to agree to these Terms of Use or the Privacy Policy, you must not access or use the Website. This Website is offered and available to users who are 13 years of age or older. If you are under the age of 13, you may not access or use the Website. Changes to the Terms of Use TDS may revise and update these Terms of Use from time to time in our sole discretion. All changes are effective immediately when we post them and apply to all access to and use of this Website thereafter. Your continued use of the Website following the posting of revised Terms of Use means that you accept and agree to the changes. You are expected to check this page from time to time so you are aware of any changes, as they are binding on you.", "source": "TowardsDataScience", "date": "2026-01-21T10:23:00.768243", "author": null, "score": null}
{"title": "Teaching a Neural Network the Mandelbrot Set", "url": "https://towardsdatascience.com/teaching-a-neural-network-the-mandelbrot-set/", "content": "Teaching a Neural Network the Mandelbrot Set. And why Fourier features change everything The Mandelbrotset is one of the most beautiful mathematical objects ever discovered, a fractal so intricate that no matter how much you zoom in, you keep finding infinite detail. But what if we asked a neural network to learn it? At first glance, this sounds like an odd question. The Mandelbrot set is fully deterministic, there’s no data, no noise, no hidden rules. But this simplicity makes it a perfect sandbox to study how neural networks represent complex functions. In this article, we’ll explore how a simple neural network can learn to approximate the Mandelbrot set, and howGaussian Fourier Featurescompletely transform its performance, turning blurry approximations into sharp fractal boundaries. Along the way, we’ll dive into why vanilla multi-layer perceptrons (MLPs) struggle with high-frequency patterns (thespectral bias problem) and how Fourier features solve it.  The Mandelbrot set is defined over the complex plane. For each complex number \\(c\\in \\mathbb{C}\\), we consider the iterative sequence: $$z_{n+1} = z_n^2 + c, z_0 = 0$$ If this sequence remains bounded, then \\(c\\) belongs to the Mandelbrot set. In practice, we approximate this condition using theescape-time algorithm. The escape-time algorithm iterates the sequence up to a fixed maximum number of steps and monitors the magnitude \\(|z_n|\\). If \\(|z_n|\\) exceeds a chosen escape radius (typically 2), the sequence is guaranteed to diverge, and \\(c\\) is classified as outside of the mandelbrot set. If the sequence does not escape within the maximum number of iterations, \\(c\\) is assumed to belong to the Mandelbrot set, with the iteration count often used for visualization purposes.", "source": "TowardsDataScience", "date": "2026-01-21T10:23:04.737049", "author": null, "score": null}
{"title": "TDS Newsletter: December Must-Reads on GraphRAG, Data Contracts, and More", "url": "https://towardsdatascience.com/tds-newsletter-december-must-reads-on-graphrag-data-contracts-and-more/", "content": "TDS Newsletter: December Must-Reads on GraphRAG, Data Contracts, and More. Don't miss our most popular articles of the previous month Never miss a new edition ofThe Variable, our weekly newsletter featuring a top-notch selection of editors’ picks, deep dives, community news, and more. Yes, it’s 2026 — and we’re already focused on an eventful year ofgrowthand learning here at TDS. We’ve also published many stellar articles last month, including at the height of the holiday season, and we wouldn’t want you to miss out on any of them. This week, we’re devoting the Variable to one last 2025 hurrah, highlighting some of our most popular stories from December. Make no mistake, however: they cover timely and actionable topics in machine learning, data science, and AI, and will remain relevant for weeks and months to come. When “vanilla” RAG systems no longer cut it, you may want to explore the power of GraphRAG — andPartha Sarkar‘s detailed guide is a great starting point for anyone interested in tinkering with this powerful approach, which leverages hybrid pipelines and can lead to lower costs. For additional hands-on RAG insights, we highly recommend Sabrine Bendimerad’s roundup of best practices, covering data quality, evaluation, and more. Quick and focused, Eirik Berge presents a guide to using open-source library Pandera when you aim to define schemas as class objects. From learning algorithms with Excel to improving Pandas’ performance, here are a few more of last month’s most-read and -shared stories. We hope you take the time to explore excellent work from TDS contributors who recently joined our community: Do your New Year’s resolutions include publishing on TDS and joining ourAuthor Payment Program? Now’s the time tosend along your latest draft!", "source": "TowardsDataScience", "date": "2026-01-21T10:23:06.197421", "author": null, "score": null}
{"title": "Beyond Prompting: The Power of Context Engineering", "url": "https://towardsdatascience.com/beyond-prompting-the-power-of-context-engineering/", "content": "Beyond Prompting: The Power of Context Engineering. Using ACE to create self-improving LLM workflows and structured playbooks Context is everythingan LLM can see before it generates an answer. This includes the prompt itself, instructions, examples, retrieved documents, tool outputs, and even the prior conversation history. Context has a huge impact on answer quality. For example, if you ask an LLM to write a SQL query without providing the data schema, the result will almost certainly be suboptimal. Worse, if the model has no access to the database at all, it may simply hallucinate a query that doesn’t work. Even when tools are available, the model still needs extra time and effort to infer the schema before it can produce a correct answer. Because context plays such a central role in LLM-based applications, context engineering has emerged as a discipline focused on systematically optimising what information goes into a model’s prompt. The goal is to build “self-improving” systems that learn from experience without relying on expensive fine-tuning (retraining models and updating millions of parameters). Context engineering comes with several key advantages: With all these advantages, it’s not surprising that context engineering is gaining so much attention. What’s interesting, though, is how quickly the approaches themselves are evolving. In this article, I’ll walk through that evolution and then experiment with one of the newer frameworks for prompt optimisation: Agentic Context Engineering (ACE). Context engineering didn’t appear overnight. It has evolved through several distinct stages. The earliest stage was static prompting.Here, prompts were hand-crafted instructions that never changed. Most of the effort went into classic prompt engineering: carefully choosing wording, structure, and formatting to squeeze better performance out of the model. The next major step was dynamic retrieval. Instead of relying on a fixed prompt, systems began pulling in relevant information (documents, examples, or facts) at inference time. Retrieval-Augmented Generation (RAG) became one of the most popular approaches in this category. By grounding responses in external data, RAG significantly improved accuracy and reduced hallucinations, especially for knowledge-heavy tasks. More recently, the focus has shifted toward self-improving contexts.Rather than treating context as something that is merely retrieved or injected, these approaches allow the system to update and refine its own context based on past performance. In other words, the prompt itself becomes adaptive, evolving through reflection and feedback.", "source": "TowardsDataScience", "date": "2026-01-21T10:23:07.470109", "author": null, "score": null}
{"title": "Retrieval for Time-Series: How Looking Back Improves Forecasts", "url": "https://towardsdatascience.com/retrieval-for-time-series-how-looking-back-improves-forecasts/", "content": "Retrieval for Time-Series: How Looking Back Improves Forecasts. An introduction to retrieval in time-series forecasting We all know how it goes: Time-series data is tricky. Traditional forecasting models are unprepared for incidents like sudden market crashes, black swan events, or rare weather patterns. Even large fancy models like Chronos sometimes struggle because they haven’t dealt with that kind of pattern before. We can mitigate this withretrieval. With retrieval, we are able to askHas anything like this happened before?and then using that past example to guide the forecast. As we all might know now, in natural language processing (NLP), this idea is calledRetrieval-Augmented Generation (RAG). It is becoming popular too in the time-series forecasting world. The model then considerspast situationsthat look similar to the current one, and from there it can make morereliablepredictions. How is this RAF different from traditional time-series? Retrieval forecasting adds anexplicit memory access step. Instead of: Past -> parameters -> forecast", "source": "TowardsDataScience", "date": "2026-01-21T10:23:09.021071", "author": null, "score": null}
{"title": "How to Improve the Performance of Visual Anomaly Detection Models", "url": "https://towardsdatascience.com/how-to-improve-the-performance-of-visual-anomaly-detection-models/", "content": "How to Improve the Performance of Visual Anomaly Detection Models. Apply the best methods from academia to get the most out of practical applications There are several methods to improve performance, which are used by authors in academia to make it easier for the proposed model to stand out by showing more impressive results compared to other models. For example, using a larger input size, which helps to detect small defects; another is removing part of the background to reduce false positives. Such an approach can be weak in academia because it makes comparisons across different models less fair and might not perform equally well across all datasets. However, these methods can also be used to improve performance in practical applications if applied carefully. In this article, we will review several of the most powerful methods and explain how to use them to achieve better results while avoiding potential downsides. Anomaly detection models are often called “unsupervised”, but this name can be misleading because most of them require only one class for training, normal images without defects. To train witha single class, the data needs to be labelled into separate classes, which differs from the usual definition ofunsupervisedlearning. Based on the normal images used during training, the model learns what “normality” looks like and should be able to identify deviations from it as images with defects. These defects are often smalland hard to see, even for professional inspectors on a production line. The example below shows a drop of welding paste on one of the contacts, which is difficult to spot without the ground truth mask showing the defect location on the right. For more details on visual industrial anomaly detection, seethis postorthis survey. If images in your dataset have small defects (less than 0.2% of the image or so, this number is arbitrary and depends on the model used and other factors) that the model cannot detect, try increasing the input size. It often helps to detect such defects by making them large enough for the model to see. When large defects (10% of the image or more, this number is also arbitrary) are present, you should be more careful with the model selection. Some models, likePatchCore, show better results for different defect sizes with larger input size, others, likeRD4AD, might degrade significantly for larger defects, as described in ourbenchmark paper,Tab. 5 and 14. The best practice is to test how the selected model performs on different defect types you have. Another important consideration when using a larger input size is the inference speed and memory constraints. As shown inMVTec AD 2 paper, Fig.6, the inference time and memory usage increased significantly for almost all tested models with larger input sizes. If you have data with an object at the center of an image, and the rest can be cropped safely, go for it. As shown in the image below, cropping closer to the inspected part helps to avoid false positives. An important side effect is that the relative size of the inspected part also increases; as described earlier, this might help you to obtain better results for small defects or increase inference speed by allowing you to make the image smaller.", "source": "TowardsDataScience", "date": "2026-01-21T10:23:10.395496", "author": null, "score": null}
{"title": "Faster Is Not Always Better: Choosing the Right PostgreSQL Insert Strategy in Python (+Benchmarks)", "url": "https://towardsdatascience.com/faster-is-not-always-better-choosing-the-right-postgresql-insert-strategy-in-python-benchmarks/", "content": "Faster Is Not Always Better: Choosing the Right PostgreSQL Insert Strategy in Python (+Benchmarks). This article compares and benchmarks various insert strategies, focusing on trade-offs between safety, abstraction, and throughput — and choosing the right tool for the job. This articledemonstrates that it’s perfectly possible to insert 2M records per second into Postgres. Instead of chasing micro-benchmarks, in this article we’ll step back to ask a more important question: Which abstractions actually fits our workload? We’ll look at 5 ways to insert data into Postgres using Python. The goal is not to look just at insert speeds and crown a winner but to understand the trade-offs betweenabstraction,safety,convenienceandperformance. In the end you’ll understand: High-volume insert workloads show up everywhere: Small inefficiencies compound quickly. Turning a 3-minute insert job into a 10-second one can reduce system load, free up workers and improve overall throughput. That said, faster does not automatically mean better. When workloads are small sacrificing clarity and safety for marginal gains rarely pays off. Understandingwhenperformance matters andwhyis the real goal. To talk to our Postgres database we need a database driver. In our case this ispsycopg3with SQLAlchemy layered on top. Here’s a quick distinction: psycopg3is a low-level PostgreSQL driver for Python. This is a very thin abstraction with minimal overhead that talks to Postgres directly.The trade-off is responsibility: you write SQL yourself, manage bathing and handle correctness explicitly.", "source": "TowardsDataScience", "date": "2026-01-21T10:23:12.058022", "author": null, "score": null}
{"title": "HNSW at Scale: Why Your RAG System Gets Worse as the Vector Database Grows", "url": "https://towardsdatascience.com/hnsw-at-scale-why-your-rag-system-gets-worse-as-the-vector-database-grows/", "content": "HNSW at Scale: Why Your RAG System Gets Worse as the Vector Database Grows. How approximate vector search silently degrades Recall—and what to do about It If you are usinga modern vector database—Neo4j, Milvus, Weaviate, Qdrant, Pinecone—there is a very high chance thatHierarchical Navigable Small World(HNSW)is already powering your retrieval layer. It is quite likely you did not choose it while building the database, nor did you tune it or even know it is there. And yet, HNSW isquietly deciding what your LLM sees as truth. It determines which document chunks are fed into your RAG pipeline, which memories your agent recalls, and ultimately, whether the model answers correctly—or hallucinates confidently. As your vector database grows, retrieval quality degrades gradually: But thecontext quality deteriorates, and your RAG system becomes less reliable over time—even though the embedding model and distance metric remain unchanged. In this article, I demonstrate—using controlled experiments and real data—how HNSW impacts retrieval quality as database size grows, why this degradation is worse than flat search, and what you can realistically do about it in production RAG systems. Specifically, I will: HNSW is a graph-based algorithm for Approximate Nearest Neighbor (ANN) search. It organizes data into multiple layers of connected neighbors and uses this graph structure to speed up search. Each vector is connected to a limited number of neighbors in each layer. During a search, it performs agreedy searchthrough these layers, and the number of neighbors checked at each layer is constant (controlled byMandef_search), which makes the search processlogarithmicwith respect to the number of vectors. Compared to flat search, where time complexity is O(N), HNSW search has a time complexity of O(log N), which means the time required for a search grows very slowly (logarithmically) as compared to linear search. We will see this in the result of our use case. 1.Build time parameters:Mandef_construction. Can be set before building the database only. M defines the maximum number of connections (neighbors) that each vector (node) can have in each layer of the graph. A higher M means more connections, making the graph denser and potentially increasing recall but at the cost ofmore memoryandslower indexing.", "source": "TowardsDataScience", "date": "2026-01-21T10:23:13.469665", "author": null, "score": null}
{"title": "I Evaluated Half a Million Credit Records with Federated Learning. Here’s What I Found", "url": "https://towardsdatascience.com/i-evaluated-half-a-million-credit-records-with-federated-learning-heres-what-i-found/", "content": "I Evaluated Half a Million Credit Records with Federated Learning. Here’s What I Found. Why privacy breaks fairness at small scale and how collaboration fixes both without sharing a single record Regulators want privacy. Compliance wants fairness. The business wants accuracy. At a small scale, you can’t have all three. At enterprise scale, something surprising happens. Disclaimer:This article presents findings from my research on federated learning for credit scoring. While I offer strategic options and recommendations, they reflect my specific research context. Every organization operates under different regulatory, technical, and business constraints. Please consult your own legal, compliance, and technical teams before implementing any approach in your organization. You’re a credit risk manager at a mid-sized bank. Your inbox just landed three conflicting mandates: Here’s what I discovered through research on 500,000 credit records:All three are harder to achieve together than anyone admits.At a small scale, you face a genuine mathematical tension. But there’s an elegant solution hiding at enterprise scale. Let me show you what the data reveals—and how to navigate this tension strategically. Before I show you the tension, let me define what we’re measuring. Think of these as three dials you can turn: Privacy (ε — “epsilon”) Lower epsilon = stronger privacy protection (counterintuitive, I know!). Fairness (Demographic Parity Gap)", "source": "TowardsDataScience", "date": "2026-01-21T10:23:15.040691", "author": null, "score": null}
{"title": "Probabilistic Multi-Variant Reasoning: Turning Fluent LLM Answers Into Weighted Options", "url": "https://towardsdatascience.com/probabilistic-multi-variant-reasoning-turning-fluent-llm-answers-into-weighted-options/", "content": "Probabilistic Multi-Variant Reasoning: Turning Fluent LLM Answers Into Weighted Options. Human Guided AI Collaboration When I watchpeople use generative AI at work, there is a pattern that repeats so often it feels like a sitcom rerun. Someone has a real decision to make: which model to ship, which architecture to deploy, which policy to roll out. They open their favorite LLM, type a single prompt, skim the answer for plausibility, maybe tweak the prompt once or twice, and then copy the “best looking” solution into a document. Six months later, when something breaks or underperforms, there is no clear record of what alternatives were considered, how uncertain the team actually was, or why they chose this path instead of others. There is just a fluent paragraph that felt convincing, once. What is missing there is not more “AI power.” It is the habit of explicit human reasoning. In this article I want to name and unpack a habit I have been using and teaching in my own work with LLMs and complex systems. I call it Probabilistic Multi-Variant Reasoning (PMR). It is not a new branch of math, and it is certainly not an algorithm. Think of it instead as a practical, applied reasoning pattern for humans working with generative models: a disciplined way to surface multiple plausible futures, label your uncertainty, think about consequences, and only then decide. PMR is for people who use LLMs to make decisions, design systems, or manage risk. GenAI just makes it cheap and fast to do this. The pattern itself applies everywhere you have to choose under uncertainty, where the stakes and constraints actually matter. The default way most people use LLMs is “single-shot, single answer.” You ask a question, get one neat explanation or design, and your brain does a quick “does this feel smart?” check. The problem is that this hides everything that really matters in a decision: what other options were plausible, how uncertain we are, how big the downside is if we are wrong. It blurs together “what the model thinks is likely,” “what the training data made fashionable,” and “what we personally wish were true.” PMR starts with a simple shift: instead of treating the model as an answer machine, you treat it as a scenario generator with weights. You ask for multiple distinct options. You ask for rough probabilities or confidence scores, and you ask directly about costs, risks, and benefits in plain language. Then you argue with those numbers and stories, adjust them, and only then do you commit.", "source": "TowardsDataScience", "date": "2026-01-21T10:23:16.451305", "author": null, "score": null}
{"title": "Why Supply Chain is the Best Domain for Data Scientists in 2026 (And How to Learn It)", "url": "https://towardsdatascience.com/why-supply-chain-is-the-best-domain-for-data-scientists-in-2026-and-how-to-learn-it/", "content": "Why Supply Chain is the Best Domain for Data Scientists in 2026 (And How to Learn It). My take after 10 years in Supply Chain on why this can be an excellent playground for data scientists who want to see their skills valued. As we step into 2026, my LinkedIn inbox is full of data scientists reaching out. Same questions. Same concerns. Is supply chain data science the right move? After 10 years in supply chain data science, including five years writing on this blog, I have developed strong views on this question. Supply chain is an exceptional playground for data scientists. Rich problems, beautiful mathematics and tangible impacts. But I’m not here to tell you what’s best for your career. In this article, I want to give an honest view of the opportunities that excite me and the challenges that frustrate me. More importantly, I will show how to explore this domain yourself using the tutorials and case studies shared across this blog. You can then test yourself to see whether supply chain analytics resonates with you.", "source": "TowardsDataScience", "date": "2026-01-21T10:23:18.050634", "author": null, "score": null}
{"title": "Terms of Use", "url": "https://towardsdatascience.com/website-terms-of-use/", "content": "Terms of Use. Last Modified: February 7, 2025 Acceptance of the Terms of Use These terms of use are entered into by and between you and Insight Media Group, LLC (“TDS,” “we,” or “us” or “our”). The following terms and conditions, together with any documents they expressly incorporate by reference (collectively, “Terms of Use“), govern your access to and use ofhttps://towardsdatascience.com, including any content, functionality, and services offered on or throughhttps://towardsdatascience.com(the “Website“), whether as a guest or a registered user. Please read these Terms of Use carefully before you start to use the Website. By using the Website or by clicking to accept or agree to the Terms of Use when this option is made available to you, you accept and agree to be bound and abide by these Terms of Use  and our Privacy Policy, found athttps://towardsdatascience.com/privacy-policy/,and incorporated herein by reference. In the event of a conflict between these Terms of Use and the Privacy Policy, the Privacy Policy shall govern. If you do not want to agree to these Terms of Use or the Privacy Policy, you must not access or use the Website. This Website is offered and available to users who are 13 years of age or older. If you are under the age of 13, you may not access or use the Website. Changes to the Terms of Use TDS may revise and update these Terms of Use from time to time in our sole discretion. All changes are effective immediately when we post them and apply to all access to and use of this Website thereafter. Your continued use of the Website following the posting of revised Terms of Use means that you accept and agree to the changes. You are expected to check this page from time to time so you are aware of any changes, as they are binding on you.", "source": "TowardsDataScience", "date": "2026-01-21T10:23:19.226395", "author": null, "score": null}
