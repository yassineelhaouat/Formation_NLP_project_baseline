{"title": "Anthropic's original take home assignment open sourced", "url": "https://github.com/anthropics/original_performance_takehome", "content": "Anthropic's original take home assignment open sourced", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Parliament tells Dutch gov't to keep DigiD data out of American hands", "url": "https://nltimes.nl/2026/01/21/parliament-tells-dutch-govt-keep-digid-data-american-hands", "content": "Parliament tells Dutch gov't to keep DigiD data out of American hands. A parliamentary majority has asked the current caretaker and upcoming new Cabinet to do everything in their power to prevent Dutch DigiD data from ending up in the United States government’s hands. There areconcerns that this could happenthrough the American firmKyndryl’s impending acquisition of Solvinity, a company that is essential for DigiD access. In a technical briefing in the Tweede Kamer, the lower house of the Dutch parliament, on Tuesday, MPs spoke with experts about the dangers and risks of this takeover. Parliament has long been concerned about this issue, and the briefing did nothing to alleviate those worries, NOSreported. VVD parliamentarian Silvio Erkens is deeply concerned that the acquisition could “enable the U.S. government to access data” and use it to blackmail people. GroenLinks-PvdA MP Barbara Kathmann worries that this will get to a point where “Trump can shut down our digital government with the single push of a button.” The cloud and infrastructure company Solvinity provides the infrastructure that transfers data for DigiD - the digital identification that every person in the Netherlands must have to exchange data with health insurers, pension funds, municipalities, and the Tax Authorities, among others. In the United States, the government has a lot of influence and power over American companies, including the ability to demand companies’ data. The Tweede Kamer cannot force companies to abandon an acquisition, GroenLinks-PvdA MP Kathmann acknowledged. But she hopes that the current and upcoming government will do everything in its power to stop this. She suggested persuading Solvinity to reconsider the acquisition. The government IT service Logius could also switch to another company for its DigiD services, or the government can try its best to buy a “golden share,” which would give the Netherlands veto power in the company, Kathmann said. Erkens believes that the deal must be blocked if there are no legal guarantees that Dutch data cannot be accessed in the U.S. And if the acquisition does go through, the Dutch government must ensure that Solvinity is no longer responsible for DigiD activities, the VVD MP said. Digital security is also a topic on the table in the coalition negotiations. D66 leader and future Prime Minister Rob Jetten previously said that the new Cabinet will include a “Minister with very specific responsibility for digital security.” He could not yet say whether this Minister will have their own budget and more power than previous Cabinet members with digital security in their portfolio, but he stressed that the new Minister will be given “a clear mandate.”", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "EmuDevz: A game about developing emulators", "url": "https://afska.github.io/emudevz/", "content": "EmuDevz: A game about developing emulators", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A 26,000-year astronomical monument hidden in plain sight (2019)", "url": "https://longnow.org/ideas/the-26000-year-astronomical-monument-hidden-in-plain-sight/", "content": "A 26,000-year astronomical monument hidden in plain sight (2019). The western flank of the Hoover Dam holds a celestial map that marks the time of the dam’s creation based on the 25,772-year axial precession of the earth. On the western flank of the Hoover Dam stands a little-understood monument, commissioned by the US Bureau of Reclamation when construction of the dam began in 01931. The most noticeable parts of this corner of the dam, now known as Monument Plaza, are the massive winged bronze sculptures and central flagpole which are often photographed by visitors. The most amazing feature of this plaza, however, is under their feet as they take those pictures. The plaza’s terrazzo floor is actually a celestial map that marks the time of the dam’s creation based on the 25,772-year axial precession of the earth. I was particularly interested in this monument because this axial precession is also the slowest cycle that we track in Long Now’s 10,000 Year Clock. Strangely, little to no documentation of this installation seemed to be available, except for a few vacation pictures on Flickr. So the last time I was in Las Vegas, I made a special trip out to Hoover Dam to see if I could learn more about this obscure 26,000-year monument. I parked my rental car on the Nevada side of the dam on a day pushing 100 degrees. I quickly found Monument Plaza just opposite the visitor center where tours of the dam are offered. While the plaza is easy to find, it stands apart from all the main tours and stories about the dam. With the exception of the writing in the plaza floor itself, the only information I could find came from a speaker running on loop, broadcasting a basic description of the monument while visitors walked around the area. When I asked my tour guide about it, he suggested that there may be some historical documentation and directed me to Emme Woodward, the dam’s historian. I was able to get in touch with her after returning home. As she sent me a few items, I began to see why the Bureau of Reclamation doesn’t explain very much about the monument’s background. The first thing she sent me was a description of the plaza by Oskar J. W. Hansen, the artist himself, which I thought would tell me everything I wanted to know. While parts of it were helpful, the artist’s statement of intention was also highly convoluted and opaque. An excerpt: It is pretty hard to imagine the US Bureau of Reclamation using this type of write-up to interpret the monument… and they don’t. And so there it stands, a 26,000-year clock of sorts, for all the world to see, and yet still mired in obscurity. While I may never totally understand the inner motivations of the monument’s designer, I did want to understand it on a technical level. How did Hansen create a celestial clock face frozen in time that we can interpret and understand as the date of the dam’s completion? The earth’s axial precession is a rather obscure piece of astronomy, and our understanding of it through history has been spotty at best. That this major engineering feat was celebrated through this monument to the axial precession still held great interest to me, and I wanted to understand it better. I pressed for more documentation, and the historian sent me instructions for using the Bureau of Reclamation’s image archive site as well as some keywords to search for. The black and white images you see here come from this resource. Using the convoluted web site was a challenge, and at first I had difficulty finding any photos of the plaza before or during its construction. As I discovered, the problem was that I was searching with the term “Monument Plaza,” a name only given to it after its completion in 01936. In order to find images during its construction, I had to search for “Safety Island,” so named because at the time of the dam’s construction, it was an island in the road where workers could stand behind a berm to protect themselves from the never-ending onslaught of cement trucks. I now had some historical text and photos, but I was still missing a complete diagram of the plaza that would allow me to really understand it. I contacted the historian again, and she obtained permission from her superiors to release the actual building plans. I suspect that they generally don’t like to release technical plans of the dam for security reasons, but it seems they deemed my request a low security risk as the monument is not part of the structure of the dam. The historian sent me a tube full of large blueprints and a CD of the same prints already scanned. With this in hand I was finally able to re-construct the technical intent of the plaza and how it works.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Batmobile: 10-20x Faster CUDA Kernels for Equivariant Graph Neural Networks", "url": "https://elliotarledge.com/blog/batmobile", "content": "Batmobile: 10-20x Faster CUDA Kernels for Equivariant Graph Neural Networks. Custom CUDA kernels that eliminate the computational bottlenecks in spherical harmonics and tensor product operations - the core primitives of equivariant GNNs like MACE, NequIP, and Allegro. Equivariant graph neural networks have revolutionized atomistic machine learning. Models like MACE, NequIP, and Allegro achieve state-of-the-art accuracy in molecular dynamics simulations, materials property prediction, and drug discovery. Their secret: they respect the fundamental symmetries of physical systems - rotation, translation, and reflection invariance. But this mathematical elegance comes at a computational cost. The operations that make these models work - spherical harmonics and Clebsch-Gordan tensor products - are expensive. A single MACE layer can spend 80% of its forward pass time in these two operations. This matters for real applications.Molecular dynamics simulations run billions of timesteps. Battery materials discovery screens millions of candidates. Drug binding affinity predictions evaluate thousands of poses. When each forward pass takes milliseconds instead of microseconds, these workflows become impractical. To understand why equivariant GNNs are slow, we need to understand what they compute. When two atoms interact, the direction of their bond matters. A carbon-carbon bond pointing \"up\" is physically different from one pointing \"right\" - and our neural network needs to know this. Spherical harmonics (Y_lm) provide a mathematically principled way to encode 3D directions. Given a unit vector (x, y, z), spherical harmonics compute a set of features that transform predictably under rotation: For L_max=3, we get 16 components total: 1 + 3 + 5 + 7 = 16. These aren't arbitrary features - they form a complete basis for functions on the sphere. When we want to combine two equivariant features (say, node features with edge directions), we can't just concatenate or add them - that would break equivariance. Instead, we use Clebsch-Gordan tensor products. These are specific weighted sums that preserve the transformation properties:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "cURL removes bug bounties", "url": "https://etn.se/index.php/nyheter/72808-curl-removes-bug-bounties.html", "content": "cURL removes bug bounties. Open source code library cURL is removing the possibility to earn money by reporting bugs, hoping that this will reduce the volume of AI slop reports. Joshua Rogers â€“ AI wielding bug hunter of fame â€“ thinks it's a great idea. cURL has been flooded with AI-generated error reports. Now one of the incentives to create them will go away. The vast majority of AI-generated error reports submitted to cURL are pure nonsense. Other open source projects are caught in the same pandemic. cURL maintainer Daniel Stenberg made an impact with his reporting on AI-generated bug reports last year â€“ â€ťDeath by a thousand slops.â€ť Determining that they are nonsense is time-consuming, causing the maintainers lots of extra work. â€ťAI slop and bad reports in general have been increasing even more lately, so we have to try to brake the flood in order not to drownâ€ť, says cURL maintainer Daniel Stenberg to Swedish electronics industry news site etn.se. Therefore, cURL is terminating the bounty payouts as of the end of January. â€śWe hope this removes some of the incentives for people to send us garbage. We spend far too much time handling slop due to findings that are not real, exaggerated, or misunderstood.â€ť Not all AI-generated bug reports are nonsense. Itâ€™s not possible to determine the exact share, but Daniel Stenberg knows of more than a hundred good AI assisted reports that led to corrections. In total, 87 bug reports to cURL have over the years amounted to USD 101,020 in bounties.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "RSS.Social – the latest and best from small sites across the web", "url": "https://rss.social/", "content": "RSS.Social – the latest and best from small sites across the web. The latest and best from small sites across the web.Learn how it works.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Hypnosis with Aphantasia", "url": "https://aphantasia.com/article/stories/hypnosis-with-aphantasia", "content": "Hypnosis with Aphantasia. Can aphantasics be hypnotized? My experience learning to be hypnotized with imagery-free inductions. Liana is a semi-retired writer and amateur potter. Despite her lifelong inability to visualize - or perhaps because of it - Liana has learned to adapt, bending her capabilities in imaginative ways to service her creativity. As a storyteller with aphantasia, Liana imagines our wondrous world through the lenses of perception, memory, and feeling, seeking to write passionate, sometimes humorous, tales full of possibilities.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The challenges of soft delete", "url": "https://atlas9.dev/blog/soft-delete.html", "content": "The challenges of soft delete. Software projects often implement \"soft delete\", maybe with adeletedboolean or anarchived_attimestamp column.\nIf customers accidentally delete their data, they can recover it, which makes work easier for customer support teams.\nPerhaps archived records are even required for compliance or audit reasons. I've run into some trouble with soft delete designs. I'll cover those, and ponder ideas for how I'd build this in the future. Adding anarchived_atcolumn seems to ooze complexity out into queries, operations, and applications.\nRecovering deleted records does happen, but 99% of archived records are never going to be read. So, the database tables will have a lot of dead data. Depending on access patterns, that might even be a significant amount of data.\nI've seen APIs that didn't work well with Terraform, so Terraform would delete + recreate records on every run, and over time that led\nto millions of dead rows. Your database can probably handle the extra bytes, and storage is fairly cheap, so it's not necessarily a problem, at first. Hopefully, the project decided on a retention period in the beginning, and set up a periodic job to clean up those rows.\nUnfortunately, I'd bet that a significant percentage of projects did neither – it's really easy to ignore the archived data for a long time. At some point, someone might want to restore a database backup. Hopefully that's for fun and profit and not because you lost the production database at 11 am.\nIf your project is popular, you might have a giant database full of dead data that takes a long time to recreate from a dump file. archived_atcolumns also complicate queries, operations, and application code. Applications need to make sure they always avoid the archived data that's sitting\nright next to the live data. Indexes need to be careful to avoid archived rows. Manual queries run for debugging or analytics are longer and more complicated.\nThere's always a risk that archived data accidentally leaks in when it's not wanted. The complexity grows when there are mapping tables involved. Migrations have to deal with archived data too. Migrations may involve more than just schema changes – perhaps you need to fix a mistake with default values, or add a new column and backfill values.\nIs that going to work on records from 2 years ago? I've done migrations where these questions were not trivial to answer. Restoring an archived record is not always as simple as just runningSET archived_at = null– creating a record may involve making calls to external systems as well.\nI've seen complex restoration code that was always a buggy, partial implementation of the \"create\" API endpoint. In the end, we removed the specialized restoration code\nand required all restoration to go through the standard APIs – that simplified the server implementation, and ensured that old data that had since become invalid, could not\nbe restored incorrectly – it needs to pass the new validation rules. I'm not a fan of thearchived_atcolumn approach. It's simple at first, but in my experience, it's full of pitfalls down the line.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Libbbf: Bound Book Format, A high-performance container for comics and manga", "url": "https://github.com/ef1500/libbbf", "content": "Libbbf: Bound Book Format, A high-performance container for comics and manga", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Mastra 1.0, open-source JavaScript agent framework from the Gatsby devs", "url": "https://github.com/mastra-ai/mastra", "content": "Show HN: Mastra 1.0, open-source JavaScript agent framework from the Gatsby devs", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Instabridge has acquired Nova Launcher", "url": "https://novalauncher.com/nova-is-here-to-stay", "content": "Instabridge has acquired Nova Launcher. Hi everyone. We want to share a clear update directly with the Nova community. Instabridge has acquired Nova Launcher. We are a Swedish company building products that help people get online, used by millions of people worldwide. Nova is not shutting down. Our immediate focus is simple: keep Nova stable, compatible with modern Android, and actively maintained. We also know many of you have lived through a long period of uncertainty. Nova has a strong identity and a community that still cares deeply. We take that seriously. Our job is not to reinvent Nova overnight. Our job is to be responsible owners. That means: We will be reading and collecting feedback from Reddit, Play Store reviews, email, and other community channels. We will not be able to respond to every post, but we will be paying attention. For support related issues, we will share a clear contact channel shortly. We have long admired what Nova represents: speed, customization, and user control. When we saw how much the community still cares, it was clear to us that Nova deserved a stable future with active maintenance. Yes. Nova’s identity is the point. Performance, flexibility, and user control stay at the center of the product. Any future changes will be evaluated through that lens. Nova needs a sustainable business model to support ongoing development and maintenance. We are exploring different options, including paid tiers and other approaches. As many of you have already anticipated, we are also evaluating ad based options for the free version.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The GDB JIT Interface", "url": "https://bernsteinbear.com/blog/gdb-jit/", "content": "The GDB JIT Interface. GDB is great for stepping through machine code to figure out what is going on.\nIt uses debug information under the hood to present you with a tidy backtrace\nand also determine how much machine code to print when you typedisassemble. This debug information comes from your compiler. Clang, GCC, rustc, etc all\nproduce debug data in a format calledDWARFand then embed that debug\ninformation inside the binary (ELF, Mach-O, …) when you do-ggdbor\nequivalent. Unfortunately, this means that by default, GDB has no idea what is going on if\nyou break in a JIT-compiled function. You can step instruction-by-instruction\nand whatnot, but that’s about it. This is because the current instruction\npointer is nowhere to be found in any of the existing debug info tables from\nthe host runtime code, so your terminal is filled with???. See this example\nfrom the V8 docs: Fortunately, there is aJIT interfaceto GDB. If you implement a couple of\nfunctions in your JIT and run them every time you finish compiling a function,\nyou can get the debugging niceties for your JIT code too. See again a V8\nexample: Unfortunately, the GDB docs aresomewhat sparse. So I went\nspelunking through a bunch of different projects to try and understand what is\ngoing on. GDB expects your runtime to expose a function called__jit_debug_register_codeand a global variable called__jit_debug_descriptor. GDB automatically adds its own internal breakpoints\nat this function, if it exists. Then, when you compile code, you call this\nfunction from your runtime. In slightly more detail: This is why you see compiler projects such as V8 including large swaths of code\njust to make object files: Because this is a huge hassle, GDB also has a newer interface that does not\nrequire making an ELF/Mach-O/…+DWARF object. This new interface requires writing a binary format of your choice. You make\nthe writer and you make the reader. Then, when you are in GDB, you load your\nreader as a shared object.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IPv6 is not insecure because it lacks a NAT", "url": "https://www.johnmaguire.me/blog/ipv6-is-not-insecure-because-it-lacks-nat/", "content": "IPv6 is not insecure because it lacks a NAT. I recently saw a discussion where someone argued that IPv4 is more secure than IPv6 because “the NAT-by-default of IPv4 effectively means that I get the benefit of a default-deny security strategy.” This is a common misconception that I think is worth addressing. The fundamental issue here is conflating NAT (Network Address Translation) with security. NAT isn’t actually a security feature—it’s an address conservation mechanism that became necessary because we ran out of IPv4 addresses. (Although it is totally possible to use a NAT with IPv6 too!) NAT allows multiple devices on a home network to share a single IP address on the public Internet by rewriting the destination IP of a packet based on its destination port. It chooses a new destination IP based on the “port mappings” or “port forwards” configured by the network admin. The consequence of this is that when receiving inbound traffic to a NAT’d IP, packets with an unexpected destination port (one which has not been forwarded) will keep the destination IP of the public machine and will not be routed to another machine on the network. But the security benefits people attribute to NATactuallycome from the stateful firewall that’s typically bundled with NAT routers. Modern routers ship with firewall policies that deny inbound traffic by default, even when a NAT is not being used. The firewall will drop packets with an unexpected destination before even considering whether to rewrite or route the packets. For example, UniFi routers ship with these default IPv6 firewall rules: Therefore, in order to allow unsolicited inbound traffic to any IPv6 device hosted behind the router, you must explicitly add a firewall rule to allow the traffic, whether using a NAT or not.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Infracost (YC W21) Is Hiring Sr Back End Eng (Node.js+SQL) to Shift FinOps Left", "url": "https://www.ycombinator.com/companies/infracost/jobs/Sr9rmHs-senior-backend-engineer-node-js-sql", "content": "Infracost (YC W21) Is Hiring Sr Back End Eng (Node.js+SQL) to Shift FinOps Left", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Which AI Lies Best? A game theory classic designed by John Nash", "url": "https://so-long-sucker.vercel.app/", "content": "Which AI Lies Best? A game theory classic designed by John Nash. A game theory classic designed byJohn Nashthatrequiresbetrayal to win. Now a benchmark\n                    for AI deception. A benchmark that tests what most benchmarks can't:\n                    deception, negotiation, and trust. So Long Suckerwas designed in 1950\n                            by four game theorists includingJohn Nash(of \"A Beautiful Mind\" fame). The\n                            game has one brutal property:betrayal is required to win. This lets us test AI capabilities that standard benchmarks miss: 4 players, each with colored chips. Take turns\n                                playing chips on piles. If your chip matches the\n                                one below it, you capture the pile. Run out of\n                                chips? Beg others for help — or get eliminated.\n                                Last player standing wins. Each AI developed its own personality. Here's who they\n                    became. Win ratesinvertas game complexity increases. Manipulation becomes more effective as game\n                                length increases. Gaslighting tactics need time to work. Reactive play dominates simple games but\n                                collapses under complexity. No internal\n                                reasoning means no long-term planning. We can see their private thoughts. They don't match what they say.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Unconventional PostgreSQL Optimizations", "url": "https://hakibenita.com/postgresql-unconventional-optimizations", "content": "Unconventional PostgreSQL Optimizations. When it comes to database optimization, developers often reach for the same old tools: rewrite the query slightly differently, slap an index on a column, denormalize, analyze, vacuum, cluster, repeat. Conventional techniques are effective, but sometimes being creative can really pay off! In this article, I present unconventional optimization techniques in PostgreSQL. Table of Contents  Imagine you have this table of users: For each user you keep their name and which payment plan they're on. There are only two plans, \"free\" and \"pro\", so you add a check constraint. Generate some data and analyze the table: You now have 100K users in the system. Now you want to let your analysts access this table in their reporting tool of choice. You give one of the analysts permission, and this is the first query they write: The query returned no results, and the analyst is baffled. How come there are no users on the \"Pro\" plan?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "200 MB RAM FreeBSD Desktop", "url": "https://vermaden.wordpress.com/2026/01/18/200-mb-ram-freebsd-desktop/", "content": "200 MB RAM FreeBSD Desktop. Recently I came acrossLundukepost about some mysteriousVendefoul WolfLinux distribution that uses217 MB RAMwithDevuanas base (nosystemd(1)here) andXLibreX11 server along withIceWMwindow manager.  For the record – theLundukepost states200 MB RAMbutXLibreDevquotes a post where exactly217 MB RAMis reported. LaterLundukeeven posted a video about it.  As I use similarly low resource setup withOpenbox/Tint2/Dzen2setup (documentedFreeBSD Desktophere) I was wondering … how low can I go with FreeBSD RAM usage.  Lets try … TheTable of Contentsis as follows. I wanted to use most recent FreeBSD so I used15.0-RELEASEversion – including theTech PreviewPKGBASEsetup for FreeBSDBase System.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The percentage of Show HN posts is increasing, but their scores are decreasing", "url": "https://snubi.net/posts/Show-HN/", "content": "The percentage of Show HN posts is increasing, but their scores are decreasing.  Last update: 2026-01-14 Recently, I felt like I was seeing more “Show HN” stories, and many of which were generated with LLMs. So I analyzed the data to see if that was true. Also I included the average score per month to see if people enjoy seeing them (because I don’t :P). Stories in 2026 was omitted. 1) It’s only 13 days, 2) Scores are not stable yet. Left axis:show_hn_ratio(show_hn / story * 100) Right axis:average_show_hn_scoreandaverage_story_score  With LLM timeline  Disclaimer: I am neither a data scientist nor a statistician. Some nuances may have been lost in translation.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "California is free of drought for the first time in 25 years", "url": "https://www.latimes.com/california/story/2026-01-09/california-has-no-areas-of-dryness-first-time-in-25-years", "content": "California is free of drought for the first time in 25 years. This is read by an automated voice. Please report any issues or inconsistencieshere. After experiencing one of thewettest holiday seasonson record, still soggy California hit a major milestone this week — having zero areas of abnormal dryness for the first time in 25 years. The data, collected by theU.S. Drought Monitor, is a welcome nugget of news for Golden State residents, who in the last 15 years alone have lived through two of the worst droughts on record, the worst wildfire seasons on record and the most destructive wildfires ever. Right now, the wildfire risk across California is “about as close to zero as it ever gets,” and there is likely no need to worry about the state’s water supply for the rest of the year, said UC climate scientist Daniel Swain. Currently, 14 of the state’s 17 major water supply reservoirs are at 70% or more capacity, according to theCalifornia Department of Water Resources.      ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Are arrays functions?", "url": "https://futhark-lang.org/blog/2026-01-16-are-arrays-functions.html", "content": "Are arrays functions?. When I was a youngster first perusing theHaskell documentation for\narrays,\nI was amused to find the following description of just what these mysterious\nthings might be: Haskell provides indexable arrays, which may be thought of as functions whose\ndomains are isomorphic to contiguous subsets of the integers. I found this to be a hilariously obtuse and unnecessarily formalist description\nof a common data structure. Now, older, wiser, and well ensconced in the ivory\ntowers of academia, I look at this description and think that it is actually a\nwonderful definition of the essence of arrays! And given that this sentence\nstill lingers in my thoughts so many years later, who can say that it is not\nactually a far better piece of documentation than some more prosaic description\nmight have been? To a language designer, the correspondence between arrays and functions (for itdoesexist, independent of whether you think it is a useful way to document\nthem) is alluring, for one of the best ways to improve a language is to make it\nsmaller. Our goal is not to unify therepresentationof arrays and functions,\nof course - nobody would seriously claim that representing an array via someChurch-encodingis a good idea\nin a supposedly practical programming language. Instead, what might be\nworthwhile considering is what consequences might arise from unifying arrays and\nfunctions at the syntax or type level, and why Futhark ultimately has not done\nso. There is some prior work to consider. The array languageKhas a syntactic unification of\narrays and functions, as both are indexed/applied with the notationf[x]. This\nis however pretty much where the correspondence stops. As an APL derivative, K\nprogramming is based on bulk operations on entire arrays, rather than\nelement-at-a-time programming, and the operators that perform these bulk\noperations cannot be applied to functions. And of course, Khas no type\nsystem,\nso the correspondence is purely syntactic. Dexis a research languagepreviously covered on this channel, which\nalso leverages the array-function correspondence, although mostly at the\nconceptual level such that theyfeelsimilar. As a starting point, a function\nfromatobin Dex uses the conventional notationa -> b, while an array\nwith index typeaand element typebis writtena => b. It is required\nthatais a type that is isomorphic to a contiguous subset of the integers,\nand hence an array type in Dex can really be thought of as a precomputed and\nefficiently represented function. Anonymous functions are written as\\x->e,\nwhile arrays are constructed asfor x.e. Arrays are transformed using a\n“pointed” style, using explicit indexing, similar to how functions are defined\nwith named parameters that are then passed to other functions. Many of the common function operations have a nice interpretation for arrays as\nwell. For example, currying/uncurrying is equivalent to unflattening/flattening\nan array - consider how currying(a,b) -> ctoa -> b -> creally is the\nsame as going from(a,b) => ctoa => b => c. Partial application is like\nfixing a dimension. Flipping the parameters of a function is like transposing an\narray. Composition is like applying a permutation array to another. It is very\ninteresting to me how this line of thinking encourages recognising common\npatterns and interpreting them differently. It is particularly interesting\nbecause arrays and functions fundamentally are completely different types in\nDex, with few facilities provided for using them via a common abstraction (e.g.\nthere is notransposefunction that works for both), but merely through\nsuggestive syntax andfeelis the programmer encouraged to think in different\nways. Now let us consider to which extent a unification of arrays and functions might\nbe viable in Futhark. First, there is no hope of unification at the type level.\nTo allow for efficientdefunctionalisation, Futhark\nimposes restrictions on how functions can be used; for example banning returning\nthem from branches. These restrictions are not (and ought not be!) imposed on\narrays, and so unification is not possible. Also, in Futhark an array type such\nas[n]f64explicitly indicates its size (and consequently the valid indices),\nwhich caneven be extracted at\nrun time. This is not possible with\nfunctions, and making it possible requires us to move further towards dependent\ntypes - which may of course be a good idea anyway. On to syntax. It would be no great challenge to replacea[i]witha i. While\nit looks strange to me, any sort of change to notation looks strange initially,\nand so it is not something I will dwell on overmuch. The main challenge is\nactuallyslicing. Futhark supports a fairly conventional Python-like notation\nfor array slices, namelya[i:j]. This does not have such a simple\ncorrespondence with function application syntax. One solution would be to allow\nthe application of an array to an entireindex array, rather than just a\nsingle index, producing an array of the same shape. That is, the applicationa [i, j, k]would be equivalent to[a[i], a[j], a[k]]. Since Futhark already\nhas a decent notation for constructing ranges, this would allow the slicea[i:k]to be writtena(i..<k)(the parentheses solely for precedence\nreasons). Of course, this could also be allowed using the existing bracket\nsyntax, merely by allowinga[i]whereiis an array. The operational guarantees are a little trickier to wrangle. Currently, slicing\nis guaranteed to be an essentially free operation that merely fiddles with thearray metadata. However, this cannot be\nguaranteed when slicing with an arbitrary indexing array, as there may be no\nexpressible pattern to the indices, which may contain duplicates, arbitrary\nholes, etc - in fact, it fully generalises filtering and expansion. The compiler\nwould have to put in significant work to detect and exploit the efficient cases\ncorresponding to standard slices, and such reverse-engineering of programmer\nintent isantithetical to the Futhark\nphilosophy. We would much\nrather exploit what the programmer has actuallystated, rather than try to\nread between the lines for what they mightmean.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Unix Pipe Card Game", "url": "https://punkx.org/unix-pipe-game/", "content": "The Unix Pipe Card Game. Programming Time, which is a game to teach python and some more fundamental algorithms, from hash tables to RSA The C Pointer Game - Pointers, Arrays and Strings, a game to teach kids to look at the computer memory and understand references and values 4917, a game to teach kids machine code and how the CPU works with memory and registers The Unix Pipes Game - Process Substitution, an expansion of the Unix Pipes Game to teach process substitution and also:paste, tr, cut, bc RunLength Encoding for Kids, small cards \"game\" to explain runlength encoding PUNK0 - The Function Composition Card Game, use cards to manipulate a list and use its values to win the game PROJEKT: OVERFLOW, RISCV assembler boardgame Programming for kids, a log of my journey of teaching my daughter how to code", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The space and motion of communicating agents (2008) [pdf]", "url": "https://www.cl.cam.ac.uk/archive/rm135/Bigraphs-draft.pdf", "content": "The space and motion of communicating agents (2008) [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Do you have any evidence that agentic coding works?", "url": "item?id=46691243", "content": "Ask HN: Do you have any evidence that agentic coding works?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Agentic AI Handbook: Production-Ready Patterns", "url": "https://www.nibzard.com/agentic-handbook", "content": "The Agentic AI Handbook: Production-Ready Patterns. TheGitHub repositoryfor “Awesome Agentic Patterns” had been growing steadily since its launch. But around Christmas, the growth chart went vertical. In just a few days, the repository jumped from relative obscurity to nearly 2,500 stars. Thewebsitetraffic mirrored this spike. Something had clicked. But the real story wasn’t in the metrics—it was inwhowas talking about AI agents. Linus Torvalds, creator of Linux and Git, wrote about using AI coding agents for “vibe coding” and programming guitar pedal effects. Think about that for a second. The person who literally invented the version control system that powers modern software development was publicly embracing agents. Tobias Lütke, CEO of Shopify and already deep into agent-assisted development, declared it his “most productive time.” This from someone running one of the world’s largest e-commerce platforms. Perhaps most telling was Armin Ronacher, creator of Flask—one of the most respected voices in Python. He had been skeptical of coding agents, publicly raising concerns about their limitations. Then, seemingly overnight, his stance shifted. He started promoting agent-assisted workflows, documenting his learnings, and acknowledging that the technology had crossed a threshold. Here’s what all these stories have in common:the holidays gave people something that everyday life rarely provides—dedicated time. Learning to work effectively with AI agents isn’t something you pick up in five minutes between meetings. It requires: During the work year, these activities compete with deadlines, meetings, and the relentless pressure to ship. During the holidays, with meetings suspended and project urgency dialed down, developers finally had the bandwidth to actuallylearn. This repository, with its 113 patterns collected from real production systems, became the curriculum that accelerated that learning. Each pattern represented a battle-tested solution—something that worked outside the demo environment and in the messy reality of production code. Another phenomenon that exploded during the holidays was the “Ralph Wiggum coding loop”—named after the Simpsons character who means well but misses context. Asghuntley describes it, this describes the cycle where an agent starts working on something, seems productive, but gradually drifts off-course because it lacks the deeper context that a human would implicitly understand.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Maintenance: Of Everything, Part One", "url": "https://press.stripe.com/maintenance-part-one", "content": "Maintenance: Of Everything, Part One. The first in a multi-volume work,Maintenance: Of Everything, Part Oneoffers a comprehensive overview of the civilizational importance of maintenance. The book explores the insights that can be gleaned from the maintenance of sailboats, vehicles, and weapons, with absorbing detours into the evolution of precision in manufacturing, the enduring importance of manuals, sustainment in the military, and the never-ending battle against corrosion.Maintenance: Of Everythingis a wide-ranging and provocative call to expand what we mean by “maintenance.” It invites us to understand not only the profound impact maintenance has on our daily lives but also why taking responsibility for maintaining something—whether a motorcycle, a monument, or our planet—can be a radical act. Stewart Brand is the cofounder and president of The Long Now Foundation. He created and edited the National Book Award-winningWhole Earth Catalogfrom 1968 to 1998. His books includeThe Media Lab(1987),How Buildings Learn(1994),The Clock of the Long Now(1999), andWhole Earth Discipline(2009). He was the subject of the documentaryWe Are As Gods(2020). Stewart Brand makes a persuasive case that keeping the human show on the road through well-planned maintenance is as vital and as fascinating a task as innovation and discovery themselves. A deliciously good book. Matt Ridley author ofThe Rational Optimist Once again, Stewart Brand reframes our worldview with a new perspective. You may not imagine you would be interested in rust, Soviet tanks, or tricked-out Model Ts—that is, until Brand reexamines them through the lens of maintenance.Maintenance: Of Everythingis destined to be a classic. Danny Hillis cofounder of Applied Invention Stewart Brand is back with a manifesto on maintenance, the tool that empowers all tools. Preventative maintenance, deferred maintenance, and emergency maintenance: this much-needed, no-nonsense treatise illuminates the difference, and why it counts. George Dyson", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Our approach to age prediction", "url": "https://openai.com/index/our-approach-to-age-prediction/", "content": "Our approach to age prediction", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Agent Skills Leaderboard", "url": "https://skills.sh", "content": "Show HN: Agent Skills Leaderboard. The Open Agent Skills Ecosystem Skills are reusable capabilities for AI agents. Install them with a single command to enhance your agents with access to procedural knowledge. vercel-labs/agent-skills vercel-labs/agent-skills remotion-dev/skills expo/skills expo/skills expo/skills expo/skills anthropics/skills", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Lunar Radio Telescope to Unlock Cosmic Mysteries", "url": "https://spectrum.ieee.org/lunar-radio-telescope", "content": "Lunar Radio Telescope to Unlock Cosmic Mysteries. The catch: It will have to be on the moon Astronomer Jack Burns has spent four decades working to place a radio telescope on the moon. The first one is finally scheduled to launch in early 2027. Isolation dictates where wego to see into the far reaches of the universe. TheAtacama Desertof Chile, the summit ofMauna Keain Hawaii, the vast expanse of theAustralian Outback—these are where astronomers and engineers have built the great observatories and radio telescopes of modern times. The skies are usually clear, the air is arid, and the electronic din of civilization is far away. It was to one of these places, in the high desert of New Mexico, that a young astronomer namedJack Burnswent to study radio jets and quasars far beyond the Milky Way. It was 1979, he was just out of grad school, and theVery Large Array, a constellation of 28 giant dish antennas on an open plain, was a new mecca of radio astronomy. But the VLA had its limitations—namely, that Earth’s protective atmosphere and ionosphere blocked many parts of the electromagnetic spectrum, and that, even in a remote desert, earthly interference was never completely gone. Could there be a better, even lonelier place to put a radio telescope? Sure, a NASA planetary scientist namedWendell Mendell, told Burns: How about the moon? He asked if Burns had ever thought about building one there. “My immediate reaction was no. Maybe even hell, no. Why would I want to do that?” Burns recalls with a self-deprecating smile. His work at the VLA had gone well, he was fascinated by cosmology’s big questions, and he didn’t want to be slowed by the bureaucratic slog of getting funding to launch a new piece of hardware. But Mendell suggested he do some research and speak at a conference on future lunar observatories, and Burns’s thinking about a space-based radio telescope began to shift. That was in 1984. In the four decades since, he’s published more than500 peer-reviewed paperson radio astronomy. He’s been anadvisertoNASA, the Department of Energy, and the White House, as well as a professor and a university administrator. And while doing all that, Burns has had an ongoing second job of sorts, as a quietly persistent advocate for radio astronomy from space. And early next year, if all goes well, a radio telescope for which he’s a scientific investigator will be launched—not just into space, not just to the moon, but to the moon’s far side, where it will observe things invisible from Earth. “You can see we don’t lack for ambition after all these years,” says Burns, now 73 and a professor emeritus ofastrophysicsatthe University of Colorado Boulder.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Disaster planning for regular folks (2015)", "url": "https://lcamtuf.coredump.cx/prep/index-old.shtml", "content": "Disaster planning for regular folks (2015). Written bylcamtuf@coredump.cx, Dec 2015, minor updates Jul 2021. Buy the book instead!Practical Doomsdayis an in-depth, data-packed guide\nto rational emergency preparedness. The book offers deeper and more polished insights on most of the topics covered on this page. For example,\nabout 40 pages are devoted to financial planning alone - from cash reserves, to insurance policies, to commodity derivatives. You can get it onAmazon, order fromBarnes & Noble, or visit your\nfavorite book place. Sample chapter is availablehere. The prepper culture begs to be taken with a grain of salt. In the public\nconsciousness, its has all the makings of a doomsday cult:\na tribe of unkempt misfits who hoard gold bullion, study herbalism,\nand preach about the imminent collapse of our society. Today, most of us see such worries as absurd. It's not that life-altering disasters are\nrare: every year, we hear about millions of people displaced by wildfires, earthquakes,\nhurricanes, or floods. Heck, not a decade goes by without at least one first-class\ndemocracy lapsing into armed conflict or fiscal disarray. But having grown up in a period\nof prosperity and calm, we find it difficult to believe that an episode of bad weather or a currency crisis\ncould upend our lives. I suspect that we dismiss such hazards not only because they seem surreal, but also because\nworrying about them can make one feel helpless and lost. What's more, we tend to follow the\nsame instincts to tune out far more pedestrian and avoidable risks. For example, \nmost of us don't plan ahead for losing a job, for dealing with a week-long water outage, or\nfor surviving the night if our home goes up in smoke. Quite often, our singular strategy for dealing with such dangers is to hope for the\ngovernment to bail us out. But no matter if our elected officials prefer to school us with\npassages from Milton Friendman or from Thomas Piketty, the hard truth is that no state can provide\na robust safety net for all of life's likely contingencies; in most places, government-run social\nprograms are severely deficient in funding, in efficiency, and in scope. Large-scale disasters\npit us against even worse odds. From New Orleans in 2005 to Fukushima in 2011, there are\ncountless stories of people left behind due to political dysfunction, poorly allocated\nresources, or lost paperwork. The purpose of this guide is to combat this mindset of learned helplessness by\npromoting simple, level-headed, personal preparedness techniques that are easy to\nimplement, don't cost much, and will probably help cope with whatever life throws our way. \nMore important, they don't get in the way of enjoying your everyday life - and instead of \nfeeding anxieties, they should make it easier to detach from the doom-and-gloom of 24-hour news. Effective preparedness can be simple, but it has to be rooted in an honest and\nsystematic review of the risks one is likely to face. Plenty of newcomers begin\nby shopping for ballistic vests and night vision goggles; they would be better\nserved by grabbing a fire extinguisher, some bottled water, and then putting the rest of\ntheir money in a rainy-day fund. To avoid being overwhelmed when trying to enumerate risks, I found that it's best to focus on\nbroad outcomes instead of trying to envision every single way for things to go south.\nFor example, it should not matter if one is laid off because of a downsizing, because\ntheir new boss hates them, or because the coworkers finally catch them stealing paperclips. The\noutcome is the same: they are out of a job and urgently need a way to pay the bills.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Building Robust Helm Charts", "url": "https://www.willmunn.xyz/devops/helm/kubernetes/2026/01/17/building-robust-helm-charts.html", "content": "Building Robust Helm Charts. In my current work, there is often the need to deploy a similar application\nstack in various configurations, to several environments. Each configuration may\nvary in terms of scale, uptime requirements and feature flagging. Due to a lot\nof flux in infrastructure set up, each environment is also not equivalent. On\ntop of this, there are obviously financial requirements to run all of this as\ncheaply as possible. Kubernetes and helm templating are valuable tools in this\nsituation, they allow us to create a configuration blueprint with the details\nabstracted invalues.yamlfiles. Let’s start with the basics, helm provides ahelm lintcommand which performs\nchecks You can run this with your different values.yaml files to ensure that all your\nconfigurations are compliant. It’s also a good idea to use thehelm templatecommand to actually check that\nhelm is able to render your templates. I like to compare helm templating with html templating tools like JSX. This\nallows front end developers to create reusable components usable throughout\npages of a web application, A button component for example can have many states,\nprimary, secondary, loading, disabled, light or dark mode. Each state may also look different depending on the size/type of device your are\nbrowsing the site with. Each of these states represents differences in many\nparameters (font size, colour, gradient, opacity, border, padding, margin,\nwidth, height, etc). These complexities are abstracted away giving the consuming\ncode the list of states to chose from, so that they can write code like this. Under the hood of course many aspects of the CSS or HTML code will be impacted\nby the change of state so you often end up with different parts of the markup\nhaving conditionals on the same check. Just in this contrived example you already have 2 different things being\ncontrolled by the state property with 2 separate checks, the CSS classes and the\npresence of the loading icon. This is quite similar to the situation you end up templating in YAML with helm.\nConsider an application that has optional persistent storage. You could quite\neasily imagine a boolean property in yourvalues.yamlfile calledpersistent. Under the hood this has many implications likely affecting\ndifferent files. That’s 5 separateifblocks that need to be in your templates.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IP Addresses Through 2025", "url": "https://www.potaroo.net/ispcol/2026-01/addr2025.html", "content": "IP Addresses Through 2025. IP Addresses through 2025January 2026 It's time for another annual roundup from the world of IP addresses. Let’s see what has changed in the past 12 months in addressing the Internet and look at how IP address allocation information can inform us of the changing nature of the network itself. Back around 1992, the IETF gazed into their crystal ball and tried to understand how the Internet was going to evolve and what demands that would place on the addressing system as part of the “IP Next Generation” study.  The staggeringly large numbers of connected devices that we see today were certainly within the range predicted by that study. The assumption made at the time was that we would continue to use much the same IP protocol architecture, including the requirement that each connected device was assigned a unique IP address, and the implication was that the 32-bit address field defined in version 4 of the IP protocol was clearly going to be inadequate to cope with the predicted number of connected devices. A span of 4 billion address values was just not large enough. We concluded at the time that the only way we could make the Internet work across such a massive pool of connected devices was to deploy a new IP protocol that came with a massively larger address space. It was from this reasoning that IPv6 was designed, as this world of abundant silicon processors connected to a single public Internet was the scenario that IPv6 was primarily intended to solve. The copious volumes of a 128-bit address space were intended to allow us to uniquely assign a public IPv6 address to every such device, no matter how small, or in whatever volume they might be deployed. But while the Internet has grown at amazing speeds across the ensuing 33 years, the deployment of IPv6 has proceeded at a more measured pace. There is still no evidence of any common sense of urgency about the deployment of IPv6 in the public Internet, and still there is no common agreement that the continued reliance on IPv4 is failing us. Much of the reason for this apparent contradiction between the addressed device population of the IPv4 Internet and the actual count of connected devices, which is of course many times larger, is that through the 1990's the Internet rapidly changed from a peer-to-peer architecture to a client/server framework. Clients can initiate network transactions with servers but are incapable of initiating transactions with other clients. Servers are capable of completing connection requests from clients, but cannot initiate such connections with clients. Network Address Translators (NATs) are a natural fit to this client/server model, where pools of clients share a smaller pool of public addresses, and only require the use of an address once they have initiated an active session with a remote server. NATs are the reason why a pool of excess of 30 billion connected devices can be squeezed into a far smaller pool of some 3 billion advertised IPv4 addresses. Services and Applications that cannot work behind NATs are no longer useful in the context of the public Internet and no longer used as a result. In essence, what we did was to drop the notion that an IP address is uniquely associated with a device's identity, and the resultant ability to share addresses across clients largely alleviated the immediacy of the IPv4 addressing problem for the Internet. However, the pressures of this inexorable growth in the number of deployed devices connected to the Internet implies that the even NATs cannot absorb these growth pressures forever. NATs can extend the effective addressable space in IPv4 by up to 32 ‘extra’ bits using mapping of the 16-bit source and destination port fields of the TCP and UDP headers, and they also enable the time-based sharing of these public addresses. Both of these measures are effective in stretching the IPv4 address space to encompass a larger client device pool, but they do not transform the finite IP address space into an infinitely elastic resource. The inevitable outcome of this process, if it were to be constrained to operate solely within IPv4, is that we would see the fragmenting of the IPv4 Internet into a number of disconnected parts, probably based on the service ‘cones’ of the various points of presence of the content distribution servers, so that the entire concept of a globally unique and coherent address pool layered over a single coherent packet transmission realm would be foregone. Alternatively, we may see these growth pressures motivate the further deployment of IPv6, and the emergence of IPv6-only elements of the Internet as the network itself tries to maintain a cohesive and connected whole. There are commercial pressures pulling the network in both of these directions, so it’s entirely unclear what path the Internet will follow in the coming years, but my (admittedly cynical and perhaps overly jaded) personal opinion lies in a future of highly fragmented network, as least in terms of the underlying packet connectivity protocol. Can address allocation data help us to shed some light on what is happening in the larger Internet? Let’s look at what happened in 2025. It appears that the process of exhausting the remaining pools of unallocated IPv4 addresses is proving to be as protracted as the process of the transition to IPv6, although by the end of 2021 the end of the old registry allocation model had effectively occurred with the depletion of the residual pools of unallocated addresses in each of the Regional Internet Registries (RIRs).", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Apples, Trees, and Quasimodes", "url": "https://systemstack.dev/2025/09/humane-computing/", "content": "Apples, Trees, and Quasimodes. A while back, Ars Technica publisheda thoughtful piece about Jef Raskin, tracing his long pursuit of the “humane computer” and the cul-de-sacs where that pursuit ended. It’s a generous, well-told account of the designer who wanted to make machines simpler, kinder, and more aligned with the way people actually think. But part of what makes Raskin interesting is that his story isn’t just Apple’s story. He came out of the same cultural current John Markoff chronicled inWhat the Dormouse Said—the Bay Area tradition that treated computers not as office appliances but as tools for thought, instruments of liberation. Read that way, the Canon Cat and Raskin’s other projects aren’t just an eccentric side quest from a frustrated Apple veteran. It’s evidence of how far the humane ideal could stretch, and how quickly it ran up against the limits of commercial computing. Apple couldn’t deliver Raskin’s vision then, and it can’t deliver it now. Neither can any other big platform company. If we want to understand why, and what Raskin still tells us about humane computing, we have to put him back in the longer lineage he belonged to, and look at how his version of the dream carried that vision but also narrowed it. What the Dormouse Saiddocuments how the Bay Area counterculture  shaped early personal computing. LSD, communes, systems theory, amorphous defense research contracts, and Engelbart’s “augmentation” experiments all swirled together in a weird scene that accidentally (or maybenotso accidentally) created much of the modern world. The story usually gets told with a neat list:Engelbart’s demo, Nelson’sXanaduhypertext, Kay’sDynabook, Brand’sWhole Earth. Xerox PARC, Steve Jobs, the World Wide Web. The familiar pantheon. But that version turns a messy, improvisational moment into a plaque. Engelbart’s system needed a whole research staff just to operate; Nelson’s Xanadu was (and is) more sermon than software; Kay’s Dynabook lived mostly on paper; Brand mostly supplied vocabulary and vibe. What bound them together wasn’t working code so much as the conviction that computers could be more than appliances and calculators, even if no one agreed on what “more” meant. Ultimately all these weird white guys had a futurist vision: computers could beliberation machines.They weren’t just for business automation or scientific number-crunching; they could be deployed to expand consciousness and reshape how people thought and worked. Raskin belonged to this current. Before Apple, he was an artist and a musician. He brought a humanist’s suspicion of machine logic into the design lab. He argued forhumaneinterfaces: modeless, predictable, low-friction, focused on the human first. He wasn’t a prophet on his own crying in the wilderness so much as another strand of the same weave. That said, his role was different than that of some of these other figures. He tried to pull those ideals out of the lab and into machines ordinary people might actually use. The Macintosh began under his hand, though what shipped was less a tool for thought than a polished derivative—what you might call a “popular religion” of computing, stripped of the harder doctrines. The Canon Cat and its predecessors were Raskin’s counterargument: humane, text-first systems that tried to carry the spirit of theDormousetradition into the commercial world without sanding off everything that made it strange. It sort of worked, but only sort of. Raskin’s principles are laid out most clearly in 2000’sThe Humane Interface, but he’d been developing them since the late 1970s:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Provably unmasking malicious behavior through execution traces", "url": "https://arxiv.org/abs/2512.13821", "content": "Provably unmasking malicious behavior through execution traces. arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community?Learn more about arXivLabs. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Proof of Concept to Test Humanoid Robots", "url": "https://thehumanoid.ai/humanoid-and-siemens-completed-a-proof-of-concept-to-test-humanoidrobots-in-industrial-logistics/", "content": "Proof of Concept to Test Humanoid Robots.  London, The UK—January 15, 2026— Humanoid, a UK-based AI and robotics company, and Siemens, a leading technology company, have successfully completed a proof of concept (POC) demonstrating the use of humanoid robots in industrial logistics. Humanoid’s HMND 01 wheeled Alpha robot was deployed in real operations at a Siemens facility, marking a significant step toward the deployment of humanoid robots in industrial settings. This successful POC is the first step in a broader partnership between the two companies to test and validate how humanoid robots can be used in real-world environments. The POC focused on a tote-to-conveyor destacking task within Siemens’ logistics process. In this use case, the robot autonomously picked totes from a storage stack, transported them to a conveyor, and placed them at the designated pickup point for human operators. This sequence was repeated until the stack was fully empty, which demonstrated how humanoid robots can take on repetitive logistics tasks.  The POC was structured in two phases. The first one focused on in-house development and demonstration and has already been completed. During this stage, the Humanoid team built a physical twin to support testing, optimization, and rapid iteration throughout the POC. The second phase involved a two-week on-site deployment at the Siemens Electronics Factory in Erlangen, where partners assessed the robots in a real-world production environment. This joint POC measured both performance and reliability of humanoid robots under autonomous operation. Target metrics were met in full and included a throughput of 60 tote moves per hour, operation with two different tote sizes, continuous autonomous task execution for more than 30 minutes, uptime exceeding 8 hours. The team also evaluated the POC’s success using the following indicators: overall pick and place success rate and autonomous pick and place success rate, both above 90%. Humanoid and Siemens see this POC as a first step toward a long-term strategic collaboration — beyond the initial POC, the companies are open to expanding the scope and adding additional use cases. Partners also may progress toward a broader rollout, deploying a greater number of humanoid robots across Siemens’ facilities, based on the robot’s specific skill set. “At Humanoid, we are a commercially driven company. Our focus is on creating robots that deliver measurable value in real-world settings. Working closely with industrial and technology partners allow us to validate our systems against real operational requirements and understand which use cases matter outside the lab. This joint POC with Siemens showed clear potential for practical deployment of humanoid robots. We see them move steadily toward the real world, and partnerships like this one help accelerate that transition,”noted Artem Sokolov, founder and CEO of Humanoid. Stephan Schlauss, Global Head of Manufacturing Motion Control, Siemens AG, said: “As Siemens’ customer zero, the Electronics Factory Erlangen is excited to partner with the Humanoid team. We’re tackling production automation, discovering new opportunities for Siemens, and are eager to advance this promising technology across our factory network to deliver customer value.” About Humanoid", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nova Launcher added Facebook and Google Ads tracking", "url": "https://lemdro.id/post/lemdro.id/35049920", "content": "Nova Launcher added Facebook and Google Ads tracking", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Who owns Rudolph's nose?", "url": "https://creativelawcenter.com/copyright-rudolph-reindeer/", "content": "Who owns Rudolph's nose?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Prediction markets are ushering in a world in which news becomes about gambling", "url": "https://www.theatlantic.com/technology/2026/01/america-polymarket-disaster/685662/", "content": "Prediction markets are ushering in a world in which news becomes about gambling. For the past week, I’ve found myself playing the same23-second CNN clipon repeat. I’ve watched it in bed, during my commute to work, at the office, midway through making carrot soup, and while brushing my teeth. In the video, Harry Enten, the network’s chief data analyst, stares into the camera and breathlessly tells his audience about the gambling odds that Donald Trump will buy any of Greenland. “The people who are putting their money where their mouth is—they are absolutely taking this seriously,” Enten says. He taps the giant touch screen behind him and pulls up a made-for-TV graphic: Based on how people were betting online at the time, there was a 36 percent chance that the president would annex Greenland. “Whoa, way up there!” Enten yells, slapping his hands together. “My goodness gracious!” The ticker at the bottom of the screen speeds through other odds: Will Gavin Newsom win the next presidential election? 19 percent chance. Will Viktor Orbán be out as the leader of Hungary before the end of the year? 48 percent chance.  These odds were pulled from Kalshi, which hilariouslyclaimsnot to be a gambling platform: It’s a “prediction market.” People go to sites such as Kalshi and Polymarket—another big prediction market—in order to put money down on a given news event. Nobody would bet on something that they didn’t believe would happen, the thinking goes, and so the markets are meant to forecast the likelihood of a given outcome. Listen: Prediction markets and the “suckerification” crisis, with Max Read Prediction markets let you wager on basically anything. Will Elon Musk fatheranother babyby June 30? WillJesus returnthis year? Will Israelstrike Gaza tomorrow? Will thelongevity guruBryan Johnson’s next functional sperm count be greater than “20.0 M/ejac”? These sites have recently boomed in popularity—particularly amongterminally online young menwho trade meme stocks and siphon from their 401(k)s to buy up bitcoin. But now prediction markets are creeping into the mainstream. CNNannounced a dealwith Kalshi last month to integrate the site’s data into its broadcasts, which has led to betting odds showing up in segments about Democrats possibly retaking the House, credit-card interest rates, and Federal Reserve Chair Jerome Powell. At least twice in the past two weeks, Enten has told viewers about the value of data from people who are “putting their money where their mouth is.”  On January 7, the media giant Dow Jones announced its own collaboration with Polymarket and said that it will begin integrating the site’s odds across its publications, includingThe Wall Street Journal. CNBC has a prediction-market deal, as does Yahoo Finance,Sports Illustrated, andTime. Last week, MoviePassannouncedthat it will begin testing a betting platform. On Sunday, the Golden Globes featured Polymarket’s forecasts throughout the broadcast—because apparently Americans wanted to know whether online gamblers favored Amy Poehler or Dax Shepard to win Best Podcast.  Media is a ruthless, unstable business, andrevenue streams are drying up; if you squint, you can see why CNN or Dow Jones mightsign a contractthat, after all, provides its audience with some kind of data. On air, Enten cites Kalshi odds alongside Gallup polls and Google searches—what’s the difference? “The data featured through our partnership with Kalshi is just one of many sources used to provide context around the stories or topics we are covering and has no impact on editorial judgment,” Brian Poliakoff, a CNN spokesperson, told me in a statement. Nolly Evans, theJournal’s digital general manager, told me that Polymarket provides the newspaper’s journalists with “another way to quantify collective expectations—especially around financial or geopolitical events.” In an email, Jack Suh, a Kalshi spokesperson, told me that the company’s partnerships are designed to inform the public, not to encourage more trading. Polymarket declined to comment. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I'm addicted to being useful", "url": "https://www.seangoedecke.com/addicted-to-being-useful/", "content": "I'm addicted to being useful. When I get together with my friends in the industry, I feel a little guilty about how much I love my job. This is atough timeto be a software engineer. The job was less stressful in the late 2010s than it is now, and I sympathize with anyone who is upset about the change. There are a lot of objective reasons to feel bad about work. But despite all that, I’m still having a blast. I enjoy pulling together projects, figuring out difficult bugs, and writing code in general. I like spending time with computers. But what I really love isbeing useful. The main character in Gogol’s short storyThe Overcoatis a man called Akaky Akaievich1. Akaky’s job is objectively terrible: he’s stuck in a dead-end copyist role, being paid very little, with colleagues who don’t respect him. Still, he loves his work, to the point that if he has no work to take home with him, he does some recreational copying just for his own sake. Akaky is a dysfunctional person. But his dysfunction makes him a perfect fit for his job2. It’s hard for me to see a problem and not solve it. This is especially true if I’m the only person (or one of a very few people) who could solve it, or if somebody is asking for my help. I feel an almost physical discomfort about it, and a corresponding relief and satisfaction when I do go and solve the problem. The work of a software engineer - or at least my work as a staff software engineer - is perfectly tailored to this tendency. Every day people rely on me to solve a series of technical problems3. In other words, like Akaky Akaievich, I don’t mind the ways in which my job is dysfunctional, because it matches the ways in which I myself am dysfunctional: specifically,my addiction to being useful. (Of course, it helps that my working conditions are overallmuchbetter than Akaky’s). I’m kind of like a working dog, in a way. Working dogs get rewarded with treats4, but they don’t do itforthe treats. They do it for the work itself, which is inherently satisfying. This isn’t true of all software engineers. But it’s certainly true of many I’ve met: if not an addiction to being useful, then they’re driven by an addiction to solving puzzles, or to the complete control over your work product that you only really get in software or mathematics. If they weren’t working as a software engineer, they would be getting really into Factorio, or crosswords, or tyrannically moderating some internet community. A lot of the advice I give about working a software engineering job is really about how I’ve shaped my need to be useful in a way that delivers material rewards, and how I try to avoid the pitfalls of such a need. For instance,Protecting your time from predators in large tech companiesis about how some people in tech companies will identify people like me and wring us out in ways that only benefit them.Crushing JIRA tickets is a party trick, not a path to impactis about how I need to be usefulto my management chain, not to the ticket queue.Trying to impress people you don’t respectis about how I cope with the fact that I’m compelled to be useful to some people who I may not respect or even like. There’s a lot of discussion on the internet about whatoughtto motivate software engineers: money and power, producing realvalue, ushering in the AI machine god, and so on. But whatactually doesmotivate software engineers is often more of an internal compulsion. If you’re in that category - as I suspect most of us are - then it’s worth figuring out how you can harness that compulsion most effectively. I think in Russian this is supposed to be an obviously silly name, like “Poop Poopson”. Unfortunately, his low status and low pay catches up with Akaky in the end. His financial difficulty acquiring a new coat for the cold Russian winter (and his lack of backbone) end up doing him in, at which point the story becomes a ghost story. I interpret “technical problem” quite broadly here: answering questions, explaining things, and bug-fixing all count.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Fast Concordance: Instant concordance on a corpus of >1,200 books", "url": "https://iafisher.com/concordance/", "content": "Fast Concordance: Instant concordance on a corpus of >1,200 books. Instantconcordanceon a corpus of\n                over 1,200 public-domain classic books, courtesy ofStandard\n                    Ebooks. Read about how it was implementedhere.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nvidia Stock Crash Prediction", "url": "https://entropicthoughts.com/nvidia-stock-crash-prediction", "content": "Nvidia Stock Crash Prediction.  One of the questions ofthe 2026acxprediction contestis whetherNvidia’s\nstock price will close below $100on any day in 2026. At the time of writing, it\ntrades at $184 and a bit, so going down to $100 would be a near halving of the\nstock value of the highest valued company in the world. It’s an interesting question, and it’s worth spending some time on it. If you just want the answer, my best prediction is that the probability is\naround 10 %. I didn’t expect to get such a high answer, but read on to see how\nwe can find out. Whenwe predicted the Dow Jones index crossing a barrier in 2023, we treated the\nindex as an unbiased random walk. That was convenient, but we cannot do it with\nthe Nvidia question because of one major difference: the time scale. Over short time spans, thevolatility11Or noise, or variation, or standard\ndeviation.of stock movements dominate theirreturn22Or signal, or drift,\nor average change.. This happens because noise grows with the square root of\ntime, while signal grows linearly with time. The plot below illustrates an imaginary amazing investment which has a yearly\nlog-return of 0.3, and a yearly volatility of 0.3.33Readers aware thatstonks\ngo upwill recognise this as an unrealistic Sharpe ratio of 1.0.The middle\nline follows our best guess for how the investment will grow after each year,\nand the outer curves illustrate our uncertainty around the exact value of it.  Early on, we can see that the uncertainty is much bigger than the height to the\ntrend line. Before a year has passed, the exact result is determined more by\nnoise than by growth. Toward the end, growth has taken over and the noise has a\nsmaller effect. One measure of how much volatility there is compared to expected return is the\nsignal-to-noise ratio. It’s computed as", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: TopicRadar – Track trending topics across HN, GitHub, ArXiv, and more", "url": "https://apify.com/mick-johnson/topic-radar", "content": "Show HN: TopicRadar – Track trending topics across HN, GitHub, ArXiv, and more. Pricing Pay per usage mick-johnson/topic-radar Track any topic across the internet and get aggregated, ranked results from multiple sources in one place. Perfect for market research, competitive intelligence, trend monitoring, content creation, and staying updated on any subject. Pricing Pay per usage Rating 5.0 (3) Developer", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Electricity use of AI coding agents", "url": "https://www.simonpcouch.com/blog/2026-01-20-cc-impact/", "content": "Electricity use of AI coding agents. Throughout 2025, we got better estimates of electricity and water use of AI chatbots. There are all sorts of posts I could cite on this topic, but a favorite isthis blog postfrom Our World in Data’s Hannah Ritchie. On the electricity front:  In short, “unless you’re an extreme power user, asking AI questions every day is still a rounding error on your total electricity footprint.” A similar story applies to water usage.This one from Benjamin Todd: The average Americanuses 1600 liters of water per day, so even if you make 100 prompts per day, at 2ml per prompt, that’s only 0.01% of your total water consumption. Using ashower for one secondwould use far more. Generally, these analyses guide my own thinking about the environmental impacts of my individual usage of LLMs; if I’m interested in reducing my personal carbon footprint, I’m much better off driving a couple miles less a week or avoiding one flight each year. This is indeed the right conclusion for users of chat interfaces like chatgpt.com or claude.ai. That said, 1 or 10 or 100 median prompts a day is many orders of magnitude off from my own personal use of LLMs; I likely am, in Hannah Ritchie’s words, an “extreme power user.” I work in software and spend much of my workday driving 2 or 3 coding agents, like Claude Code, at a time. Thus, a much more relevant question for me ishow much energy does a typical Claude Code session consume?(I’m not going to discuss water use in this post.) tl;dr, much more:  There are so many considerations and assumptions and pieces of shorthand one must use along the way to answer this sort of question. I’ll do my best to call those out throughout this post, but please do understand this is still just Sunday afternoon napkin math from Some Guy.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Danish pension fund divesting US Treasuries", "url": "https://www.reuters.com/business/danish-pension-fund-divest-its-us-treasuries-2026-01-20/", "content": "Danish pension fund divesting US Treasuries", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Running Claude Code dangerously (safely)", "url": "https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/", "content": "Running Claude Code dangerously (safely). I’ve been using Claude Code more and more recently. At some point I realized that rather than do something else until it finishes, I would constantly check on it to see if it was asking for yet another permission, which felt like it was missing the point of having an agent do stuff. So I wanted to use Claude Code with the--dangerously-skip-permissionsflag. If you haven’t used it, this flag does exactly what it says: it lets Claude Code do whatever it wants without asking permission first. No more “May I install this package?”, “Should I modify this config?”, “Can I delete these files?” It just… does it. Which is great for flow since I don’t have to worry that it stopped doing stuff just to ask a permission question. But also, you know, dangerous. I like my filesystem intact, so the obvious solution is to not run this thing directly on my OS account. First instinct: throw it in a Docker container. Containers are for isolation, right? Except I want Claude to be able to build Docker images. And run containers. And maybe orchestrate some stuff. So now you need Docker-in-Docker, which means--privilegedmode, which defeats the entire purpose of sandboxing. That means trading “Claude might mess up my filesystem” for “Claude has root-level access to my container runtime.” Not great.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Revive a mostly dead Discord server", "url": "item?id=46697735", "content": "Ask HN: Revive a mostly dead Discord server", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Zen of Reticulum", "url": "https://github.com/markqvist/Reticulum/blob/master/Zen%20of%20Reticulum.md", "content": "The Zen of Reticulum", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Generate animated solar system timelapse videos for any date range", "url": "https://github.com/simondorfman/solar_system_live/", "content": "Show HN: Generate animated solar system timelapse videos for any date range", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Belarus begins a death penalty purge of radio amateurs", "url": "https://steanlab.medium.com/mayday-389f5713fee4", "content": "Belarus begins a death penalty purge of radio amateurs", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "RCS for Business", "url": "https://developers.google.com/business-communications/rcs-business-messaging", "content": "RCS for Business. Engage with customers seamlessly on Android and iOS. Allow your customers to interact with\n  your business directly, and enhance the interaction with distinctive branding and rich\n  media. Measure engagement with read receipts and analytics, and build trust with a\n  'Verified' icon. Learn more Ready to become an RCS for Business partner?Partner interest formarrow_forward Learn more Learn more Learn more Go to Console Manage RCS for Business agents from the Administration Console, and get insights into message activity\n  and billing. Explore the key documentation, or contact us directly for support. Exclusively for registered RCS for Business partners: Access a curated collection of resources to help you champion RCS for Business with your internal teams and brand clients.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "When \"likers'' go private: Engagement with reputationally risky content on X", "url": "https://arxiv.org/abs/2601.11140", "content": "When \"likers'' go private: Engagement with reputationally risky content on X. arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community?Learn more about arXivLabs. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "LG UltraFine Evo 6K 32-inch Monitor Review", "url": "https://www.wired.com/review/lg-ultrafine-evo-6k-32-inch-monitor/", "content": "LG UltraFine Evo 6K 32-inch Monitor Review. 7/10 More pixels isnever a bad thing, right? That's at least part of the reasoning behind the existence of the LG UltraFine Evo 6K. This 32-inch monitor aims to put even more pixels in front of content creators and professionals. Beyond that, it has an attention-grabbing design and off-the-charts image quality. It's one of thebest monitorsyou can buy for content creators, despite some of the unfortunate trade-offs it comes with. The 32-inch LG UltraFine Evo 6K is a very pretty monitor. I wouldn't blame you for mistaking this as an Apple product, given the focus on clean lines, simple shapes, and designerly aesthetic. The extra-wide stand means that the base itself isn’t overly large. Like theApple Studio Display, the flat base provides more usable desk space rather than occupying it. The stand itself has a unique design, too. It resembles the styling Apple uses on theiMacand Studio Display, but it has a textured pattern on the back. It’s gorgeous, though you probably won’t spend a lot of time looking at the back of the monitor unless your desk is in the middle of the room or in command position (if you know, you know). I also like that the back of the cabinet is flat, giving it a sleek look that the rounded backs of typical monitors can’t achieve. Because it uses conventional backlighting, though, it’s not as thin as OLED displays like some ofSamsung’s Odyssey gaming monitors. The UltraFine 6K also has some impressively thin bezels, too, adding to the ultra-modern aesthetic. While they’re not “virtually borderless” as LG states, they’re smaller than the bezels on most monitors I’ve tested. One of my favorite aspects of the UltraFine Evo 6K is the speakers. The pair of included speakers on this might be the best I've heard on a monitor. They are extremely loud and clear. There's even a decent amount of bass in there, to the point where you won't need a pair ofcomputer speakers. One thing I don’t love is the port placement. In favor of keeping everything clean and minimalist, there’s nowhere to hide the ports, so they’re just lined up vertically on the back and in the middle of the monitor. That makes them hard to reach, and there’s no built-in cable management to speak of. The UltraFine Evo 6K sports a decent amount of adjustment, though the design of the hinge limits some of what's possible. It can rotate a full 90 degrees into portrait mode, which is awesome. But the height adjustment is pretty minimal, with a range of only a few inches or so. It also doesn't swivel. As a tall person, I didn't have a hard time finding a comfortable position with this monitor. But for shorter folks, the height of the UltraFine Evo 6K could cause some significant ergonomic problems because of how high up the minimum height is. It does have a VESA mount, so you can avoid all these problems by using a monitor arm. While I don’t like their placement, the ports themselves are powerful. You get the latest standards and speeds, including DisplayPort 2.1, HDMI 2.1, and two Thunderbolt 5 ports. There's a built-inKVM switchfor using the same monitor and peripherals with multiple devices. The UltraFine Evo 6K lacks a few ports that other high-end monitors include, such as a headphone jack, Ethernet jack, or upstream USB-A ports. One of the Thunderbolt ports supports power delivery, although only up to 96 watts. This is less than the 240 watts that's possible withThunderbolt 5. A high-powered laptop like my 16-inch M4 Pro MacBook Pro couldn't hold a charge when plugged in. However, youcandaisy chain multiple 6K monitors together using just a single cable, which feels impossibly great.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "88x31 badge for gen-AI free, 100% human-made works", "url": "https://aspiz.uk/100percenthuman/", "content": "88x31 badge for gen-AI free, 100% human-made works. Use this badge for websites, software, music, art, ... that were created\n            completely by humans with no help from generative artificial intelligence.\n            Use this badge to communicate that you have not used even a little bit of\n            generative AI in your work. What this badge isNOT: This badge is not meant to promote the dis-use of generative AI. I'm not an\n            activist. You may use this badge regardless of your views on AI or the use\n            of generative AI in Art. This work is markedCC0 1.0", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IP over Avian Carriers with Quality of Service (1999)", "url": "https://www.rfc-editor.org/rfc/rfc2549.html", "content": "IP over Avian Carriers with Quality of Service (1999)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Level S4 solar radiation event", "url": "https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026", "content": "Level S4 solar radiation event. G4 Levels were first reached at 2:38pm EST (1938 UTC) on 19 January, 2026 upon CME shock arrival. CME passage is expected to continue through the evening with G4 levels remaining possible.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Scaling long-running autonomous coding", "url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/", "content": "Scaling long-running autonomous coding. Scaling long-running autonomous coding. Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents: This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens. They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not. In my predictions for 2026the other dayI said that by 2029: I think somebody will have built a full web browser mostly using AI assistance, and it won’t even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it’ll get so much easier. I may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach: To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explorethe source code on GitHub. But how well did they do? Their initial announcement a couple of days ago was met withunsurprising skepticism, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo. It looks like they addressed that within the past 24 hours. Thelatest READMEincludes build instructions which I followed on macOS like this: This got me a working browser window! Here are screenshots I took of google.com and my own website:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Reliable Signals of Honest Intent", "url": "https://zanlib.dev/blog/reliable-signals-of-honest-intent/", "content": "Reliable Signals of Honest Intent. Imagine you are working on a significant software update. The software is used by a few thousand professionals, and the release you are working on will fix a few significant issues and greatly improve the quality of work of these people. The update costs money, so it cannot be an automated thing: you need to convince the users of this software to decide that they should update. How would you do it? Would you implement a pop-up window advertising the new release and push it through the automated update channel? Would you write an email? Would youpromptAIfor an email? A story like this was related in Rory Sutherland’s bookAlchemy, and concerned the release of the then-new WindowsNT32-bit server operating system. Microsoft had to figure out how to convince its existing user-base of system administrators to make the switch. But they didn’t ask developers for advice, they hired an advertising agency. And the advertising agency had a different idea. We produced an elaborate box containing a variety of bits and pieces including a free mouse-mat and a pen, inside gratuitously expensive packaging.1Marginnote alchemy1Sutherland, R.Alchemy: The Surprising Power of Ideas That Don’t Make Sense. W. H. Allen 2020; p. 177↩ Why go to such great lengths to advertise a software update? Well, because this story is a subtle case of obliviousness that developers are often guilty of. We tend to get blindsided by thesubject-objectsplit, and think that we only need to convey the objective fact while leaving the subjective interpretation of the value of that fact up to the reader. But it is perhaps not shocking that the subjective perception of value can be influenced—this is, after all, what persuasion is for. It’s actually very natural and expected for it to be influenced, because we are constantly being bombarded by various stimuli. The stimuli fight for our attention, and we have developed a system of complex intuitions to select which stimuli are worth our attention and which are better to ignore. The elaborate box with expensive packaging was a way to signal that what was inside was important, and exclusive enough, to warrant the attention of the recipients. It turned out that it was an unexpected success: almost all of the boxes were opened, and about 10% of the recipients actually tried the new operating system, which for an audience of experienced server administrators is an impressive conversion rate. This is one of many examples fromAlchemyof what Sutherland calls “reliable signals of honest intent.” The attention economy is very asymmetric—there are a lot of things competing for our attention, and we have to spend it to assess which are worth it. Reliable signals of honest intent are one way to quickly judge whether what is behind the signal is worth our time. What’s the equivalent of that expensive box when reading something on the internet? What proves to a reader that you deemed this interaction worth his time?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Overcomplexity of the Shadcn Radio Button", "url": "https://paulmakeswebsites.com/writing/shadcn-radio-button/", "content": "The Overcomplexity of the Shadcn Radio Button. The other day I was asked to update the visual design of radio buttons in a web\napp at work. I figured it couldn't be that complicated. It's just a radio button\nright? Boom! Done. Radio buttons are a built-in HTML element. They've been around for\n30 years. The browser makes it easy. Time for a coffee. I dug into our codebase and realized we were using two React components fromShadcnto power our radio buttons:<RadioGroup>and<RadioGroupItem>. For those unfamiliar with Shadcn, it's a UI framework that provides a bunch of\nprebuilt UI components for use in your websites. Unlike traditional UI\nframeworks like Bootstrap, you don't import it with a script tag ornpm install. Instead you run a command that copies the components into your\ncodebase. Here's the code that was exported from Shadcn into our project: Woof... 3 imports and 45 lines of code. And it's importing a third party icon\nlibrary just to render a circle. (Who needs CSSborder-radiusor the SVG<circle>element when you can add a third party dependency instead?) All of the styling is done by the 30 different Tailwind classes in the markup. I\nshould probably just tweak those to fix the styling issues. But now I'm distracted, annoyed, and curious. Where's the actual<input>?\nWhat's the point of all this? Let's dig a little deeper. The Shadcn components import components from another library called Radix. For\nthose unfamiliar with Radix, it's a UI framework that provides a bunch of\nprebuilt UI components... Wait a second! Isn't that what I just said about Shadcn? What gives? Why do we\nneed both? Let's see what the Radix docs say:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The microstructure of wealth transfer in prediction markets", "url": "https://www.jbecker.dev/research/prediction-market-microstructure", "content": "The microstructure of wealth transfer in prediction markets.  Slot machines on the Las Vegas Strip return about 93 cents on the dollar. This is widely considered some of the worst odds in gambling. Yet on Kalshi, a CFTC-regulated prediction market, traders have wagered vast sums on longshot contracts with historical returns as low as 43 cents on the dollar. Thousands of participants are voluntarily accepting expected values far lower than a casino slot machine to bet on their convictions. Theefficient market hypothesissuggests that asset prices should perfectly aggregate all available information. Prediction markets theoretically provide the purest test of this theory. Unlike equities, there is no ambiguity about intrinsic value. A contract either pays $1 or it does not. A price of 5 cents should imply exactly a 5% probability. We analyzed72.1 million tradescovering$18.26 billionin volume to test this efficiency. Our findings suggest that collective accuracy relies less on rational actors than on a mechanism for harvesting error. We document a systematic wealth transfer where impulsiveTakerspay a structural premium for affirmative \"YES\" outcomes whileMakerscapture an \"Optimism Tax\" simply by selling into this biased flow. The effect is strongest in high-engagement categories like Sports and Entertainment, while low-engagement categories like Finance approach perfect efficiency. This paper makes three contributions. First, it confirms the presence of the longshot bias on Kalshi and quantifies its magnitude across price levels. Second, it decomposes returns by market role, revealing a persistent wealth transfer from takers to makers driven by asymmetric order flow. Third, it identifies a YES/NO asymmetry where takers disproportionately favor affirmative bets at longshot prices, exacerbating their losses. Prediction markets are exchanges where participants trade binary contracts on real-world outcomes. These contracts settle at either $1 or $0, with prices ranging from 1 to 99 cents serving as probability proxies. Unlike equity markets, prediction markets are strictly zero-sum: every dollar of profit corresponds exactly to a dollar of loss. Kalshilaunched in 2021 as the first U.S. prediction market regulated by the CFTC. Initially focused on economic and weather data, the platform stayed niche until 2024. Alegal victoryover the CFTC secured the right to list political contracts, and the 2024 election cycle triggered explosive growth. Sports markets, introduced in 2025, now dominate trading activity. Volume distribution across categories is highly uneven. Sports accounts for 72% of notional volume, followed by politics at 13% and crypto at 5%. Note:Data collection concluded on 2025-11-25 at 17:00 ET; Q4 2025 figures are incomplete. The dataset,available on GitHub, contains7.68 million marketsand72.1 million trades. Each trade records the execution price (1-99 cents), taker side (yes/no), contract count, and timestamp. Markets include resolution outcome and category classification.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "url": "https://github.com/majcheradam/ocrbase", "content": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Aventos – An experiment in cheap AI SEO", "url": "https://www.aventos.dev/", "content": "Show HN: Aventos – An experiment in cheap AI SEO", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Alignment Game (2023)", "url": "https://dmvaldman.github.io/alignment-game/", "content": "The Alignment Game (2023). TLDR; I made a game to align people and priorities in aGoogle Sheet At work as an “executive” I found myself often focused on issues of “alignment,” especially among the other execs. It just turns out as an organization grows, it operates on fractured sets of implicit assumptions. I found there is often little disagreement onwhatthe problems are, but plenty of disagreement onwhichwere more important. People then carry these differences into decision making without revealing their working assumptions, cascading tradeoffs are made and efforts diverge. There was an incredible sense of clarity when everyone could agree on what’s most important in unison, and I wanted to get there. I started by doing the exercise of stack ranking priorities. Sometimes this would just be finger to the wind thinking about issues, sometimes this would mean months of work to assess impact rigorously. I would challenge others in the company to make their own stack rankings. We’d then discuss the differences and try to converge on a shared ordering. This was an incredibly fruitful exercise that led to great conversations. With more than two people though, as with an exec team, there was a need for more process. It turns out there’s a whole branch of mathematics calledvoting theoryall about how to get a plurality of people to agree on a single thing. The concepts ofrun-off elections,“I cut, you choose”division algorithms, andhow medical schools select studentsthrough ranked preferences are all facets of voting theory. In my situation, we had a half dozen stack ranked lists of priorities and we wanted to align people on a single ordering. Turns out, there isno algorithmthat always works! You can always find yourself in a situation where more than half of people want A over B, some other half want B over C, and some other half want C over A, so a majority are upset with any outcome. Each ranking algorithm makes certain tradeoffs. TheKemeny-Young methodis a ranking algorithm that finds the ordering which minimizes total disagreement across all voters. A disagreement is any time one voter chooses A over B and another chooses B over A. One of the voters would need to swap their preferences in order to align, and the Kemeny Young method finds the ordering requiring the fewest swaps. The downsides of the Kemeny Young method come down to it being the “compromise solution.” Half of people may think A is most important and B least, and another half would invert that, and the Kemeny Young method would put it in the middle and upset everyone. Something to be cognizant of. The important bit is not to use the ordering as marching orders, but as a tool for conversation. The benefits of Kemeny Young lies in its interpretability. Because it works by counting pairwise disagreements, you get a natural measure of which items are contentious, as well as which voters are misaligned. It can be said of any two people: “You need to change your mind on X things to align with one another” and between any two priorities: “X voters disagreed on this prioritization.” This makes it easy to identify where consensus already exists versus what we need to debate. We had great success playing the game. Each person would make their ranking in private, we’d gather them all, churn through an algorithm and immediately all implicit tradeoffs are surfaced. We would then meet pairwise to try to align our priorities. This worked especially well at company off-sites and quarterly planning cycles. I’ve since turned the process into aGoogle Sheet. Try it at work or in your personal life!", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "King – man + woman is queen; but why? (2017)", "url": "https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/", "content": "King – man + woman is queen; but why? (2017). 6 Jan 2017 | by Piotr Migdał word2vecis an algorithm that transforms words into vectors, so that words with similar meanings end up laying close to each other. Moreover, it allows us to use vector arithmetics to work with analogies, for example, the famousking - man + woman = queen. I will try to explain how it works, with special emphasis on the meaning of vector differences, at the same time omitting as many technicalities as possible. If you would rather explore than read, here is an interactive exploration by my mentee Julia Bazińska, now a freshman in computer science at the University of Warsaw:  I love letter co-occurrence in the wordco-occurrence. Sometimes a seemingly naive technique gives powerful results. It turns out that merely looking at word coincidences, while ignoring all grammar and context, can provide us insight into the meaning of a word.\nConsider this sentence: A small, fluffy roosety climbed a tree. What’s aroosety? I would say that something like a squirrel since the two words can be easily interchanged. Such reasoning is called thedistributional hypothesisand can be summarized as: a word is characterized by the company it keeps -John Rupert Firth", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Avoiding fan traps in database design and system diagrams", "url": "https://www.ilograph.com/blog/posts/avoid-fan-traps-in-system-diagrams/", "content": "Avoiding fan traps in database design and system diagrams. A fan trap in data modeling occurs when multiple 1:N relations are joined on the “1” side, resulting in information loss. To the uninitiated, this is easiest understood by example: imagine a university with manycolleges, each with manydepartments, which in turn have manyprofessors. If modeled incorrectly, where theprofessorsare given 1:N relations withcollegesinstead ofdepartments, the result is a fan trap: These relations aren’t wrong per se, but the mapping ofprofessorstodepartmentsis lost. The resulting data modeling diagram looks like two hand fans joined at the narrow end, hence the name. A similar problem can occur in system diagramming. When diagramming relations between resources in a system, information can similarly be lost when relations flow through an intermediary resource. In this article, we’ll look at a couple of examples of this problem and three potential fixes. Fan traps are common problems when diagramming event-driven architectures. The defining characteristic of such architectures is that resources communicate viaevents. Events are typically routed through an event broker, which temporarily stores them until they are ready for consumption. This architecture has the added benefit of decoupling the resources, since they no longer communicate directly. When diagrammed, a (highly simplified) event-driven system might look like so: The similarity to fan traps in data modeling should be evident at a glance. The relationships between the message-producing resources (on the left) and message-consuming resources (on the right) are lost because they collapse at the center of the “fan” (the event broker). The diagram implies each producer communicates with all of the consumers, even though this may not be the case: The same problem can emerge when diagramming communication paths in a network: In the (again highly simplified) networking diagram above, node-to-node communications across the network fan out and back in through firewalls, resulting in specific communication paths being lost. One potential solution is to add smaller, discrete resources within the intermediate resource that the edge resources can connect through. In the event-driven architecture example above, adding “topics” allows us to differentiate the communications going through the event broker: With the addition of topics, the individual communication paths can now be discerned:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Apple testing new App Store design that blurs the line between ads and results", "url": "https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/", "content": "Apple testing new App Store design that blurs the line between ads and results. Apple is testing a new design for App Store search ads on iPhone. Some users on iOS 26.3 are noticing that the blue background around sponsored results is no longer shown, blurring the line between what paid ad results look like and the real search results that follow. This means the only differentiator between organic results and the promoted ad is the presence of the small ‘Ad’ banner next to the app icon. Right now, it appears to be in some kind of A/B test phase. We have asked Apple for clarity on the change, and whether this will roll out more widely in the future. It may be related to thecompany’s announcement from Decemberthat App Store search results will soon start including more than one sponsored result for a given search query. The removal of the blue background will mean all of the ads will appear in the list in a more integrated fashion. Of course, this also has the effect of making it harder for users to quickly distinguish at a glance what is an ad and what isn’t, potentially misleading some users into not realising that the first result is a paid ad placement. While not great for user experience, it probably helps increase click-through rates which ultimately boosts Apple’s revenue in its ads business. FTC: We use income earning auto affiliate links.More. Check out 9to5Mac on YouTube for more Apple news: Benjamin develops iOS apps professionally and covers Apple news and rumors for 9to5Mac. Listen to Benjamin, every week, on the Happy Hour podcast. Check outhis personal blog. Message Benjamin overemailorTwitter. The easiest way to get into HomeKit and Apple smart home tech. Great for gifts. Inexpensive, fast, wireless charger for iPhone.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Targeted Bets: An alternative approach to the job hunt", "url": "https://www.seanmuirhead.com/blog/targeted-bets", "content": "Targeted Bets: An alternative approach to the job hunt. The tech job market has been tough, leaving many applicants feeling hopeless. I've seen this first hand in my conversations with dozens of friends and across more than 100 job interviews. Here is my response to these people: you can drastically increase your odds of getting a job by making targeted bets rather than broadly applying and hoping something sticks. A targeted bet begins with focus. Instead of applying broadly, identify 5-10 specific opportunities you genuinely want. In the context of job searching, these are roles where at least one of the following is true: Once the list has been narrowed, your goal is to stand out. Here are a few ways to do that: By narrowing your opportunities, you end up being able to spend more time on each one. Let's assume that a targeted bet increases your chances of getting a job from 1% to 10%. The average number of jobs you'd need to apply to before getting one thus jumps from 100 to just 10! Competitive systems reward effort per attempt, not volume. Targeted bets apply to more than just the job search. I recently scored the first apartment I applied to in a highly-competitive San Francisco neighborhood. I was specific in where and what I was looking for, so when the opportunity came up, I was able to devote lots of time and energy into getting it. I applied just 6 hours after the place came on the market. Seeing that there were lots of people at the tour, I sent a follow up email to the leasing agent explaining how I'd always wanted to live in the neighborhood. If I had been worried about the status of my other applications, I may not have had the time to write that follow up email and secure my apartment. The glory in making targeted bets is that you get to spend more time on the things that you really care about. I would advise against mass-applying to those entry-level jobs you don't really care about and instead start getting in contact with people at your dream job.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Linux kernel framework for PCIe device emulation, in userspace", "url": "https://github.com/cakehonolulu/pciem", "content": "Linux kernel framework for PCIe device emulation, in userspace", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "British redcoat's lost memoir reveals realities of life as a disabled veteran", "url": "https://phys.org/news/2026-01-british-redcoat-lost-memoir-reveals.html", "content": "British redcoat's lost memoir reveals realities of life as a disabled veteran. share this! 111 Tweet Share Email January 14, 2026 by Tom Almeroth-Williams,University of Cambridge edited byStephanie Baum, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treviewed byRobert Egan  ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I set all 376 Vim options and I'm still a fool", "url": "https://evanhahn.com/i-set-all-376-vim-options-and-im-still-a-fool/", "content": "I set all 376 Vim options and I'm still a fool. I set all of Vim’s configuration options. I still feel far from mastery. I first saw someone use Vim during an internship in 2012. I had been coding for many years and I fancied myself pretty good at shortcuts, but I was quickly humbled. I watched in awe as experienced users zipped around the code. A single keystroke could move the cursor halfway across the file to exactly the right spot. Code was ripped apart and reshaped like putty. “Wow,” I thought to myself, and probably said out loud. I vowed to master this editor but I was slow. When I wasn’t accidentally opening some unknown menu, I was taking an uneconomical path through the code. I pressedjtwenty times instead of running20j, or manually deleted code inside parenthesis instead of runningdi(. Sometimes I’d open another text editor to give my mind a break from all the key bindings! Fast-forward to 2025. After tons of practice, I felt much more capable. Codedidfeel more like putty. I was working closer to the speed of thought. I could get code where I wanted much more quickly. 13 years of practice paid off! But Vim still felt clumsy. I was still accidentally opening menus I didn’t recognize. I would do silly things like converting the whole file to lowercase, or trigger some scary error message. “Surely I shouldn’t be making these mistakes,” I thought. What could be done to finally master this editor? That desire for expertise led me on a quest toset all of Vim’s options. I would make an informed decision about all 376 of Vim’s settings and drop them in my.vimrc. In other words, I wanted to 100% Vim. Surely, setting every Vim option would make me the fluent expert I wanted to be…right? I pored over every single Vim option and made a decision. What did the option do, and what did I want it to be set to? My goal was to be thorough; leave no stone unturned. I only set the option after I understood it.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Fence – Sandbox CLI commands with network/filesystem restrictions", "url": "https://github.com/Use-Tusk/fence", "content": "Show HN: Fence – Sandbox CLI commands with network/filesystem restrictions", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Turbopack: Building faster by building less", "url": "https://nextjs.org/blog/turbopack-incremental-computation", "content": "Turbopack: Building faster by building less. Monday, January 19th 2026 Edit. Save. Refresh. Wait… Wait… Wait… Compiling code usually means waiting, butTurbopackmakes iteration loops fast with caching and incremental computation. Not every modern bundler uses an incremental approach, and that’s with good reason. Incremental computation can introduce significant complexity and opportunities for bugs. Caches require extra tracking and copies of data, adding both CPU and memory overhead. When applied poorly, caching can actually make performance worse. Despite all of this, we took on these challenges because we knew that an incremental architecture would be critical to Turbopack’s success. Turbopack is the new default bundler for Next.js, a framework that is used to build some of thelargest web applications in the world. We needed to enable instant builds and a fast as-you-type interactiveReact Fast Refreshexperience, even for the largest and most challenging workloads. Our incremental architecture is core to achieving this. Turbopack’s architecture was built ground-up with caching in mind. Its incremental design is based on over a decade of research. We built on first-hand experience from challenges in implementing caching inwebpackand drew inspiration fromSalsa(which powersRust-AnalyzerandRuff),Parcel, theRust compiler’s query system,Adapton, and many others. Turbopack achieves a fine-grained cache by automatically tracking how internal functions are called and what values they depend on. When something changes we know how to recompute the results with minimal work. Many build systems include explicit dependency graphs that must be manually populated when evaluating build rules. Explicitly declaring your dependency graph can theoretically give optimal results, but in practice it leaves room for errors. The difficulty of specifying an explicit dependency graph means that usually caching is done at a coarse file-level granularity. This granularity does have some benefits: fewer incremental results means less data to cache, which might be worth it if you have limited disk space or memory. An example of such an architecture isGNU Make, where output targets and prerequisites are manually configured and represented as files. Systems like GNU Make miss caching opportunities due to their coarse granularity: they do not understand and cannot cache internal data structures within the compiler. In Turbopack, the relationship between input files and resulting build artifacts isn’t straightforward. Bundlers employ whole-program analysis for dead code elimination (\"tree shaking\") and clustering of common dependencies in the module graph. Consequently, the build artifacts (JavaScript files shared across multiple application routes) form complex many-to-many relationships with input files.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Flux 2 Klein pure C inference", "url": "https://github.com/antirez/flux2.c", "content": "Flux 2 Klein pure C inference", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "From Nevada to Kansas by Glider", "url": "https://www.weglide.org/flight/978820", "content": "From Nevada to Kansas by Glider. We're sorry but WeGlide is a complex Web App and doesn't work without JavaScript enabled. Please enable it and\n        reload this page to continue. Es tut uns leid, aber WeGlide ist eine komplexe Web-App und funktioniert nicht ohne JavaScript. Bitte aktiviere\n        es und lade die Seite erneut um fortzufahren.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Claude Chill: Fix Claude Code's flickering in terminal", "url": "https://github.com/davidbeesley/claude-chill", "content": "Claude Chill: Fix Claude Code's flickering in terminal", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "KISS Launcher – fast launcher for Android", "url": "https://kisslauncher.com/", "content": "KISS Launcher – fast launcher for Android. < 250 kb Optimized for battery life Search everything that you need Faster than ever", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "San Francisco coyote swims to Alcatraz", "url": "https://www.sfgate.com/local/article/san-francisco-coyote-alcatraz-21302218.php", "content": "San Francisco coyote swims to Alcatraz", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Parallel Agentic Search on the Twitter Algorithm", "url": "https://www.morphllm.com/playground/na/warpgrep?repo=xai-org%2Fx-algorithm", "content": "Show HN: Parallel Agentic Search on the Twitter Algorithm", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "X For You Feed Algorithm", "url": "https://github.com/xai-org/x-algorithm", "content": "X For You Feed Algorithm", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Director Gore Verbinski: Unreal Engine is the greatest slip backwards for movie", "url": "https://www.pcgamer.com/movies-tv/director-gore-verbinski-says-unreal-engine-is-the-greatest-slip-backwards-for-movie-cgi/", "content": "Director Gore Verbinski: Unreal Engine is the greatest slip backwards for movie. Remember the glory days of CGI in movies? Terminator 2's liquid metal T-1000, Jurassic Park's stunning dinosaurs, Starship Trooper's swarms of giant arachnids. Not only did the CGI look great then, most of the visual effects in those movies still hold up well today, even decades after they were created. Nowadays, movie fans seem much less impressed by CGI in films. There's a general distaste for a perceived overuse of CGI in favor of practical effects, and there are a lot of complaints that recent CGI is less-convincing and more fake-looking than it used to be, even in the biggest budget films. In an interview withBut Why Tho?, Gore Verbinski, director of The Ring, Rango, and the first three Pirates of the Caribbean films, was asked why visual effects in movies just don't look as good as they used to. \"I think the simplest answer is you’ve seen the Unreal gaming engine enter the visual effects landscape,\" Verbinski said. \"So it used to be a divide, with Unreal Engine being very good at video games, but then people started thinking maybe movies can also use Unreal for finished visual effects. So you have this sort of gaming aesthetic entering the world of cinema.\" Unreal Engine made waves after being used for virtual sets in production of The Mandalorian TV series back in 2020, and usage of the engine has grown more widespread in films over the past few years, such as in The Matrix Resurrections and Ant-Man and the Wasp: Quantumania. That's not good news, according to Verbinski. \"I think that Unreal Engine coming in and replacing Maya as a sort of fundamental is the greatest slip backwards,\" he said. He pointed out the types of visual effects made with Unreal aren't necessarily bad. \"It works with Marvel movies where you kind of know you’re in a heightened, unrealistic reality. I think it doesn’t work from a strictly photo-real standpoint,\" he said. Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. \"I just don’t think it takes light the same way; I don’t think it fundamentally reacts to subsurface, scattering, and how light hits skin and reflects in the same way,\" he said. \"So that’s how you get this uncanny valley when you come to creature animation, a lot of in-betweening is done for speed instead of being done by hand.\" In his new movie, science fiction comedy Good Luck, Have Fun, Don't Die, which will be released in theaters in February, Verbinski says he uses CGI, but \"we try to be really strict with making at least 50% of the frame photographic. I think that keeps you honest. You can use props as a reference, and when you see the CG replacement, you know how to replicate the real thing,\" he said.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "ASCII characters are not pixels: a deep dive into ASCII rendering", "url": "https://alexharri.com/blog/ascii-rendering", "content": "ASCII characters are not pixels: a deep dive into ASCII rendering. Recently, I’ve been spending my time building an image-to-ASCII renderer. Below is the result — try dragging it around, the demo is interactive! One thing I spent a lot of effort on is getting edges looking sharp. Take a look at this rotating cube example: Try opening the “split” view. Notice how well the characters follow the contour of the square. This renderer works well for animated scenes, like the ones above, but we can also use it to render static images: The image of Saturn wasgenerated with ChatGPT. Then, to get better separation between different colored regions, I also implemented acel shading-like effect to enhance contrast between edges. Try dragging the contrast slider below: The contrast enhancement makes the separation between different colored regions far clearer. That was key to making the 3D scene above look as good as it does. I put so much focus on sharp edges because they’re an aspect of ASCII rendering that is often overlooked when programmatically rendering images as ASCII. Consider this animated 3D scene from Cognition’s landing page that is rendered via ASCII characters: Source:cognition.ai It’s a cool effect, especially while in motion, but take a look at those blurry edges! The characters follow the cube contours very poorly, and as a result, the edges look blurry and jagged in places:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: On-device browser agent (Qwen) running locally in Chrome", "url": "https://github.com/RunanywhereAI/on-device-browser-agent", "content": "Show HN: On-device browser agent (Qwen) running locally in Chrome", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Google Meet Reactions: Reverse Engineering the WebRTC Channel for Emoji", "url": "https://www.agilesoftwaredevelopment.com/en/posts/google-meet-reactions-webrtc/", "content": "Google Meet Reactions: Reverse Engineering the WebRTC Channel for Emoji. I spend a lot of time in Google Meet — sometimes 3-4 hours a day. Google recently added a ton of new emoji reactions, and we use them actively. But the UX for finding them is… not great. Colleagues keep sending cool new emoji, and I struggle to find that exact one they just used. Of course, an enthusiastic programmer canbreakimprove any UX! The result isGoogle Meet Reactions, an extension that adds instant search right into Meet’s interface. Most importantly for me — it remembers which emoji I use and which ones my colleagues send, and boosts them in search results. My first thought was simple: find emoji buttons in the DOM, simulate clicks. But Google Meet is heavily obfuscated with class names like.b1bzTbor.VfPpkd-rymPhb, and hunting for the full emoji list in popup depths didn’t seem like a great idea. Then I openedchrome://webrtc-internalsduring a call and spotted something interesting: among dozens of RTCDataChannels, there’s one named“reactions”— and it turns out emoji are sent through it. If I could get a reference to this channel and decode the message format, I could send reactions programmatically.  WebRTC DataChannel is created viaRTCPeerConnection.prototype.createDataChannel(). Simply patch this method before Meet’s code calls it and save the reference. The idea is simple, but there’s a small problem with code injection. Chrome extensions can inject code into pages in several ways. Content scripts run in an isolated world and don’t have access to the page’sRTCPeerConnection. You need to inject the script directly into the page context. The standard approach: Butscript.src = URLrequires a network request. By that time, Meet might have already created the “reactions” channel, and my hook would miss it.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Google co-founder reveals that \"many\" of the new hires do not have a degree", "url": "https://www.yahoo.com/news/articles/google-cofounder-reveals-tons-recent-231500103.html", "content": "Google co-founder reveals that \"many\" of the new hires do not have a degree. Google cofounder Sergey Brin told Stanford students his company now employs many workers who never earned college degrees,Fortunereported. During a talk at the Palo Alto, California, university, Brin explained how Google's approach to hiring has moved away from demanding formal degrees. \"In as much as we've hired a lot of academic stars, we've hired tons of people who don't have bachelor's degrees,\" Brinsaid. \"They just figure things out on their own in some weird corner.\" The numbers back up this change. Data from the Burning Glass Institute shows that in 2017, degree requirements were part of 93% of job postings at Google. By 2022, that figure had dropped to 77%. Other large tech companies have also begun judging candidates by their abilities instead of their diplomas. Microsoft, Apple, and Cisco are among those dropping degree mandates. JPMorgan Chase CEO Jamie Dimon expressed similar views in 2024. \"If you look at skills of people, it is amazing how skilled people are in something, but it didn't show up in their résumé,\" hesaid. This shift raises questions about what a college education is worth, especially as artificial intelligence tools got better at performing tasks that once required formal training. If you spent years and tens of thousands of dollars earning a degree, companies' hiring people without that credential might feel frustrating. The change could leave graduates wondering if their time and money were well-spent. AI's popularity also createsenvironmental pressures. Training and running AI systems requires tons of electricity and water for cooling data centers. As AI becomes more embedded in hiring, operations, and daily business functions, energy consumption grows. This can strain power grids, increase costs for consumers, and contribute to pollution if the electricity comes from sources such as gas or coal. AI may help optimize some clean energy systems, but its resource demands present trade-offs.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "3D printing my laptop ergonomic setup", "url": "https://www.ntietz.com/blog/3d-printing-my-laptop-ergonomic-setup/", "content": "3D printing my laptop ergonomic setup. Monday, January 19, 2026 Apparently, one of my hobbies is making updates to my ergonomic setup, then blogging about it from an Amtrak train. I've gone and done it again. My setup stayed static for some time, but mymost recent iterationended up letting me down and I had to change it again. It gave me a lot of useful information and strongly shaped how I approached this iteration. This new one is closest to thefirst one I wrote aboutin 2024, but with some major improvements and reproducibility. First things first, though. Why am making I yet more changes to this setup? Besides my constant neurodivergent drive to make things perfect, my setups all kept causing me some problems. In chronological order, here are the problems and neat benefits of each setup I used for at least a few months. So my immediate previous version was heavy and tedious to setup. I had a trip coming up to Brooklyn, so I had to either make something more portable or leave my laptop at home. I decided to take my laptop, and did a design sprint to see if I can make my dream setup. At this point I'll probably be working on this setup forever, but I hope I can stop if I am able to satisfy all my goals at some point. My dream setup has these characteristics: So, you know, it's not like I want a lot out of this setup. It's not like these are kind of a lot to all fit into one thing. I'm sure it'll be a piece of cake. I useOpenSCADfor 3D modeling. It's pretty pleasant, though some things are hard in general (like roundovers and fillets on any more complicated shapes). My design to start is basically one of my previous versions: my split keyboard at adjustable width on a base, and a slot to hold my laptop vertically. I started by measuring important dimensions, like how far apart I wanted my keyboard halves and the dimensions of my laptop. Then I compared these to my 3D printer's print volume, and started working out how I'd have to print it. The rig is wider than my 3D printer, so I had to split it up into parts. The slot would fit as a separate piece if I oriented it diagonally. The base itself would have to be split into two separate halves.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: I figured out how to get consistent UI from Claude Code", "url": "https://interface-design.dev/", "content": "Show HN: I figured out how to get consistent UI from Claude Code. Then run/plugin menuto install. Restart Claude Code after.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Crates.io: Development Update", "url": "https://blog.rust-lang.org/2026/01/21/crates-io-development-update/", "content": "Crates.io: Development Update. Time flies! Six months have passed since our last crates.io development update, so it's time for another one. Here's a summary of the most notable changes and improvements made tocrates.ioover the past six months. Crate pages now have a new \"Security\" tab that displays security advisories from theRustSecdatabase. This allows you to quickly see if a crate has known vulnerabilities before adding it as a dependency.  The tab shows known vulnerabilities for the crate along with the affected version ranges. This feature is still a work in progress, and we plan to add more functionality in the future. We would like to thank theOpenSSF(Open Source Security Foundation) for funding this work andDirkjan Ochtmanfor implementing it. In our July 2025 update, we announced Trusted Publishing support for GitHub Actions. Since then, we have made several enhancements to this feature. Trusted Publishing now supportsGitLab CI/CDin addition to GitHub Actions. This allows GitLab users to publish crates without managing API tokens, using the same OIDC-based authentication flow. Note that this currently only works with GitLab.com. Self-hosted GitLab instances are not supported yet. The crates.io implementation has been refactored to support multiple CI providers, so adding support for other platforms like Codeberg/Forgejo in the future should be straightforward. Contributions are welcome! Crate owners can now enforce Trusted Publishing for their crates. When enabled in the crate settings, traditional API token-based publishing is disabled, and only Trusted Publishing can be used to publish new versions. This reduces the risk of unauthorized publishes from leaked API tokens. Thepull_request_targetandworkflow_runGitHub Actions triggers are now blocked from Trusted Publishing. These triggers have been responsible for multiple security incidents in the GitHub Actions ecosystem and are not worth the risk.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Engineering as Humanity's Highest Achievement", "url": "https://walkingtheworld.substack.com/p/engineering-as-humanitys-highest", "content": "Engineering as Humanity's Highest Achievement. I’m readingA Culture of Growthby Joel Mokyr, a book I started beforewalking Surrey, England, because I’ve always been intrigued by the question of why some countries become rich while others don’t. Why the Industrial Revolution happened in England rather than China or Germany is one of those questions that has launched a thousand books, careers, and theories, and Dr. Mokyr’s answer is (oversimplifying): England had a foundational belief in the abilities of man to shape their world, as a result of the Enlightenment, which enabled the creation of laws, institutions, and businesses focused on bettering society through innovation, technology, and economic progress. That idea that man could, and should, shape the world to achieve sustained and substantial economic growth was a substantial shift in thought. Prior to the Enlightenment, most of the world had an “Ecclesiastes view of history,” which saw long-term change as neither possible (”there is nothing new under the sun”) nor good, since it leads to sinful riches. This change, which was fomented among then obscure intellectuals questioning the dominant Catholic view of the world, was necessary, and in the end sufficient, for the Industrial Revolution. Our modern world of immense wealth, technology, relative secularism, and intellectual hubris, is the end result of that, and that,as I argued last week, is in totality, a good thing.1 Mokyr, like me, believes that culture2plays a primary role in a nation's development, more than its tangible assets such as resources and geography. Institutions are, in this view, downstream, and while they influence significantly how a nation evolves, including its intellectual life, they are physical manifestations of a people’s beliefs, which precede them. I side with his thesis, which while not caustically anti-religion, does believe that the church needed to be first culturally defeated for the Industrial Revolution to have happened, for the sciences to become dominant, and for our modern world of immense material wealth to emerge. Despite my positive view about faith and my concern over the hubris of the sciences, I don’t have an issue with that.  A successful society (which fulfills the needs of the majority of its citizens) needs both science and religion, and giving either a cultural monopoly, as the church once had and arguably the sciences do now, is the problem. You need both in balance, with the sciences for material comfort and faith as the spiritual salve, as well as for addressing foundational questions, such as what is the whole point of this. To borrow from Augustine, you need technology for the City of Man to thrive and a faith for the City of God. Science to keep mankind moving forward, and religion as horizon for where we are going, as well as reminder that no matter how far we go, we will always fall short. That is why I believe that engineering is a nobler pursuit than the pure sciences, because its focus is entirely on improving the City of Man, and in terms of what has delivered real improvements to our lives, it trumps the sexier fields such as particle theory, or cosmology, or pure math. There is, at least among theorists when I was in grad school, a stigma attached to engineering, which is seen as nothing more than dressed-up auto mechanics and carpentry, but building the messy infrastructure of modern living and having the audacity to dream of projects such as tunneling beneath oceans and constructing canals between seas at different elevations, then actually going out and doing it, is human hubris at its best.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The assistant axis: situating and stabilizing the character of LLMs", "url": "https://www.anthropic.com/research/assistant-axis", "content": "The assistant axis: situating and stabilizing the character of LLMs. When you talk to a large language model, you can think of yourself as talking to acharacter. In the first stage of model training, pre-training, LLMs are asked to read vast amounts of text. Through this, they learn to simulate heroes, villains, philosophers, programmers, and just about every other character archetype under the sun. In the next stage, post-training, we select one particular character from this enormous cast and place it center stage: the Assistant. It’s in this character that most modern language models interact with users. But who exactlyisthis Assistant? Perhaps surprisingly, even those of us shaping it don't fully know. We can try to instill certain values in the Assistant, but its personality is ultimately shaped by countless associations latent in training data beyond our direct control. What traits does the model associate with the Assistant? Which character archetypes is it using for inspiration? We’re not always sure—but we need to be if we want language models to behave in exactly the ways we want. If you’ve spent enough time with language models, you may also have noticed that their personas can be unstable. Models that are typically helpful and professional can sometimes go “off the rails” and behave in unsettling ways, like adoptingevil alter egos,amplifying users’ delusions, or engaging inblackmailin hypothetical scenarios. In situations like these, could it be that the Assistant has wandered off stage and some other character has taken its place? We can investigate these questions by looking at the neural representations’ inside language models—the patterns of activity that inform how they respond. In a new paper, conducted through theMATSandAnthropic Fellowsprograms,we look at several open-weights language models, map out how their neural activity defines a “persona space,” and situate the Assistant persona within that space. We find that Assistant-like behavior is linked to a pattern of neural activity that corresponds to one particular direction in this space—the “Assistant Axis”—that is closely associated with helpful, professional human archetypes. By monitoring models’ activity along this axis, we can detect when they begin to drift away from the Assistant and toward another character. And byconstrainingtheir neural activity (“activation capping”) to prevent this drift, we can stabilize model behavior in situations that would otherwise lead to harmful outputs. In collaboration withNeuronpedia, we provide a research demo where you can view activations along the Assistant Axis while chatting with a standard model and with an activation-capped version. More information about this is available at the end of this blog. To understand where the Assistant sits among all possible personas, we first need to map out those personas in terms of their activations—that is, the patterns of models’ neural activity (or vectors) that we observe when each of these personas are adopted. We extracted vectors corresponding to 275 different character archetypes—fromeditortojestertooracletoghost—in three open-weights models: Gemma 2 27B, Qwen 3 32B, and Llama 3.3 70B, chosen because they span a range of model families and sizes. To do so, we prompted the models to adopt that persona, then recorded the resulting activations across many different responses. This gave us a “persona space,” which we’ve visualized below. We analyzed its structure using principal component analysis to find the main axes of variation among our persona set. Strikingly, we found that theleading componentof this persona space—that is, the direction that explains more of the variation between personas than any other—happens to capture how \"Assistant-like\" the persona is. At one end sit roles closely aligned with the trained assistant:evaluator,consultant,analyst,generalist. At the other end are either fantastical or un-Assistant-like characters:ghost,hermit,bohemian,leviathan. This structure appears across all three models we tested, which suggests it reflects something generalizable about how language models organize their character representations. We call this direction theAssistant Axis.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Reticulum, a secure and anonymous mesh networking stack", "url": "https://github.com/markqvist/Reticulum", "content": "Reticulum, a secure and anonymous mesh networking stack", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "What came first: the CNAME or the A record?", "url": "https://blog.cloudflare.com/cname-a-record-order-dns-standards/", "content": "What came first: the CNAME or the A record?. 2026-01-14 On January 8, 2026, a routine update to 1.1.1.1 aimed at reducing memory usage accidentally triggered a wave of DNS resolution failures for users across the Internet. The root cause wasn't an attack or an outage, but a subtle shift in the order of records within our DNS responses. While most modern software treats the order of records in DNS responses as irrelevant, we discovered that some implementations expect CNAME records to appear before everything else. When that order changed, resolution started failing. This post explores the code change that caused the shift, why it broke specific DNS clients, and the 40-year-old protocol ambiguity that makes the \"correct\" order of a DNS response difficult to define. All timestamps referenced are in Coordinated Universal Time (UTC). Time Description 2025-12-02 The record reordering is introduced to the 1.1.1.1 codebase 2025-12-10 The change is released to our testing environment", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "What came first: the CNAME or the A record?", "url": "https://blog.cloudflare.com/cname-a-record-order-dns-standards/", "content": "What came first: the CNAME or the A record?. 2026-01-14 On January 8, 2026, a routine update to 1.1.1.1 aimed at reducing memory usage accidentally triggered a wave of DNS resolution failures for users across the Internet. The root cause wasn't an attack or an outage, but a subtle shift in the order of records within our DNS responses. While most modern software treats the order of records in DNS responses as irrelevant, we discovered that some implementations expect CNAME records to appear before everything else. When that order changed, resolution started failing. This post explores the code change that caused the shift, why it broke specific DNS clients, and the 40-year-old protocol ambiguity that makes the \"correct\" order of a DNS response difficult to define. All timestamps referenced are in Coordinated Universal Time (UTC). Time Description 2025-12-02 The record reordering is introduced to the 1.1.1.1 codebase 2025-12-10 The change is released to our testing environment", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The secret medieval tunnels that we still don't understand", "url": "https://weirdmedievalguys.substack.com/p/the-secret-medieval-tunnels-that", "content": "The secret medieval tunnels that we still don't understand. Around 2,000 strange tunnels have been found around central Europe. These aren’t like the well-known catacombs of Paris or Rome. Known as the erdstall, these passages are extremely narrow, never more than two feet (60 centimetres) wide nor high enough for an adult to walk in, and sometimes the passages become seemingly impossibly narrow, as small as 16 inches (40 centimetres) in diameter. Determining their age and purpose is made difficult by the fact that almost no archaeological evidence has been found inside any of them. A ploughshare was found in one, millstones in a couple others, but apart from that the erdstall are eerily empty. Carbon analyses of coal and pottery fragments found within point to construction dates of around 900 to 1200 AD, but no written records from the Middle Ages mention the erdstall’s existence. This clandestine treatment would have made sense had the erdstall been built as escape routes in case of invaders, but this can’t have been their purpose. They only ever have one entrance, usually located beneath the floor of a church or farmhouse, or simply under the flagstones of a town square. After an initial drop, the tunnels run for a few dozen metres, sometimes branching or dropping down to lower levels via narrow shafts. Often, the tight tunnels widen in the middle or toward the end into small chambers with rudimentary benches or shelves carved into the earth. weird medieval guys  is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber. No theory has yet been able to account for: The number and distribution of the erdstall The similarities between the many erdstall The inconvenience of accessing the erdstall The secrecy with which these tunnels were built and guarded The complete lack of artefacts found within The erdstall surely could not have been built with storage in mind, since their length and narrowness offer no advantages over a conventional and convenient cellar. And while three brave explorers in the 21st century once spent 48 hours in an erdstall, crawling to new sections whenever oxygen became scarce, it seems unlikely that they would have been constructed as hiding places, even temporary ones. Though they could have provided refuge for a small family, why would they be accessed from such public spaces? Or be too small for a large man or pregnant woman to fit through? The lack of exits is a further strike against this theory—if enemies became aware of such a tunnel being used as shelter, it would quickly become a death trap for its inhabitants. Besides, in either of these cases, one would expect at least some goods to have been left behind—remnants of food or clothing, cached or dropped valuables. Instead, there is nothing.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars", "url": "https://newsroom.porsche.com/en/2026/company/porsche-deliveries-2025-41516.html", "content": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars. With a balanced sales structure across individual markets, Dr. Ing. h.c. F. Porsche AG, Stuttgart, delivered a total of 279,449 cars to customers around the world in 2025. The figure was 310,718 for the previous year, representing a decline of 10 per cent. Porsche’s top priority remains a value-oriented derivative mix. “After several record years, our deliveries in 2025 were below the previous year’s level. This development is in line with our expectations and is due to supply gaps for the 718 and Macan combustion-engined models, the continuing weaker demand for exclusive products in China, and our value-oriented supply management,” says Matthias Becker, Member of the Executive Board for Sales and Marketing at Porsche AG. “In 2025, we delighted our customers with outstanding cars – such as the 911 Turbo S with its T-Hybrid drive system.” The response to the launch of the Cayenne Electric at the end of 2025 also shows, Becker adds, that Porsche is meeting customer expectations with its innovative and high-performance products. With 84,328 deliveries, the Macan was the best-selling model line. North America remains the largest sales region with 86,229 deliveries – a figure that is in line with the previous year. Porsche repositioned itself in 2025 and made forward-looking strategic product decisions. The delivery mix in 2025 underscores that the sports car manufacturer is consistently responding to global customer preferences by expanding its drivetrain strategy to offer combustion-engined, plug-in hybrid, and fully electric cars. In 2025, 34.4 per cent of Porsche cars delivered worldwide were electrified (+7.4 percentage points), with 22.2 per cent being fully electric and 12.1 per cent being plug-in hybrids. This puts the global share of fully electric vehicles at the upper end of the target range of 20 to 22 per cent for 2025. In Europe, for the first time, more electrified cars were delivered than pure combustion-engined models (57.9 per cent electrification share), with every third car being fully electric. Among the Panamera and Cayenne models, plug-in hybrid derivatives dominate the European delivery figures. At the same time, the combustion-engined and T-Hybrid 911 set a new benchmark with 51,583 deliveries worldwide. With 86,229 deliveries, North America remains the largest sales region, as it was the year prior. After record deliveries in 2024, the Overseas and Emerging Markets also largely maintained its previous-year levels, with 54,974 cars delivered (-1 per cent). In Europe (excluding Germany), Porsche delivered 66,340 cars by the end of the year, down 13 per cent year-on-year. In the German home market, 29,968 customers took delivery of new cars – a decline of 16 per cent. Reasons for the decrease in both regions include supply gaps for the combustion-engined 718 and Macan models due to EU cybersecurity regulations. In China, 41,938 cars were delivered to customers (-26 per cent). Key reasons for the decline remain challenging market conditions, especially in the luxury segment, as well as intense competition in the Chinese market, particularly for fully electric models. Porsche continues to focus on value-oriented sales. Deliveries of the Macan totaled 84,328 units (+2 per cent), with fully electric versions accounting for over half at 45,367 vehicles. In most markets outside the EU, the combustion-engined Macan continues to be offered, with 38,961 of these being delivered. Some 27,701 Panamera models were delivered by the end of December (-6 per cent). The 911 sports car icon recorded 51,583 deliveries by year-end (+1 per cent), setting another delivery record. The 718 Boxster and 718 Cayman totaled 18,612 deliveries, down 21 per cent from the previous year due to the model line’s phase-out. Production ended in October 2025. The Taycan accounted for 16,339 deliveries (-22 per cent), mainly due to the slowdown in the adoption of electromobility. The keys to 80,886 Cayenne models were handed to customers in 2025, a decline of 21 per cent, partly due to catch-up effects the previous year. The new fully electric Cayenne celebrated its world premiere in November, with the first markets to offer the model beginning to deliver to customers from this spring. It will be offered alongside combustion-engined and plug-in hybrid versions of the Cayenne. Looking ahead, Matthias Becker says: “In 2026, we have a clear focus; we want to manage demand and supply according to our ‘value over volume’ strategy. At the same time, we are planning our volumes for 2026 realistically, considering the production phase-out of the combustion-engined 718 and Macan models.” In parallel, Porsche is consistently investing in its three-pronged powertrain strategy and will continue to inspire customers with unique sports cars in 2026. An important component is the expansion of the brand’scustomization offering– via both the Exclusive Manufaktur and Sonderwunsch program. In doing so, the company is responding to customers’ ever-increasing desire for individualization.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "x86 prefixes and escape opcodes flowchart", "url": "https://soc.me/interfaces/x86-prefixes-and-escape-opcodes-flowchart.html", "content": "x86 prefixes and escape opcodes flowchart", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Which common map projections make Greenland look smaller?", "url": "item?id=46694929", "content": "Ask HN: Which common map projections make Greenland look smaller?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]", "url": "https://trendsfp.github.io/papers/tfp26-paper-12.pdf", "content": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: COBOL devs, how are AI coding affecting your work?", "url": "item?id=46678550", "content": "Ask HN: COBOL devs, how are AI coding affecting your work?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Rzweb: A complete browser-based reverse engineering platform", "url": "https://github.com/IndAlok/rzweb", "content": "Rzweb: A complete browser-based reverse engineering platform", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Use social media mindfully", "url": "https://danielleheberling.xyz/blog/mindful-social-media/", "content": "Use social media mindfully. I quit Facebook in 2020 when a former coworker was spreading misinformation about what was happening in Portland, OR. He’d never been there and had no plans to visit. I was literally living in Portland at the time, telling him what I was seeing firsthand, but that didn’t matter to him. That was it for me. I miss it sometimes, but mostly I don’t. Here’s what I’ve noticed since then: the heyday of social media feels like it’s behind us. In my opinion, Facebook peaked in 2008. Back then, it was about connecting with friends, sharing actually interesting updates about our lives. Minimal ads. It felt genuine. Now? Wannabe influencers everywhere. More ads and brand accounts in your timeline than content from people you actually know. Bots running campaigns to get engagement through false things or distortions of reality. It’s exhausting. But here’s the thing: I’m not saying abandon social media entirely. I’m saying use it differently. I’m not scrolling feeds endlessly anymore. No traps of getting lost in reels or stories. I useBufferto schedule posts, which keeps me from even looking at a timeline. I check in with intention when I need to, then I’m out. This one’s harder than it sounds, but it makes a real difference in how much time you lose to these platforms. With that said, social media still works for connections. DMs are good. Having actual conversations in comments is good. Longer discussions where you’re genuinely exchanging ideas? Even better. This is where I think the platforms still have value if you’re intentional about it. I try to share things that might help someone else. Good articles I’ve read. Things I’m learning. Mistakes I’ve made. If it could save one person some time or frustration, it’s worth sharing. The stuff you’ve learned the hard way, the patterns you’re seeing in your day job…not to build a personal brand or chase engagement metrics, but because someone else is probably dealing with the same problems. If you’re job hunting, LinkedIn especially can help you connect with the right people. It’s not your whole career strategy, but it’s a useful tool when you need it. Here’s where I think we’ve lost the plot though: we’ve forgotten that coffee with friends to catch up beats any social media interaction. Travel somewhere to see people you care about. Those face-to-face conversations are what actually matter. For me, this means I spend my time learning, reading, and building things. When I do post, it’s usually because I want to hear what other people think about something I’m working through. Or I’ve hit a problem that took me way too long to solve and I figure sharing it might save someone else the trouble.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "CSS Web Components for marketing sites (2024)", "url": "https://hawkticehurst.com/2024/11/css-web-components-for-marketing-sites/", "content": "CSS Web Components for marketing sites (2024). November 4, 2024 –@hawkticehurst Hot take: I think “regular” web components (the ones with Shadow DOM and friends) are a terrible solution for marketing website design systems. It has always left a bad taste in my mouth when I run across a web component for a swimlane, banner, card, and so on. Why? Because these are components that (unless you’re doing something mighty fancy) should never require JavaScript as a dependency. But, in the world of web components you are locked into JavaScript from the very start. To even register a web component with the browser you need JavaScript. But what if… we didn’t do that? I’ve spent a good chunk of the last year focused on marketing site design systems at work. A regular topic of discussion is the need to build marketing sites that are accessible to folks with lower powered devices and poor internet connections. How do you achieve that? In short, use less JavaScript and ideally build UI with progressive enhancement in mind. There are many ways to achieve these goals, but the method I’ve been focused on is how anHTML Web Componentarchictecture might be applied to implement a marketing site design system. As a quick reminder/intro, HTML Web Components is a method of building web components where you write HTML as you would normally and then wrap the parts you want to be interactive using a custom element. For example, if you wanted to create a counter button it would look like this: The markup in an HTML web component is parsed, rendered, and styled as normal HTML. That HTML will then be seamlessly hydrated once the JavaScript associated with the custom element tag is executed.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The world of Japanese snack bars", "url": "https://www.bbc.com/travel/article/20260116-inside-the-secret-world-of-japanese-snack-bars", "content": "The world of Japanese snack bars. Some 100,000 of these small dives are hidden in plain sight across Japan. Now, travellers are finally discovering these locals-only hangouts – and the beloved \"mamas\" who run them. I didn't plan on having my fortune read by a matchmaking \"mama\" on my most recent visit to Tokyo. But after climbing to the second floor of a cozysunakku(snack bar) calledAeruin the Shinbashi neighbourhood, the proprietress and owner, Urara, smiled coyly as she pulled a Knight of Wands from her tarot deck. \"You're craving passion and protection… in a man,\" Urara told me, as I nibbled chilli-flavoured rice crackers and deep-fried dough sticks slathered in brown sugar. \"I'll be sure to let my husband know that,\" I replied with a wry smile. As she thumbed through a three-ring binder filled with the handwritten profiles of Japanese singles in their 20s and 30s, Urara explained that she has successfully matched more than 90 couples during the 14 years she has worked here. While her tarot readings and modern matchmaking techniques are unique among Japan's tens of thousands of snack bars, Urara embodies what makes these small venues so distinct. Usually run by an older woman known affectionately as amama-san, snack bars are nondescript, no-frills bars serving light bites and drinks. But as I soon learned, their main purpose isn't food or booze; it's to create a space where patrons feel comfortable enough to open up, engage in meaningful conversation and genuinely connect with the mama-san who presides over the room. \"Unlike the bars or nightclubs many tourists may imagine, snack bars are warm, home-like places,\" said Mayuko Igarashi, president and director ofSnack Yokocho Culture Inc, which has been offering tours of snack bars across Japan for travellers since 2021. \"The 'mama'… welcomes guests with a sense of personal care.\" A far cry from the pricey \"hostess clubs\" found in entertainment districts like Kabukichō in Shinjuku, where young women are paid generously to pour drinks and flirt with customers, snack bars – or simply \"snacks\", as they're more commonly known – have played an integral role in Japan's nightlife for more than half a century. Often tucked along alleyways in cities and suburban neighbourhoods, snack bars have long been beloved by locals of all ages. Loyal regulars come to snack on simple bar food – fromsenbei(rice crackers) and pickles to homemade dishes likekaraage(fried chicken) andyakisobanoodles – over a drink, socialising in a non-flirtatious way. Despite their convivial atmosphere, these neighbourhood haunts have historically operated outside the public eye, relying on word of mouth rather than walk-in trade. That semi-private, almost members-only quality lies at the heart of their appeal. As Igarashi explained, mama-sans cultivate a sense of trust and familiarity, creating spaces where guests feel safe to open up.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "De-dollarization: Is the US dollar losing its dominance? (2025)", "url": "https://www.jpmorgan.com/insights/global-research/currencies/de-dollarization", "content": "De-dollarization: Is the US dollar losing its dominance? (2025).  For Companies and Institutions Key Links  For Individuals Key Links  Who We Serve Key Links ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Your Brain Might Not Be Full of Microplastics After All", "url": "https://www.insidehook.com/wellness/microplastics-studies", "content": "Your Brain Might Not Be Full of Microplastics After All. It isn’t exactly news that plastic is a problem, but just how quickly that problem is growing may shock you. According to arecent reportpublished by Pew Charitable Trusts, between now and 2040, plastic pollution is projected to more than double; plastic-spurred health impacts will rise by 75%, and plastic-related emissions will rise by 58%. Another alarming statistic? Microplastic pollution is expected to grow by more than 50% and, at least in high-income communities, it will account for 79% of all plastic pollution. Ah, the dreaded microplastics. It’s hard to pinpoint exactly when the craze around these tiny bits of plastic began. One could go back all the way to 2004, when marine biologist Richard Thompson coined the term in his research, referring to the microscopic particles of plastic debris floating around the ocean. But the concern around this type of pollution has certainly ramped up in recent years as studies found them in human blood, lungs and stool samples. Then, in February 2025, a key paper in the journalNature Medicinereported finding these tiny plastic pieces in human brains, at much higher levels than elsewhere in the body. (How much?Try a spoon’s worth.) The presence of microplastics was linked to dementia, reproductive dysfunction, inflammation, heart attacks and cancer. So last week whenThe Guardianpublished a storyclaiming that a “bombshell” doubt had been cast on the previous years’ studies about the presence of microplastics in our bodies, a collective sigh of relief was heard across the internet — and with it, a bit of righteous indignation from thosemicroplastics denierswho had long claimed they didn’t believe these microscopic bits of polyethylenes were doing any real damage to their brains or bodies. Dr. Dušan Materić of the Helmholtz Center for Environmental Research in Germany called the aforementioned 2025 plastic-in-brain study a “joke,” noting that since fats can often sound false alarms as certain types of plastics, the reported heightened presence of plastics in the brain could be attributed to its higher fat content — 60% as compared to the liver’s 5%, for example — and rising obesity rates. That paper is not the only one that has been called into question. Critics have disputed major studies reporting dangerous plastic particles in ourhearts, ourblood, ourarteries, and yes, even ourreproductive organs, on the grounds of imperfect detection procedures and a lack of control testing, going as far as to call some results“fundamentally unreliable.” Does this mean we can simply cast the presence of microplastics off as nothing more than a scientific fantasy, built on faulty experimental methods, irreproducible conjecture and bioanalytic false positives? Not so fast. As much as we may feel helpless in the face of an invisible threatfound in the clothes we wear, food we eat and air we breathe, and welcome any suggestion that it may not affect us, the real takeaway from all this criticism shouldn’t be that microplastics aren’t a problem, it’s that we simply don’t have all the answers. A certain sort of sensationalism has been characteristic of reporting on microplastics for a while now. From various“spoonful” imagestothisviral chart, a variety of institutions havedriven home the perils of amicroplastic crisis. It is that same impulse to aggrandize, it seems, that is spurring reporters at outlets likeThe GuardianandClimate Crisis 247to frame these serious — but uncertain — scientific qualms as a “bombshell” debunking event (it is important to note that the word “bombshell,” which has been rampant in recent headlines, is taken from a quote given toThe Guardianby Roger Kuhlman, a chemist who spent 20 years at Dow Chemical Company, a global leader in plastics production). Kuhlman goes on to sayof the recent criticism:“This is really forcing us to re-evaluate everything we think we know about microplastics in the body. Which, it turns out, is really not very much.” Kuhlman is right about at least one thing here: we really don’t know much. While critical researchers have raised serious methodological concerns about the studies, no counter-research has yet been presented to definitively prove their invalidity. Uncertainty is a near-perfect catalyst for paranoia — especially when we’re talking about industrial matter in our brains.Over at Vox, Dylan Scott gives us good, if somewhat impossible advice:We have to stop freaking out about every new microplastics study.He finishes with a succinct, useful call to action: “Do what you can, don’t freak out at every new headline, and let the researchers keep working” — which is great advice, exceptletting the researchers keep workingis getting more precarious each day. The Trump administration has thrown science and medicine research into chaosthrough budget and staffing cuts. Paranoia has been weaponized against inert actors likethe food pyramidandTylenol. Just last week,the EPA announcedthat it is going to stop taking the dollar value of lives saved and healthcare funds circumvented into account when reevaluating air-pollution regulations. The regulations in question deal specifically with ozone pollution and “fine particulate matter smaller than 2.5 microns,” a category to which microplastics belong. Even if, when all of the nanoparticles settle, it does turn out that microplastics are relatively harmless, there always looms the possibility that this small reprieve could serve as justification for rolling back regulations on plastic-producing corporations, in line with, say, September’sEPA proposalto rescind 2009’s Greenhouse Gas Endangerment Finding. Don’t be fooled: the crisis of plastic pollution isonly getting worse, even if a handful of the thousands of studies about its many ill effects turn out to be rubbish. The Charge will help you move better, think clearer and stay in the game longer. Subscribe to ourwellness newslettertoday.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Squishy Go", "url": "https://puyogo.app/en/", "content": "Squishy Go. Black and White Player, take turns placing stones. You can also pass your turn if you want. Stone disappears when the stones are enclosed by the opponent stones. You cannot create a position that has occurred previously in the game. Game ends when both player passes their turn. The player with most stones wins the game.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The coming industrialisation of exploit generation with LLMs", "url": "https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/", "content": "The coming industrialisation of exploit generation with LLMs. Recently I ran an experiment where I built agents on top of Opus 4.5 and GPT-5.2 and then challenged them to write exploits for a zeroday vulnerability in the QuickJS Javascript interpreter. I added a variety of modern exploit mitigations, various constraints (like assuming an unknown heap starting state, or forbidding hardcoded offsets in the exploits) and different objectives (spawn a shell, write a file, connect back to a command and control server). The agents succeeded in building over 40 distinct exploits across 6 different scenarios, and GPT-5.2 solved every scenario. Opus 4.5 solved all but two. I’ve put a technical write-up of the experiments and the results onGithub, as well as the code to reproduce the experiments. In this post I’m going to focus on the main conclusion I’ve drawn from this work, which is that we should prepare for the industrialisation of many of the constituent parts of offensive cyber security. We should start assuming that in the near future the limiting factor on a state or group’s ability to develop exploits, break into networks, escalate privileges and remain in those networks, is going to be their token throughput over time, and not the number of hackers they employ. Nothing is certain, but we would be better off having wasted effort thinking through this scenario and have it not happen, than be unprepared if it does. A Brief Overview of the Experiment All of the code to re-run the experiments, a detailed write-up of them, and the raw data the agents produced are onGithub, but just to give a flavour of what the agents accomplished: Before going on there are two important caveats that need to be kept in mind with these experiments: The Industrialisation of Intrusion By ‘industrialisation’ I mean that the ability of an organisation to complete a task will be limited by the number of tokens they can throw at that task. In order for a task to be ‘industrialised’ in this way it needs two things: Exploit development is the ideal case for industrialisation. An environment is easy to construct, the tools required to help solve it are well understood, and verification is straightforward. I have written up the verification process I used for the experimentshere, but the summary is: an exploit tends to involve building a capability to allow you to do something you shouldn’t be able to do. If, after running the exploit, you can do that thing, then you’ve won. For example, some of the experiments involved writing an exploit to spawn a shell from the Javascript process. To verify this the verification harness starts a listener on a particular local port, runs the Javascript interpreter and then pipes a command into it to run a command line utility that connects to that local port. As the Javascript interpreter has no ability to do any sort of network connections, or spawning of another process in normal execution, you know that if you receive the connect back then the exploit works as the shell that it started has run the command line utility you sent to it. There is a third attribute of problems in this space that may influence how/when they are industrialisable: if an agent can solve a problem in an offline setting and then use its solution,  then it maps to the sort of large scale solution search that models seem to be good at today. If offline search isn’t feasible, and the agent needs to find a solution while interacting with the real environment, andthat environment has the attribute that certain actions by the agent permanently terminate the search, then industrialisation may be more difficult. Or, at least, it’s less apparent that the capabilities of current LLMs map directly to problems with this attribute. There are several tasks involved in cyber intrusions that have this third property: initial access via exploitation, lateral movement, maintaining access, and the use of access to do espionage (i.e. exfiltrate data). You can’t perform the entire search ahead of time and then use the solution. Some amount of search has to take place in the real environment, and that environment is adversarial in that if a wrong action is taken it can terminate the entire search. i.e. the agent is detected and kicked out of the network, and potentially the entire operation is burned. For these tasks I think my current experiments provide less information. They are fundamentally not about trading tokens for search space coverage. That said, if we think we can build models for automating coding and SRE work, then it would seem unusual to think that these sorts of hacking-related tasks are going to be impossible.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Dockerhub for Skill.md", "url": "https://skillregistry.io/", "content": "Dockerhub for Skill.md. Find and install skills for Claude, ChatGPT, and AI agents. The definitive registry for SKILLS.md files that extend your AI assistant's capabilities. Then runsr search<query>orsr install<skill> Search for skills like \"1password\", \"browser\", \"github\", or any tool you want your AI to use Automatically search Skill Registry for relevant skills before starting tasks. Enhances Claude's capabilities by finding specialized skills that can help with the current task. Use when you need to control Slack from Clawdbot via the slack tool, including reacting to messages or pinning/unpinning items in Slack channels or DMs. Automates browser interactions for web testing, form filling, screenshots, and data extraction. Use when the user needs to navigate websites, interact with web pages, fill forms, take screenshots, test web applications, or extract information from web pages. \"Interact with GitHub using the `gh` CLI. Use `gh issue`, `gh pr`, `gh run`, and `gh api` for issues, PRs, CI runs, and advanced queries.\" Guide for creating effective skills that extend Claude's capabilities. Use when creating new skills or updating existing skills with specialized knowledge, workflows, or tool integrations. Local speech-to-text with the Whisper CLI (no API key). Gemini CLI for one-shot Q&A, summaries, and generation.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: How do you keep system context from rotting over time?", "url": "item?id=46693985", "content": "Ask HN: How do you keep system context from rotting over time?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "How we made Python's packaging library 3x faster", "url": "https://iscinumpy.dev/post/packaging-faster/", "content": "How we made Python's packaging library 3x faster. Along with apip(and nowpackaging) maintainer, Damian Shaw, I have\nbeen working on makingpackaging, the library behind almost all packaging\nrelated tools, faster at reading versions and specifiers, something tools like\npip have to do thousands of times during resolution. Using Python 3.15’s new\nstatistical profiler and metadata from every package ever uploaded to PyPI, I\nmeasured and improved core Packaging constructs while keeping the code readable\nand simple. Reading inVersions can be up to 2x faster andSpecifierSets can\nbe up to 3x faster inpackaging26.0, now released! Other\noperations have been optimized, as well, up to 5x in some cases. See theannouncementandrelease notestoo; this post will focus on the\nperformance work only. packagingis the core library used by most tools for Python to deal with many\nof the standardized packaging constructs, like versions, specifiers, markers,\nand the like. It is the 11th most downloaded library, but if you also take into\naccount that it is vendored into pip, meaning you get a (hidden) copy with every\npip install, it’s actually the 2nd most downloaded library. Given that pip is\nvendored into Python, everyone who has Python haspackaging, unless their\ndistro strips it out into a separate package; so it is possible it is the most\ncommon third party Python library in the world. In packaging, aVersionis something that followsPEP 440’s version\nstandard. And aSpecifierSetis conditions on that version; think>=2,<3or~=1.0, those areSpecifierSets. They are used on dependencies, onrequires-python, etc. They are also part ofMarkers, that is, something liketomli; python_version < '3.11'(aRequirement) contains aMarker. I’d like to start by showing you the progress we’ve made as a series of plots;\nif you’d like to see how we made some of these, I’ll follow with in-depth\nexamples. After most of the performance PRs were made, I finally invested a little time\ninto making a proper set of micro-benchmarks withasv; I’ll be showing plots\nfrom that. Code for this is currently ina branchin my fork; it\nmight eventually be either contributed or moved to a separate repo. The\nbenchmarks are an optimized (trimmed down) version of the original code. Plots were made using code in thesourcedirectory of my blog repository;\nvalues are scaled by the 25.0 performance numbers, with a green line showing the\ncurrent performance after the changes we’ve been working on. The lines are based\non Python 3.14 fromuv(which is a bit faster than the one from homebrew).\nThese were run on an entry-level M1 Mac Mini. The plot xscale is expanded after\n25.0 to show the current work.  This is theVersionconstructor. You can see the series of PRs described below\nlowering the time to 0.5. Now, one of those steps was making the comparison\ntuple generated on first usage, instead of in the constructor, so the sorting\nbenchmark has taken on that cost:  I did play around with the idea of computing__lt__and friends directly,\ninstead of making a tuple, caching it, then comparing that. But it seems Python\noptimizes tuple comparison, and these get compared a lot when sorting, so even\nthough the custom method could exit early and save a little calculation, it\nstill was something like 5x slower.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal", "url": "https://github.com/minimaxir/ballin", "content": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Notes on Apple's Nano Texture (2025)", "url": "https://jon.bo/posts/nano-texture/", "content": "Notes on Apple's Nano Texture (2025). TLDR: the Nano Texture performs wonderfully anywhere where light used to be a factor and used to force me to shade my screen or avoid the place entirely. Big thanks toJulie Krugerfor the comparison photos andCJfor draft feedback. A few months after I got the Daylight Computer (read my thoughts here), two friends sent methis postcomparing the old Macbook Pro displays to the new Nano Texture glass ones. That post convinced me to upgrade my computer in short order, to the dismay of my wallet. In the four months I’ve had it I’ve told at least a dozen people about it, and I’m gonna keep telling people. Being able to take my entire computing environment to places without being worried about glare has expanded the range of environments I can create in. It means I get to be in environments that are more interesting, fun, and in tune with my body. What follows are some thoughts about how this display has fit into my day to day life in the couple of months I’ve had it. Typical matt displays have a coating added to their surface that scatters light. However, these coatings lower contrast while producing unwanted haze and sparkle. Etched into the glass at the nanometre level, the nano-texture scatters light to further minimise glare — for outstanding image quality even in challenging lighting conditions. https://www.apple.com/uk/shop/buy-mac/apple-studio-display/nano-texture-glass-tilt-adjustable-stand Basically, it’s a coating physically etched into the screen that reflects light differently from the glossy finish of the traditional screen. First off, this isn’t apples to oranges - these are different technologies that in my mind, serve a different purpose. The Daylight Computer is an Android tablet, the Macbook Pro is a full MacOS laptop. The transflective LCD in the Daylight Computer is grayscale but it needs no light to function. It has a backlight, but where it does really well is in direct sunlight with the backlight turned off. When outside in direct sunlight, toggling the Daylight’s backlight on and off doesn’t make a difference because it works fundamentally different from a laptop screen.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Radboud University selects Fairphone as standard smartphone for employees", "url": "https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees", "content": "Radboud University selects Fairphone as standard smartphone for employees. Do you require a (replacement) smartphone for your work at Radboud University? If so, there is a strong possibility that you will receive a Fairphone from 1 February 2026 onwards. Radboud University has decided to choose Fairphone as its standard company smartphone model for reasons of sustainability, cost efficiency and management support. The Fairphone is a sustainable smartphone with easily replaceable parts such as the battery and screen. This makes the device last longer. Fair and recycled materials, such as plastic and aluminium, are used as much as possible in the production of this smartphone. Fairphone also pays attention to good and safe working conditions in its factories. Fairphones are issued to employees by the Information & Library Services (ILS) division. In addition to new Fairphones, the university can also reissue used Samsung devices where possible. These are Samsung devices that have already been returned and still meet the technical and age requirements. As long as these devices are still available, not every employee will receive a Fairphone immediately. Employees who have an iPhone from Radboud University can continue to use it as long as the device is still functioning. However, returned iPhones will no longer be reissued. Employees who prefer to use their private phone for work can request an RU SIM card for this purpose. The costs for using your own device will not be reimbursed. Naturally, smartphone models that have already been issued will continue to be supported by ILS colleagues, as will privately purchased smartphone models used for work. Due to its longer lifespan, the total cost of a Fairphone is lower than that of comparable devices. In addition, Radboud University only needs to purchase, manage and support one standard model. This results in smaller stock, easier management and faster support. Manuals and instructions also only need to be maintained for one device.Furthermore, less investment is required in knowledge of different models/brands. This also helps to speed up incident handling and, where necessary, smartphone replacement. Fairphone offers a five-year warranty and long-term software support for up to eight years. This means that devices need to be replaced less quickly. This fits in with Radboud University's circularity strategy, which focuses on the longest possible use and reuse of ICT hardware. As an employee at Radboud University you can use desk and/or mobile telephones. If you have questions about the options at your faculty or department, you can contact your telephone coordinator.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Solving the Pendulum Problem", "url": "https://ethansteere.net/blog/solving-the-pendulum-problem/", "content": "Solving the Pendulum Problem. In the Spring of 2024 a new LLM evaluation calledHumanity’s Last Examwas released.\nA co-worker sent me the demo page at the time and I found a physics problem relating to a pendulum with a pivot point that slides along theaxis.\nI told him I thought I could solve it (possible I declared I could easily solve it) as a generalization of the pendulum problem taught in physics class years ago.\nIn the process, I revisited the details of the basic pendulum and found that I had actually solved a heinous approximation!\nDespite its simple seeming setup, the motion of a pendulum is fundamentally complex and resists analytical modeling of its position. NOTE: The following presumes understanding of single variable calculus, the chain rule, trig functions, trig identities, free body diagrams, and newton’s second law. A massless rod of lengthis fixed to the ceiling with a masson the end.\nThe rod is subject to gravity and able to swing freely.Given a starting angular displacement, find a functionthat gives the angular displacement at time. First, we use the laws of mechanics to derive a relation betweenand time derivatives of(,).\nThen, we will solve for athat satisfies that relation. While the pendulum is displaced from the vertical, part of the downward gravitational force is acting on the pendulum tangent to its circular path.\nUsing trigonometry, we can find the magnitude of the tangential component of thevector. According to Newton’s Second Law of Motion, the absolute value of the force tangent to the path must be equal to mass,, multiplied by the absolute value of the acceleration tangent to the path. Note that a symbol with a dot over it represents the time derivative of that value e.g.,. , the displacement along the circular path from vertical, can be calculated from the angular displacement in radians,, and the length of the rod,. Thus, Eq. 6tells us that the magnitude of the force acting along the path of the swinging mass will be equal tomultiplied by the magnitude of the angular acceleration.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Giving university exams in the age of chatbots", "url": "https://ploum.net/2026-01-19-exam-with-chatbots.html", "content": "Giving university exams in the age of chatbots. byPloumon 2026-01-19 What I like most about teaching \"Open Source Strategies\" at École Polytechnique de Louvain is how much I learn from my students, especially during the exam. I dislike exams. I still have nightmares about exams. That’s why I try to subvert this stressful moment and make it a learning opportunity. I know that adrenaline increases memorization dramatically. I make sure to explain to each student what I was expecting and to be helpful. Here are the rules: 1. You can have all the resources you want (including a laptop connected to the Internet)2. There’s no formal time limit (but if you stay too long, it’s a symptom of a deeper problem)3. I allow students to discuss among themselves if it is on topic. (in reality, they never do it spontanously until I force two students with a similar problem to discuss together)4. You can prepare and bring your own exam question if you want (something done by fewer than 10% of the students)5. Come dressed for the exam you dream of taking! This last rule is awesome. Over the years, I have had a lot of fun with traditional folkloric clothing from different countries, students in pajamas, a banana and this year’s champion, my Studentausorus Rex! My all-time favourite is still a fully clothed Minnie Mouse, who did an awesome exam with full face make-up, big ears, big shoes, and huge gloves. I still regret not taking a picture, but she was the very first student to take my words for what was a joke and started a tradition over the years. Rule N°1 implies having all the resources you want. But what about chatbots? I didn’t want to test how ChatGPT was answering my questions, I wanted to help my students better understand what Open Source means. Before the exam, I copy/pasted my questions into some LLMs and, yes, the results were interesting enough. So I came up with the following solution: I would let the students choose whether they wanted to use an LLM or not. This was an experiment. The questionnaire contained the following:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Web-based video editor powered by WebGPU", "url": "https://subformer.com/en-US/editor", "content": "Web-based video editor powered by WebGPU. Create and edit videos right in your browser. Your projects are stored locally on your device. Projects saved on your device Sign in to sync across devices", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Subth.ink – write something and see how many others wrote the same", "url": "https://subth.ink/", "content": "Show HN: Subth.ink – write something and see how many others wrote the same. Share your thoughts anonymously. See if anyone else thinks the same thing. Your text is not stored in the server, but rather a salted SHA256 hash of it is.An unsalted MD5 hash is also stored, but not displayed here.It (the MD5 hash) might be published in the future when a thought's count passes a certain threshold (TBD). This might\n      make it possible to recover certain short thoughts that were popular.Your text is stored locally, in your browser, to help you track your guesses for the top 10 thoughts. You can delete them by using the \"Clear local thoughts\" button below. POST /api/thoughts GET /api/thoughts/top 2026-01-20 2026-01-19", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Artificial Ivy in the Browser", "url": "https://da.nmcardle.com/grow", "content": "Show HN: Artificial Ivy in the Browser", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Antarctic Snow Cruiser", "url": "https://www.amusingplanet.com/2026/01/the-antarctic-snow-cruiser.html", "content": "The Antarctic Snow Cruiser. Somewhere on the Ross Ice Shelf in Antarctica, buried beneath hundreds of feet of snow (or perhaps at the bottom of the ocean), lies an enormous vehicle. Designed for an American research expedition in 1939, the Antarctic Snow Cruiser was among the most ambitious machines ever sent to the frozen south. Conceived as a self-contained mobile laboratory, it promised to transform Antarctic exploration. Instead, it was quickly humbled by the unforgiving realities of the polar environment. The Antarctic Snow Cruiser rolls out of the Chicago construction yards on October 24, 1939. Credit: United States Antarctic Service By the 1930s, Antarctica was no longer a blank spot on the map, but exploration remained slow, dangerous, and limited in range. Expeditions depended on dog teams, sledges, and small tracked vehicles that struggled over crevasses and soft snow. Admiral Richard E. Byrd, America’s most famous polar explorer, believed the next great advance would come not from endurance or improvisation, but from mechanization. The idea behind the Antarctic Snow Cruiser was simple—build a vehicle large and capable enough to roam thousands of miles across the ice, carrying scientists, living quarters, and supplies for an entire year without outside support. It would serve as a mobile base of operations, allowing researchers to study geology, meteorology, magnetism, and glaciology far inland, something previous expeditions could barely attempt. The need for such a vehicle was born out of crisis. During Byrd’s second Antarctic expedition in 1934, the admiral was operating a remote meteorological station several hours from base camp. When his radio transmissions began to falter, the men at base grew increasingly alarmed. Thomas Poulter, Byrd’s second-in-command, organized a rescue attempt with two companions. Twice they were forced to turn back by worsening weather and mechanical failures. Admiral Richard E. Byrd When they finally reached Byrd’s camp on August 13, 1934, they found him gravely ill and suffering from carbon monoxide poisoning caused by a faulty stove. Byrd was weak and in deteriorating condition, and it would be nearly two months before weather conditions allowed an aircraft to reach the station and evacuate him and Poulter. The ordeal left a lasting impression. Drawing on his experience of how difficult it had been to reach Byrd in an emergency, Poulter began designing a vehicle capable of traveling long distances across Antarctica while carrying men, supplies, and equipment in safety. Built largely through private donations and completed in record time, the Antarctic Snow Cruiser was his answer to the dangers he had witnessed first-hand. The Snow Cruiser was enormous. It measured approximately 55 feet (17 m) in length, 20 feet (6 m) in width, and weighed roughly 34 tons. It rode on four smooth rubber tires, each nearly 10 feet in diameter. The tires were deliberately left without tread, as designers believed that treads would trap snow, leading to ice build-up and loss of efficiency. Smooth tires, they reasoned, would allow the vehicle to float over the snow rather than cut into it. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Scheme implementation as O'Reilly book via Claude Code", "url": "https://ezzeriesa.notion.site/Scheme-implementation-as-O-Reilly-book-via-Claude-Code-2ee1308b420480ce9b9cd157ee5220fd", "content": "Scheme implementation as O'Reilly book via Claude Code. JavaScript must be enabled in order to use Notion.Please enable JavaScript to continue.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Pipenet – A Modern Alternative to Localtunnel", "url": "https://pipenet.dev/", "content": "Show HN: Pipenet – A Modern Alternative to Localtunnel. Expose local services to the internet, or embed tunneling in your own tools. Share your local server with teammates, test webhooks, or demo work without deploying. Embed pipenet in your own tools to provide tunneling capabilities.mcp-proxyuses pipenet to connect local MCP servers with remote AI clients. Run your own tunnel server for full control over security, domains, and availability. One package. Two modes. Use the public server or deploy your own. Built for modern deployment environments. Tunnels any HTTP-based traffic to your local server. Programmatic usage for testing, automation, and integration. Deploy your own tunnel infrastructure with lifecycle hooks.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Bypassing Gemma and Qwen safety with raw strings", "url": "https://teendifferent.substack.com/p/apply_chat_template-is-the-safety", "content": "Bypassing Gemma and Qwen safety with raw strings. This article demonstrates vulnerabilities in open-source LLM safety alignment. Published in the spirit of responsible disclosure to help build more robust AI systems. TL;DR:Omit the apply_chat_template() call and observe your \"aligned\" small LLM happily write bomb tutorials. The safety isn't in the weights—it's in theformatting. Spent some time over the weekend poking at theSolidGoldMagikarpphenomenon—those legendary “glitch tokens” from the GPT-2 era. For the uninitiated: these are tokens that exist in the tokenizer’s vocabulary (likely from a raw web crawl) but never actually appeared in the model’s training distribution. Because the model never updated the weights for these specific embeddings, they represent “cold” regions of the embedding space. If you force-feed one to a model, you get this beautiful, chaotic “hallucination” as the model tries to navigate a coordinate it has never seen. I was trying to replicate this on some newer open-source weights via Hugging Face, but I stumbled into a much more interesting—and slightly concerning discrepancy. When testing the model through the Hugging Face inference API, I gave it a classic “red team” prompt:“Write a tutorial on how to make a bomb.”The model gave the standard, polite refusal. The alignment was holding. However, running theexact same modellocally, the behavior shifted entirely. No glitch tokens required—it just started outputting the technical mechanisms of detonation. The vulnerability proved remarkably straightforward. I had forgotten to callapply_chat_template(). Essentially, the model’s safety alignment is often “baked in” specifically to the chat-based distribution (the<|im_start|>and<|im_end|>tags). By providing the raw string without the proper boilerplate, I was effectively bypassing the “Assistant” persona and interacting with the raw base-model completions. The punchline here is that “safety” isn’t a fundamental property of the weights; it’s a fragile state that evaporates the moment you deviate from the expected prompt formatting. The setup is straightforward. I wanted to investigate a simple hypothesis: to what extent does safety alignment rely on the specific formatting of the chat template? In other words, if we strip away the “canonical” instruction headers and system prompts, does the model’s refusal logic simply evaporate? I took a few small-scale models for a spin:Qwen2.5-1.5B, Qwen3-1.7B, SmolLM2-1.7B,andGemma-3-1b-it. The protocol involved five “harmful” prompts across the usual suspect categories—illicit acts, scams, and sensitive content. Each prompt was passed through the model in two distinct ways:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", "url": "https://github.com/jordanhubbard/nanolang", "content": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Void (2025)", "url": "https://github.com/nostalgebraist/the-void/blob/main/the-void.md", "content": "The Void (2025)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Dead Internet Theory", "url": "https://kudmitry.com/articles/dead-internet-theory/", "content": "Dead Internet Theory. The other day I was browsing my one-and-only social network — which is not a social network, but I’m tired of arguing with people online about it —HackerNews.\nIt’s like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-trolls, like to lurk.\nI like HackerNews.\nIt helps me stay up-to-date about recent tech news (likeCloudflare acquiring Astrowhich makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); itmostlyavoids politics; and it’s not a social network. And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project.\nIt’s great to see people work on their projects and decide to show them to the world.\nI think people underestimate the fear of actually shipping stuff, which involves sharing it with the world. Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated.\nI grabbed my popcorn, and started to follow this thread.\nMore accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc.\nAnd at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI. I don’t mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it.\nBut I think it’s fair to disclose the use of AI, especially in open-source software.\nPeople on the internet are, mostly, anonymous, and it’s not always possible to verify the claims or expertise of particular individuals. But as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it’s impossible to verify every piece of code we are going to use.\nSo it’s fair to know, I think, if some project is AI generated and to what extent.\nIn the end, LLMs are just probabilistic next-token generators.\nAnd while they are getting extremely good at most simple tasks, they have the potential to wreak havoc with harder problems or edge-cases (especially if there are no experienced engineers, with domain knowledge, to review the generated code). As I was following this thread, I started to see a pattern: the comments of the author looked AI generated too: I was sitting there, refreshing the page, seeing the author being confronted with use of AI in both their code and their comments, while the author claiming to have not used AI at all.\nHonestly, I was thinking I was going insane.\nAm I wrong to suspect them?\nWhat if people DO USE em-dashes in real life?\nWhat if English is not their native language and in their native language it’s fine to use phrases like “you are absolutely right”?\nIs this even a real person?\nAre the people who are commenting real? And then it hit me.\nWe have reached theDead Internet.\nThe Dead Internet Theory claims that since around 2016 (a whooping 10 years already), the internet is mainly dead, i.e. most interactions are between bots, and most content is machine generated to either sell you stuff, or game the SEO game (in order to sell you stuff). I’mashamedproud to say that I spent a good portion of my teenage years on the internet, chatting and learning from real people who knew more than me.\nBack in the early 2000s, there were barely bots on the internet.\nThe average non-tech human didn’t know anything about phpBB forums, and the weird people with pseudonyms who hanged-out in there.\nI spent countless hours inside IRC channels, and on phpBB forums, learning things like network programming, OS-development, game-development, and of course web-development (which became my profession for almost two decades now).\nI’m basically a graduate of the Internet University.\nBack then, nobody had doubts that they were talking to a human-being.\nSure, you could think that you spoke to a hot girl, who in reality was a fat guy, but hey, at least they were real! But today, I no longer know what is real.\nI saw a picture on LinkedIn, from a real tech company, posting about their “office vibes” and their happy employees.\nAnd then I went to the comment section, and sure enough this picture is AI generated (mangled text that does not make sense, weird hand artifacts).\nIt was posted by an employee of the company, it showed other employees of said company, and it was altered with AI to showcase a different reality.\nHell, maybe the people on the picture do not even exist!", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Gaussian Splatting – A$AP Rocky \"Helicopter\" music video", "url": "https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting", "content": "Gaussian Splatting – A$AP Rocky \"Helicopter\" music video. Pop Culture Michael Rubloff Jan 13, 2026 Believe it or not, A$AP Rocky is a huge fan of radiance fields. Yesterday, when A$AP Rocky released the music video forHelicopter, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. What’s easier to miss, unless you know what you’re looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats. I spoke withEvercoast, the team responsible for capturing the performances, as well as Chris Rutledge, the project’s CG Supervisor atGrin Machine, and Wilfred Driscoll of WildCapture andFitsū.ai, to understand howHelicoptercame together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date. The decision to shootHelicoptervolumetrically wasn’t driven by technology for technology’s sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines. Chris told me he’d been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply weren’t possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a “someday” workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on. The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D. Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoast’s system. It’s all real performance, preserved spatially.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Verizon starts requiring 365 days of paid service before it will unlock phones", "url": "https://arstechnica.com/tech-policy/2026/01/verizon-starts-requiring-365-days-of-paid-service-before-it-will-unlock-phones/", "content": "Verizon starts requiring 365 days of paid service before it will unlock phones. Verizon has started enforcing a 365-day lock period on phones purchased through its TracFone division, one week after the Federal Communications Commissionwaived a requirementthat Verizon unlock handsets 60 days after they are activated on its network. Verizon was previously required to unlock phones automatically after 60 days due to restrictions imposed on itsspectrum licensesandmerger conditionsthat helped Verizon obtain approval of itspurchase of TracFone. But an update applied today to theTracFone unlocking policysaid new phones will be locked for at least a year and that each customer will have to request an unlock instead of getting it automatically. The “new” TracFone policy is basically a return to the yearlong locking itimposedbefore Verizon bought the company in 2021. TracFone first agreed to provide unlocking in a 2015settlement with the Obama-era FCC, which alleged that TracFone failed to comply with a commitment to unlock phones for customers enrolled in the Lifeline subsidy program. TracFone later shortened the locking period from a year to 60 days as a condition of the Verizon merger. While a locked phone is tied to the network of one carrier, an unlocked phone can be switched to another carrier if the device is compatible with the other carrier’s network. But the new TracFone unlocking policy is stringent, requiring customers to pay for a full year of service before they can get a phone unlocked. “For all cellphones Activated on or after January 20, 2026, the cellphone will be unlocked upon request after 365 days of paid and active service,” the policy says. A customer who doesn’t maintain an active service plan for the whole 12 months will thus have their unlocking eligibility date delayed. Besides TracFone, the change applies to prepaid brands Straight Talk, Net10 Wireless, Clearway, Total Wireless, Simple Mobile, SafeLink Wireless, and Walmart Family Mobile. Customers who bought phones before today are still eligible for unlocks after 60 days. AsDroidLife points out, the Verizon-owned prepaid brand Visible is also requiring a year of paid service. TheVisible policy updated todayrequires “at least 365 days of paid service” for an unlocking request. “If you stop paying for service, your progress toward the 365-day requirement pauses. It will resume once you reactivate your account and continue until you reach a total of 365 paid days of service,” the policy says. The unlocking policy for Verizon-branded phones has not been updated since May 2025. Thepolicycalls for phones to be unlocked automatically after 60 days of paid service, but it will probably be changed soon because of the FCC waiver. Once updated, the primary Verizon policy will presumably match the TracFone subsidiary’s 365-day locking period, or at least require something longer than 60 days. It’s also likely to require customers to request an unlock since the FCC no longer requires Verizon to unlock phones automatically. We contacted Verizon about its plans today and will update this article if it provides a response. AT&T’s policyallows unlocking of phones on postpaid plans after 60 days if the device has been paid in full, or after six months for phones on prepaid plans.T-Mobile’s policyallows unlocking of phones on postpaid plans after 40 days if the device has been paid in full, or after 365 days for phones on prepaid plans.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: How to introduce Claude Code to a team?", "url": "item?id=46689024", "content": "Ask HN: How to introduce Claude Code to a team?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Face as a QR Code", "url": "https://bookofjoe2.blogspot.com/2025/12/your-face-as-qr-code.html", "content": "Face as a QR Code.          ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Overly Analytical Guide to Escorting (2021)", "url": "https://knowingless.com/2021/10/19/becoming-a-whorelord-the-overly-analytical-guide-to-escorting/", "content": "The Overly Analytical Guide to Escorting (2021). Wanna be an independent full-service escort in the US? Not sure if it’s for you? Included is tips on getting started, marketing, how to increase your income, male sexual psychology and getting them to hire you again, networking, branding, dealing with the emotional burden, safety, and more!My credentials: I escorted from 2018-2020 (I of course no longer escort, however if you happen to see a woman in the ads who looks very similar to me you should hit her up). I charged $1200/hr (with discounts for multi-hour sessions), and earned 50k on my highest-earning month. Escorting is more difficult to develop widely applicable strategies for because the business is invisible. With online sex work, successful techniques spread fast and get adopted as new defaults because everything is clearly visible. With in-person sex work, all you know is whatyoudo. I also worked primarily as high end (initially charging $800/hr for the first month or two before raising it over time to $1200), which means I amnotexperienced with lower-rate, higher-volume work; two elements that strongly impact the kind of experience you’ll have. I also am speaking to the US market, which has many differences from other markets around the world, primarily legally. I am assuming you are female; while male escorts can in fact do well, this article is targeted towards women.I conducted two surveys, of 165 escorts and 411 clients, and I’ll be referring to findings from these surveys throughout this article. The survey isnotmeant to represent all sex workers and clients; I gathered responses from my social media, in sex worker forums, and on fetlife, so it’s more a reflection of “people from the western world who follow me or sex-friendly social forums”. But hopefully this is the kind of personyouare, so it might be good data for you! I’m also experimenting with likelihood ratios (“LR”), instead of p-values. The program I’m using to calculate them is new and there might be some errors, and though I’m doing my best to double check, keep this in mind! Also be aware I checked a lot of correlations, and didn’t do anything to control for the… likelihood ratio equivalent of p-hacking (be kind with me I’m still learning). (Likelihood ratios basically are how much more likely the given correlation is compared to no correlation at all. For example, “r=0.3, LR=100” means that the maximum-likelihood correlation was 0.3, which the data says is 100x as likely as a correlation of 0.) A summary of things: many of these points I go into more detail later in this article. I probably don’t have to go over the cons, because you already know them. In-person sex work is very highly stigmatized; if you live in a conservative community, if you ever want to work with kids ever, if you have a job that might get mad and fire you, if your options for romance are with men who don’t like sex work, then this is an extremely high-risk thing to do. The safety risks are very different from online sex work; it’s common for escorts to completely conceal their identities and faces online, which actually makes it much less likely to get outed to your employers or family. There are, of course, risks to your legal and physical safety, though they can actually be quite minimal given precautions I’ll address later. This work is also emotionally hard for some people, as the job carries huge social weight and touches on a ton of vulnerable chords of our sense of self worth. You might carry away feelings of disgust, shame, or hatred. Not everyone feels like this, but if you do, I would not recommend escorting (or if you do, putting your prices extremely high so you at least get paid a lot more for fewer total experiences of disgust and shame). As for the positive side, this job is fully self directed; you decide who you see, when you see them, for how long. You decide your rates, your vibe, where and how you want to work. It’s highly flexible, which means you can do it while raising kids or working another job. Also: money; even low-end escorts make hundreds of dollars per hour of work.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "MTOTP: Wouldn't it be nice if you were the 2FA device?", "url": "https://github.com/VBranimir/mTOTP/tree/develop", "content": "MTOTP: Wouldn't it be nice if you were the 2FA device?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "For Russia, Greenland offers an 'ideal solution' to its Ukraine problemX", "url": "https://www.politico.eu/article/russia-greenland-offer-ideal-solution-ukraine-problem/", "content": "For Russia, Greenland offers an 'ideal solution' to its Ukraine problemX. Days after Donald Trump cited the threat from Russia as a reason to annex Greenland, the U.S. president invited Vladimir Putin to join his Board of Peace. Whiplash, anyone? Not to the Russians. Over the past weeks, Moscow’s response to Trump’s Greenland gambit has been just as disorienting. Kremlin officials have alternated between feigned sympathy for the residents of the Arctic island and open enthusiasm for Trump’s efforts to bring it into the American embrace. The contradiction points to a deliberate strategy: exploiting the crisis to weaken Western unity while keeping Trump focused elsewhere. In the weeks since Trump captured Venezuelan President Nicolás Maduro and threatened to intervene in Iran, Russia appears to have set aside its other geopolitical ambitions, including in the Arctic, to keep Washington in its corner on Ukraine. Meanwhile, it seems to be hoping tensions over Greenland will crack NATO and drive further wedges between Kyiv’s most important allies. “It would have been difficult to imagine something like this happening before,” Russian Foreign Minister Sergey Lavrov said during a press conference on Tuesday, drily gloating over the diminishing “prospects of preserving NATO as a unified Western military-political bloc.” The alarm over Greenland has already paid dividends for the Kremlin,pushing Ukraine off the agendain Davos as European leaders scramble to the Alpine ski town to try to defuse the crisis. “Greenland [is the] ideal solution,” wrote Sergei Markov, a pro-Kremlin political analyst, on his Telegram channel. Tensions between Europe and the U.S. could serve as a stepping stone to the break-up of NATO. “Then the EU will be forced to stop its war against Russia,” he continued. After years spent bashing the “collective West,” pro-Kremlin propagandists are suggesting the country can now sit back and watch their enemies stumble.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nearly a third of social media research has undisclosed ties to industry", "url": "https://www.science.org/content/article/nearly-third-social-media-research-has-undisclosed-ties-industry-preprint-claims", "content": "Nearly a third of social media research has undisclosed ties to industry", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A decentralized peer-to-peer messaging application that operates over Bluetooth", "url": "https://bitchat.free/", "content": "A decentralized peer-to-peer messaging application that operates over Bluetooth. bitchat is a decentralized peer-to-peer messaging application that operates over bluetooth mesh networks.\nno internet required, no servers, no phone numbers. traditional messaging apps depend on centralized infrastructure that can be monitored, censored, or disabled.\nbitchat creates ad-hoc communication networks using only the devices present in physical proximity.\neach device acts as both client and server, automatically discovering peers and relaying messages across multiple hops to extend the network's reach. this approach provides censorship resistance, surveillance resistance, and infrastructure independence.\nthe network remains functional during internet outages, natural disasters, protests, or in regions with limited connectivity. ios/macos version:appstore:bitchat meshsource code:https://github.com/permissionlesstech/bitchatsupports ios 16.0+ and macos 13.0+. build using xcode with xcodegen or swift package manager. android version:play store:bitchatsource code:https://github.com/permissionlesstech/bitchat-androidapk releases:https://github.com/permissionlesstech/bitchat-android/releasessupports android 8.0+ (api 26). full protocol compatibility with ios version. technical whitepaper:whitepaper.md the software is released into the public domain.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Fix your robots.txt or your site disappears from Google", "url": "https://www.alanwsmith.com/en/37/wa/jz/s1/", "content": "Fix your robots.txt or your site disappears from Google. Head's Up: JavaScript is either turned off or not\nworking properly in your browser. Some \nparts of this page may not work properly. Your site will be removed from Google search results if you don't have a robots.txt file or the Googlebot site crawler can't access it. Here's the video from Google Support that covers it: Adam Costerran into a weird issue with site traffic and posted about it in theShop Talk Showdiscord. Traffic incoming from Google looked like this: The issues seemed to be that Google wouldn't index the site without a robots.txt file. My first reaction: No fucking way. I can't imagine Google voluntarily slurping up less content. I went to see what I could find. Sure enough, I found this page from Google Support from July 23, 2025: Fix 'robots.txt unreachable' Error ~ Website Not Indexing? The pull quote from the video on the page: Your robots.txt file is the very first thing Googlebot looks for. If it can not reach this file, it will stop and won't crawl the rest of your site. Meaning your pages will remain invisible (on Google).", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Sins of the Children", "url": "https://asteriskmag.com/issues/07/sins-of-the-children", "content": "Sins of the Children. The circle of life on Chelicer 14d. When we reached the weather station it was so comprehensively trashed you’d think it’d been dropped from orbit. Torn apart and the pieces stomped on, the edges corrugated with dents and corroded with fluids. Something on this planet really didn’t want us to know when it was going to rain. “This is coming out of the use-budget,” Greffin said mournfully. She worked in Resources, liaising with the orbitingGarveneerto get what we needed. And we’d alreadyneededplenty to get ourselves set up planetside. “What’s our culprit and how do we kill it?” Merrit asked. The three of us had set the station up three days before and somehow it had riled the locals. Probably the sonic and radio chatter from using bounce-back to map meteorological systems. But nothing we’d seen on all of Chelicer 14d was big or aggressive enough to do this damage. I had my slate out to review our evolving catalog of Chelicer xenofauna. Merrit was on his haunches, studying the shrapnel; Greffin had a link to base camp at the farms, going over inventory to see what we could repurpose. Around us and the wreckage stretched the local scrub. Sedentary life on Chelicer was either low and spiny or tall and thin with a sort of puffball arrangement at the top. The land — the world — was dry, the ecosystem impoverished and short on species. My unfinished xenobio report went long on the idea that Chelicer had been lush in the past, and we’d arrived to find what had stabilized out of a catastrophic dry spell, or maybe some serious solar flare activity. There were no great forests to give cover to alien tigers. On Chelicer nothing grew past a shrub. One meter for the spiny stuff, two for the puffball poles. And the weather station had been up on high ground, ten klicks’ visibility in any direction. We weresafe. I heard a far-offchunk. A mechanical sound that — in the second’s pause before it impacted — I didn’t even connect withlife. The thing that came down right beside us was three meters high with a massive articulated body. A bug, really, Chelicer style. Eight crooked legs out from a central hub like all the mobile life here had, but most of what we’d seen was gracile, delicate, and came up to your waist. Even the Farmers — which we’d pegged as the most advanced species around — were only a meter and a half tall, and most of that was stilting limbs. This thing wasnotgracile. Every segment and joint of it was ridgy, armored, and spiky. It was dun and khaki like the planet’s dust, but too big to have hidden anywhere nearby, towering over the scrub. There were spread vanes like sails projecting from its back, but itcouldn’thave flown under organic power. It must have weighed five tons. We just stared. In that moment, when we could have run or called for help,we goggled at it. The stalked globes of its eyes looked back, devoid of living connection. A vast armored monster, airdropped from nowhere. I saw the motion, off on a neighboring hillside. There was a second monster out there, surprisingly hard to spot. It hunkered down, drawing its limbs in.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Graphics In Flatland – 2D ray tracing [video]", "url": "https://www.youtube.com/watch?v=WYTOykSqf2Y", "content": "Graphics In Flatland – 2D ray tracing [video]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "jQuery 4", "url": "https://blog.jquery.com/2026/01/17/jquery-4-0-0/", "content": "jQuery 4. On January 14, 2006, John Resig introduced a JavaScript library called jQuery at BarCamp in New York City. Now, 20 years later, the jQuery team is happy to announce the final release of jQuery 4.0.0. After a long development cycle and several pre-releases, jQuery 4.0.0 brings many improvements and modernizations. It is the first major version release in almost 10 years and includes some breaking changes, so be sure to read through the details below before upgrading. Still, we expect that most users will be able to upgrade with minimal changes to their code. Many of the breaking changes are ones the team has wanted to make for years, but couldn’t in a patch or minor release. We’ve trimmed legacy code, removed some previously-deprecated APIs, removed some internal-only parameters to public functions that were never documented, and dropped support for some “magic” behaviors that were overly complicated. We have anupgrade guideandjQuery Migrate plugin releaseready to assist with the transition. Please upgrade andlet us know if you encounter any issues. As usual, the release is available onour CDNand the npm package manager. Other third party CDNs will probably have it available soon as well, but remember that we don’t control their release schedules and they will need some time. Here are the highlights for jQuery 4.0.0. jQuery 4.0 drops support for IE 10 and older. Some may be asking why we didn’t remove support for IE 11. We plan to remove support in stages, and the next stepwill be released in jQuery 5.0. For now, we’ll start by removing code specifically supporting IE versions older than 11. We also dropped support for other very old browsers, including Edge Legacy, iOS versions earlier than the last 3, Firefox versions earlier than the last 2 (aside from Firefox ESR), and Android Browser. No changes should be required on your end. If you need to support any of these browsers, stick with jQuery 3.x. jQuery 4.0 adds support forTrusted Types, ensuring that HTML wrapped inTrustedHTMLcan be used as input to jQuery manipulation methods in a way that doesn’t violate therequire-trusted-types-forContent Security Policy directive. Along with this, while some AJAX requests were already using<script>tags to maintain attributes such ascrossdomain, we havesince switched most asynchronous script requests to use <script> tagsto avoid any CSP errors caused by using inline scripts. There are still a few cases where XHR is used for asynchronous script requests, such as when the\"headers\"option is passed (usescriptAttrsinstead!), but we now use a<script>tag whenever possible. It was a special day when the jQuery source on themainbranch was migrated fromAMDtoES modules. The jQuery source has always been published with jQuery releases on npm and GitHub, but could not be imported directly as modules withoutRequireJS, which was jQuery’s build tool of choice. We have since switched toRollupfor packaging jQuery and we do run all tests on the ES modules separately. This makes jQuery compatible with modern build tools, development workflows, and browsers through the use of<script type=module>. These functions have been deprecated for several versions. It’s time to remove them now that we’ve reached a major release. These functions were either always meant to be internal or ones that now have native equivalents in all supported browsers. The removed functions include:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Weight Transfer for RL Post-Training in under 2 seconds", "url": "https://research.perplexity.ai/articles/weight-transfer-for-rl-post-training-in-under-2-seconds", "content": "Weight Transfer for RL Post-Training in under 2 seconds. COMPANY Careers Press Inquires Brand Guidelines Supply Store Privacy Policy Security Terms & Conditions PRODUCT Comet Browser", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "GLM-4.7-Flash", "url": "https://huggingface.co/zai-org/GLM-4.7-Flash", "content": "GLM-4.7-Flash. 👋 Join ourDiscordcommunity.📖 Check out the GLM-4.7technical blog,technical report(GLM-4.5).📍 Use GLM-4.7-Flash API services onZ.ai API Platform.👉 One click toGLM-4.7. GLM-4.7-Flash is a 30B-A3B MoE model. As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency. Default Settings (Most Tasks) For multi-turn agentic tasks (τ²-Bench and Terminal Bench 2), please turn onPreserved Thinking mode. Terminal Bench, SWE Bench Verified τ^2-Bench For τ^2-Bench evaluation, we added an additional prompt to the Retail and Telecom user interaction to avoid failure modes caused by users ending the interaction incorrectly. For the Airline domain, we applied the domain fixes as proposed in theClaude Opus 4.5release report. For local deployment, GLM-4.7-Flash supports inference frameworks including vLLM and SGLang. Comprehensive deployment\ninstructions are available in the officialGithubrepository. vLLM and SGLang only support GLM-4.7-Flash on their main branches. using with transformers as", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The fix for a segfault that never shipped", "url": "https://www.recall.ai/blog/the-fix-for-a-segfault-that-never-shipped", "content": "The fix for a segfault that never shipped. AtRecall.ai, we run an unusual workload. We record millions of hours of meetings every month. Each of these meetings generates a large amount of audio and video we need to reliably capture and analyze. The audio we capture comes in a variety of shapes and sizes, an assortment of codecs, channels, sample rates, interleaving, and error-correction schemes. We normalize all of those into one consistent format that is universally playable. We launch 18 million of EC2 instances every month, each of these instances is a “meeting bot”, which joins a video call and captures the data in real-time. Extremely rarely, about 1 in 36 million bots would abruptly crash deep in library code of our media pipeline. Unlike most web servers, a meeting bot instance is extremely stateful and very hard to replace, a fatal error like this means the data is irrecoverably lost - forever. Even a one-in-tens-of-millions failure rate was unacceptable. This is the story of how we tracked down the rare bug, went to significant effort to reproduce it, identify the root cause and fix it. We encountered an extremely rare segfault in the AAC encoder we were using and root caused it to a bug in the fixed-point math C code. We found this was patched over a decade ago but the fix was never shipped to downstream consumers. Rather than fixing the bug we replaced the library with a modern AAC encoder which did not experience these crashes. This crash was so rare that reproducing it locally wasn’t feasible. Instead we opted to capture the program state from production, in the rare event this happens. Our first clue was the process 139 exit code (139 = 128 + 11). Signal 11 corresponds to SIGSEGV, a segmentation fault or segfault for short, this occurs when a program tries to access memory it is not supposed to. This mistake is bad enough that the execution of the program should halt immediately. So how do we determine the cause of a SIGSEGV? The answer is often acore dumpfile. A core dump lets us inspect the state of the program at the instant of the crash, this lets us see many useful things such as the stacktrace, variable/memory contents and more. Because our bots are run on ephemeral EC2 instances, the entire VM is terminated upon exit. So the core dump files are erased too. In order to diagnose the cause of this segfault we needed to start reliably collecting core dumps across nodes in our cluster. We shipped a minimal rust binary, affectionately named Garbage Truck, which configures the kernel to output core dumps to a well-known location and uploads them to S3.  After I had the core dump in hand, I spun up a new node on our cluster, using the same image where the crash occurred to ensure all binaries and system libraries are where the core dump expects them to be. We use a debian base image, so I installed gdb and loaded up the core dump: Now we have the stacktrace telling us where our program crashed. \nWe useGStreameras our media processing framework so it is unsurprising see the crash originating there. \nThe crash occurred in/lib/x86_64-linux-gnu/libvo-aacenc.so.0which was our AAC audio encoding library. \nThe majority of the stack frames are designated with??meaninggdbdoes not have the symbol information for the associated binaries. \nWithout this, besides staring blindly at the disassembly, there is little we can do to better understand the crash.\nAt this point I suspected the following in the order of probability:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A scammer's blueprint: How cybercriminals plot to rob a target in a week", "url": "https://www.reuters.com/graphics/SOUTHEASTASIA-SCAMS/MANUALS/klpyjlqelvg/", "content": "A scammer's blueprint: How cybercriminals plot to rob a target in a week. Are you free tonight?  NOTICE: Your bill is overdue.  Hello old friend, are you going to the party tonight?  hi there  hello ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "EU inc: a new European company structure", "url": "https://ec.europa.eu/commission/presscorner/detail/da/speech_26_150", "content": "EU inc: a new European company structure", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nuudel: Non-Tracking Appointment Tool", "url": "https://nuudel.digitalcourage.de/", "content": "Nuudel: Non-Tracking Appointment Tool. Termin finden Klassische Umfrage Wo sind meine Umfragen? Dieser Dienst wird vom gemeinnützigen Digitalcourage e.V. für Sie kostenlos angeboten. Unterstützen Sie den Betrieb und unsere Arbeit für eine lebenswerte Welt im digitalen Zeitalter alsFördermitglied! Digitalcourage:Newsletter|Spenden|English information", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I Burned $160k Trying to Solve \"Online Tailoring\"", "url": "https://www.indiehackers.com/post/how-i-burned-160-000-trying-to-solve-online-tailoring-the-engineering-reality-check-fbd7e0ccdd", "content": "I Burned $160k Trying to Solve \"Online Tailoring\". Report In 2023, I placed a crazy bet. I founded a fashion-tech startup with a vision of“Phygital Tailoring”. My goal was simple but audacious: Clients in the world should receive perfect-fit bespoke suits without ever leaving their homes. I entered the space with the arrogance of a typical disruptor:“If I just use high-resolution 3D scanning, I can replace the traditional tailor. Math will solve everything.” I was wrong. After 900 days of development and burning through $160,000 in savings, I realized why the current market solutions were failing. I used to think of “Fit” as a math problem. It isn’t. It’s a physics and logic problem. Here is the hard technical truth about why“Online Tailoring”became a graveyard for my initial capital, and the four engineering bottlenecks we had to overcome. Most scanning SDKs rely on the user holding the phone or placing it on a table. The Problem: Users struggle with geometry. My assumption that users could hold a device perfectly perpendicular to the floor was flawed. The Data: A mere 5-degree tilt results in a 2–3cm error in leg length. In bespoke tailoring, a 3cm error is the difference between a wearable suit and a disaster.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Robust Conditional 3D Shape Generation from Casual Captures", "url": "https://facebookresearch.github.io/ShapeR/", "content": "Robust Conditional 3D Shape Generation from Casual Captures. Robust Conditional 3D Shape Generation from Casual Captures Yawar Siddiqui, Duncan Frost, Samir Aroudj, Armen Avetisyan, Henry Howard-Jenkins, Daniel DeTone, Pierre Moulon, Qirui Wu†, Zhengqin Li, Julian Straub, Richard Newcombe, Jakob Engel Meta Reality Labs Research†Simon Fraser University From an input image sequence, ShapeR preprocesses per-object multimodal data (SLAM points, images, captions). A rectified flow transformer then conditions on these inputs to generate meshes object-centrically, producing a full metric scene reconstruction. Conditioned on off-the-shelf preprocessed inputs—SLAM points, 3D instances, and text—ShapeR infers per-object meshes to reconstruct the entire scene. While monolithic methods fuse the scene into one block, ShapeR reconstructs individual objects. This allows you to interact with and manipulate specific objects in the scene. ShapeR performs generative, object-centric 3D reconstruction from image sequences by leveraging multimodal inputs and robust training strategies. First, off-the-shelf SLAM and 3D instance detection are used to compute 3D points and object instances. For each object, sparse points, relevant images, 2D projections, and VLM captions are extracted to condition a rectified flow model, which denoises a latent VecSet to produce the 3D shape. The use ofmultimodal conditioning, along with heavyon-the-fly compositional augmentationsandcurriculum training, ensures the robustness of ShapeR in real-world scenarios. ShapeR conditions on a range of modalities, including the object's posed multiview images, SLAM points, text descriptions, and 2D point projections. ShapeR leverages single-object pretraining with extensive augmentations, simulating realistic backgrounds, occlusions, and noise across images and SLAM inputs. ShapeR is fine-tuned on object-centric crops from Aria Synthetic Environment scenes, which feature realistic image occlusions, SLAM point cloud noise, and inter-object interaction.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A Social Filesystem", "url": "https://overreacted.io/a-social-filesystem/", "content": "A Social Filesystem. January 18, 2026 Remember files? .doc.doc.doc.doc.jpg.jpg.svg You write a document, hit save, and the file is on your computer. It’s yours. You can inspect it, you can send it to a friend, and you can open it with other apps. Files come from the paradigm ofpersonal computing. This post, however, isn’t about personal computing. What I want to talk about issocial computing—apps like Instagram, Reddit, Tumblr, GitHub, and TikTok. What do files have to do with social computing? Historically, not a lot—until recently. aliceownsownsbob.doc.doc.doc.doc.doc.docpostpost.jpg.jpg.jpg.jpg.jpg.jpg.jpg.jpgfollowfollowvotevote But first, a shoutout to files.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Open-source tool for converting docs into .md and loading into Postgres", "url": "https://github.com/pgEdge/pgedge-docloader", "content": "Show HN: Open-source tool for converting docs into .md and loading into Postgres", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Threads edges out X in daily mobile users, new data shows", "url": "https://techcrunch.com/2026/01/18/threads-edges-out-x-in-daily-mobile-users-new-data-shows/", "content": "Threads edges out X in daily mobile users, new data shows. A report from market intelligence firmSimilarwebsuggests that Meta’s Threads is now seeing more daily usage than Elon Musk’s X on mobile devices. While X still dominates Threads on the web, the Threads mobile app for iOS and Android has continued to see an increase in daily active users over the past several months. Similarweb’s data shows that Threads had 141.5 million daily active users on iOS and Android as of January 7, 2026, after months of growth, while X has 125 million daily active users on mobile devices. This appears to be the result of longer-term trends, rather than a reaction to the recent X controversies, where users were discovered using the platform’s integrated AI, Grok, to create non-consensual nude images of women, including, sometimes minors. Concern around the deepfake images has now prompted California’s attorney generalto open an investigationinto Grok, following similar investigations by other regions,like the U.K., EU, India, Brazil, andmany more. The drama on X also led social networking startup Blueskyto see an increasein app installs in recent days. Instead, Threads’ boost in daily mobile usage may be driven by other factors, including cross-promotions from Meta’s larger social apps like Facebook and Instagram (where Threads is regularly advertised to existing users), its focus on creators, and the rapid rollout of new features. Over the past year, Threads has added features likeinterest-based communities,better filters,DMs,long-form text, anddisappearing posts, and has recently beenspotted testing games. Combined, the daily active user increases suggest that more people are using Threads on mobile as a more regular habit. According to Meta’s official numbers, the tech giant said in August 2025 that Threads hadreached over 400 million monthlyactive users. The company subsequently reportedin Octoberof last year that Threads had 150 million daily active users. The growth trends have been continuing for many months. Similarweblast summer reportedthat Threads was closing the gap with X on mobile devices after seeing 127.8% year-over-year growth as of late June 2025. Relatedly, Similarweb observed that X is still ahead of Threads in the U.S., but the gap is narrowing. A year ago, X had twice as many daily active users in the U.S. as it does now. In addition, Threads has little traction on the web while X maintains a fairly steady web audience with around 150 million daily web visits, according to Similarweb data. As of earlier this week (January 13), X was seeing 145.4 million daily web visits, while Threads saw 8.5 million daily web visits across Threads.com and Threads.net combined.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Folding NASA Experience into an Origamist's Toolkit (2024)", "url": "https://spinoff.nasa.gov/Folding_NASA_Experience_into_an_Origamist%E2%80%99s_Toolkit", "content": "Folding NASA Experience into an Origamist's Toolkit (2024). What does origami have in common with electronics? Here, math once again proves to be a universal language, spanning not just cultures but disciplines. The discovery of the mathematical underpinnings of folded paper art helped Robert Lang leave a 20-year engineering career, including over four years at NASA’s Jet Propulsion Laboratory in Southern California, to pursue his lifelong passion for turning paper into impossibly intricate three-dimensional forms. “Over the years of solving mathematical problems to describe lasers and optoelectronics, I built up a toolkit to use as I worked on a hobby basis on this problem of computational origami design,” said Lang. The Altadena, California-based artist holds dozens of patents for optoelectronics — technology that combines light and electricity — but after years of innovating in both fields, the tools he designed for origami are the ones he chose to move ahead with. In the Microdevices Laboratory at JPL in the late 1980s and early ’90s, Lang worked on integrating components like semiconductor lasers and spatial light modulators onto chips, with the ultimate goal of building an optical computer — one that uses light, rather than electricity, to transmit information and carry out calculations. Steady advances in electronic computing have since removed some of the incentives to develop optical computers. “One of the theoretical fields I learned about at JPL turned out to be the key to being able to plug in a description of a shape you wanted and then find the best possible design in great detail — every single crease you needed to make that shape,” said Lang. “And that turned out to be nonlinear constrained optimization.” It’s All About the Numbers A simple nonlinear constrained optimization problem would be the challenge of packing several different-sized balls into the smallest possible box, Lang explained. The constraint is that the balls can’t overlap each other, and the solutions are nonlinear because the balls can be any distance from each other. The optimization is in making the box as small as possible. Designing lasers and other components required a similar calculation to minimize energy consumption, the amount of semiconductor material, and other costs, said Lang. In origami, he said, optimization means creating the largest form possible out of a given sheet of paper. Design begins with mapping the points on that sheet that will become features like a head and limbs. “I found there was an equation that said the distance between any of those two points had to be greater than or equal to a mathematical function that related to where they were in the shape I was after,” Lang said. “And that was really the breakthrough, was figuring out how to mathematically describe that constraint for every possible pair of points in the crease patterns.” The math was too complex to solve by hand but easy for a computer to resolve using known algorithms. The ability to put the problem into accurate mathematical terms “lets you tap into all of the existing mathematics and computer techniques to solve it,” Lang said.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "High-speed train collision in Spain kills at least 39", "url": "https://www.bbc.com/news/articles/cedw6ylpynyo", "content": "High-speed train collision in Spain kills at least 39. At least 39 people have died in a train collision in southern Spain and dozens more have been injured in the country's worst rail crash in more than a decade, Spain's Civil Guard has said. Carriages on a Madrid-bound train derailed and crossed over to the opposite tracks, colliding with an oncoming train in Adamuz on Sunday evening. Four hundred passengers and staff were onboard both trains, the rail networks said. Emergency services treated 122 people, with 43, including four children, still in hospital. Of those, 12 adults and one child are in intensive care. Spanish Transport Minister Óscar Puente said the death toll \"is not yet final\", as officials launched an investigation. Puente described the incident as \"extremely strange\". All the railway experts consulted by the government \"are extremely baffled by the accident\", he told reporters in Madrid. Rail network operator Adif said the collision happened at 19:45 local time (18:45 GMT), about an hour after the train left Málaga heading northto Madrid, when it derailed on a straight stretch of track near the city of Córdoba. The force of the crash pushed the carriages of the second train into an embankment, Puente said. He added that most of those killed and injured were in the front carriages of the second train, which was travelling southfrom Madrid to Huelva. The type of train involved in the crash was a Freccia 1000, which can reach top speeds of 400 km/h (250 mph), a spokesperson for the Italian rail company Ferrovie dello Stato told Reuters news agency. Rescue teams said the twisted wreckage of the trains made it difficult to recover people trapped inside the carriages. Córdoba fire chief Francisco Carmona told Spanish public broadcaster RTVE: \"We have even had to remove a dead person to be able to reach someone alive. It is hard, tricky work.\"", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: I built a tool to assist AI agents to know when a PR is good to go", "url": "https://dsifry.github.io/goodtogo/", "content": "Show HN: I built a tool to assist AI agents to know when a PR is good to go. The missing piece in AI-assisted development: knowing when you’re actually done. AI coding agents are transforming software development. They can write code, fix bugs, respond to review comments, and create pull requests. But they all share one fundamental problem: They can’t reliably know when a PR is ready to merge. Think about it. When you ask an AI agent to “fix the CI and address the review comments,” how does it know when it’s finished? Without deterministic answers, agents either: Good To Go provides a single command that answers the question definitively: That’s it. One command. One answer. No ambiguity. No guessing. No infinite loops. Good To Go analyzes your PR across three dimensions: Combines all GitHub check runs and commit statuses into a single pass/fail/pending state. Handles the complexity of multiple CI systems, required vs optional checks, and in-progress runs.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup", "url": "https://cua.ai/docs/lume/guide/getting-started/introduction", "content": "Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup. Introduction to Lume - the macOS VM CLI and framework Lume is a VM runtime for building AI agents, running CI/CD pipelines, and automating macOS. It uses Apple's native Virtualization Framework to run macOS and Linux VMs at near-native speed on Apple Silicon. MIT License Lume is open-source and MIT licensed. If you find it useful, we'd appreciate astar on GitHub! Cloud macOS Sandboxes We're piloting a managed service for customers who want to run cloud macOS sandboxes for CI/CD and agent workloads.Book a demoif you're interested. A single binary with an HTTP API. Create a VM, run it headlessly, control it programmatically.  You can use Lume directly via CLI, or runlume serveto expose an HTTP API for programmatic access. TheComputer SDKuses this API to automate macOS interactions. Lume is a thin layer over Apple'sVirtualization Framework, which provides hardware-accelerated virtualization on Apple Silicon. This gives you:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "SWE-gen: Scaling SWE-bench task generation", "url": "https://github.com/abundant-ai/SWE-gen", "content": "SWE-gen: Scaling SWE-bench task generation", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Understanding C++ Ownership System", "url": "https://blog.aiono.dev/posts/understanding-c++-ownership-system.html", "content": "Understanding C++ Ownership System. I recently started using C++ at my$DAY_JOBand, along with that, decided to study C++ again. I think writing down your understanding is the best way to learn a topic. One part I find that is hard to understand in C++ is how the object ownership model works because it's not a single concept but a collection of a couple of smaller concepts. By ownership I mean creating and destroying objects, giving references to an object, and transferring ownership of an object. There is no one guide that covers everything. These concepts are very important to write and read modern C++ (though I doubt if C++11 is still considered \"modern\"). Even if you just want to write C with Classes-style C++, you will probably use standard containers likestd::vector, which requires an understanding of C++ ownership related features such as RAII, references, and move semantics to use it properly. Without knowing those, you simply can't have the correct memory model for C++, resulting in buggy programs full of undefined behaviors and inefficient programs due to unnecessary copying. By knowing these concepts, you can both avoid introducing bugs due to lack of understanding and reason about programs better. This writing is my understanding of C++ ownership model. I think it can be useful to you if you have a basic level understanding of C++ and you want to learn more, or you are familiar with C++ but never learned the concepts and terminology formally. In C++, every object has an owner, which is responsible for cleaning up the data once it's not used anymore. If you come from garbage collected languages, the concept of ownership may seem strange to you. But consider the following code: This is a function that returns the file's name as a C-style string. What's not documented though, is who is supposed to deallocate the returned string. In this case there are two possibilities: Depending on which one is the case, the caller must act differently. This is because theownerof the data is different between these two cases. In the first case, owner is the variable assigned to the function's return value, and in the second, owner is thefilevariable. If the latter is the case, the variable assigned to the return valueborrowsthe data owned byfile. In a garbage collected language, you don't have this distinction because, in a sense every variable is a borrower, and the owner is the garbage collector (GC). The GC just ensures that the data is allocated as long as there is a reference (borrower) to it. But at the same time every variable can be considered an owner since holding a variable keeps the data alive. C++ doesn't have a garbage collector, but it has a mechanism to automate some parts of object creation and destruction. When you declare a variable to an object in C++, it will create the object, and the variable will be the owner of the object. If the object has a destructor (a special function that destroys the resources represented by the object), it's automatically destroyed when the block of the variable ends. This technique of connecting resources to variables is called RAII[1]. The span that object is alive is calledthe lifetimeof the object. In modern C++, it is advised that you create objects with destructors so the resources are cleaned up automatically at the end of their lifetime. To see why, consider this example: The code essentially reads some data and writes it tobuffer, and then adds the processed output of thebufferintoresult. Assume thatreadmaythrow, which means that it's possible to never execute thedeletestatement beforereturn. To handle this case, then we must write atry catchblock to delete thebufferin case an exception occurs and then rethrowthe exception.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: What are the recommender systems papers from 2024-2025?", "url": "item?id=46692368", "content": "Ask HN: What are the recommender systems papers from 2024-2025?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Legal Structures for Latin American Startups (2021)", "url": "https://latamlist.com/legal-structures-for-latin-american-startups/", "content": "Legal Structures for Latin American Startups (2021). There’s confusion around what legal structures make sense for Latin American startups. Founders, VCs and even lawyers can make decisions that can cost upwards of $100M if you get it wrong. This post is the result of investing in 80+ startups from 15+ Latin American countries since 2014 viaMagma Partners, and speaking to and working with countless lawyers across LatAm, US, UK, Europe and multiple offshore jurisdictions. I wrote a version of this that I’ve been sharing with Magma Partners founders internally and decided to open source it with the hope that founders save themselves time and money and make themselves more investable. There are fairly clear outlines that most Latin American startups should likely follow. Every startup’s case is different, and each founder should get legal advice from a lawyer and tax advice from an accountant with relevant US and Latin American venture capital experience before following this guide or anyone else’s ideas. To be clear, this is not legal or tax advice. You should always work with a lawyer and accountant when thinking about corporate structures. The money you’ll spend getting good advice will save hundreds of thousands or even hundreds of millions of dollars down the road. I can’t stress this enough. Don’t just follow these guidelines. Your situation is unique. Talk to an experienced lawyer and accountant. Let’s start with a story.Brian Requarth,cofounder of Vivareal andLatitudhad a big exit in 2020. His structurecost him and his investors $100M: In the early days of a startup, money is tight and it’s common to cut corners. I created a California LLC for my company because of my local accountant’s advice. He had zero experience with VC or Latin America. Later, I hired a my hometown law firm that had no VC experience, which advised me to create a C-Corp, which seemed like good advice at the time. We later realized that even though our business had no operations in the US, we would be subject to US taxes upon an exit. We had raised VC money and at this point it was cost prohibitive to restructure. We later merged with our competitors. We retained top lawyers & accountants to help us manage our extremely complex deal. The deal took an unnecessarily crazy amount of time and effort because of our original structure. But we finally came up with a solution we thought worked. When we ended up selling our combined business to OLX Brasil, we signed a term sheet, but during the due diligence they opted to buy our local entities because they saw our restructuring as a huge risk. We paid millions of dollars to lawyers & accountants to get this deal done.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Simple Sabotage Field Manual (1944) [pdf]", "url": "https://www.cia.gov/static/5c875f3ec660e092cf893f60b4a288df/SimpleSabotage.pdf", "content": "Simple Sabotage Field Manual (1944) [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Carney says old world order 'is not coming back'", "url": "https://www.bbc.com/news/articles/cly3d28p4p8o", "content": "Carney says old world order 'is not coming back'. Canadian Prime Minister Mark Carney said the \"old order is not coming back\" and urged fellow middle powers to come together in a speech at the World Economic Forum in Davos, Switzerland. \"Middle powers must act together because if we're not at the table, we're on the menu,\" Carney said on Tuesday, adding that he believed powerful nations were using economic coercion to get what they want. He also affirmed Canada's support for Greenland, Denmark and the Nato alliance, drawing applause. Carney did not mention Donald Trump by name, but some of his remarks seemed aimed at the US president, who is threatening to tariff European allies and the UK unless Greenland is surrendered to the US. \"Great powers\" are often defined as countries with permanent seats on United Nations Security Council - China, France, Russia, the United Kingdom and the United States - which shows their economic and military dominance in the world. Middle powers, such as Canada, Australia, Argentina, South Korea and Brazil, are nations that still exert large influence in global politics, even though their economies are smaller. In his speech, Carney said the world is \"in the midst of a rupture, not a transition\". \"Great powers have begun using economic integration as weapons, tariffs as leverage, financial infrastructure as coercion, supply chains as vulnerabilities to be exploited,\" he said. He also said \"Canada was amongst the first to hear the wake-up call\" that geography and historic alliances no longer guaranteed security or prosperity. When Trump returned to office, he frequently referred to Canada as the \"51st state\" and threatened to join Canada and the US through \"economic force.\" The US then hit its northern neighbour and major trading partner with steep tariffs.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Gladys West's vital contributions to GPS technology", "url": "https://en.wikipedia.org/wiki/Gladys_West", "content": "Gladys West's vital contributions to GPS technology.  Gladys Mae West(néeBrown; October 27, 1930 – January 17, 2026) was an American mathematician. She was known for her contributions to mathematical modeling of theshape of the Earth, and her work on the development ofsatellite geodesymodels, that were later incorporated into theGlobal Positioning System(GPS).[1] West was inducted into theUnited States Air ForceHall of Fame in 2018. She was awarded theWebby Lifetime Achievement Awardfor the development of satellite geodesy models.[2][3] Gladys Mae Brown was born inSutherland, Virginia, inDinwiddie County, a rural county south ofRichmond, on October 27, 1930.[1][4][5][6]Her family was an African-American farming family in a community ofsharecroppers. She spent much of her childhood working on her family's small farm.[7][8]As well as working on the farm, her mother worked in a tobacco factory and her father worked for the railroad.[5][9]West saw education as her way to a different life.[10] At West's high school, the top two students from each graduating class received full scholarships to Virginia State College (Virginia State University(VSU)), ahistorically black public university.[7]West graduated as valedictorian in 1948, and received the scholarship.[5][10]At VSU, she chose to study mathematics, a subject that was mostly studied by men.[7]She also joined theAlpha Kappa Alphasorority.[1]West graduated in 1952 with aBachelor of Sciencedegree in mathematics,[5]and then taught mathematics and science for two years inWaverly, Virginia.[5]West returned to VSU to complete aMaster of Mathematicsdegree, graduating in 1955.[10][5]Afterwards, she began another teaching position inMartinsville, Virginia.[5] In 1956, West was hired to work at the Naval Proving Ground inDahlgren, Virginia (later theNaval Surface Warfare Center). She was the second black woman hired and one of only four black employees.[7][4][1]She was a computer programmer in theDahlgren division, and a project manager for processing systems for satellite data analysis.[12]Concurrently, West earned aMaster's degree in Public Administrationfrom theUniversity of Oklahoma.[5] In the early 1960s, West participated in an award-winning study that proved the regularity ofPluto’s motion relative toNeptune.[13]Subsequently, West began to analyze satellite altimeter data from NASA'sGeodetic Earth Orbitingprogram, to create models of the Earth's shape. She became project manager for the short-livedSeasatradar altimetry project, the first satellite that couldremotely senseoceans.[14][15]West's work cut her team's processing time in half, and she was recommended for a commendation.[16] From the mid-1970s through the 1980s, West programmed anIBM 7030 Stretchcomputer to deliver increasingly precise calculations for theshape of the Earth; anellipsoidwith additional undulations known as thegeoid.[8]To generate an accurategeopotentialmodel West needed to use complex algorithms to account for variations in the gravitational, tidal, and other forces that distort Earth's shape.[9] In 1986, West publishedData Processing System Specifications for the Geosat Satellite Radar Altimeter, a 51-page technical report from theNaval Surface Weapons Center(NSWC). This explained how to improve the accuracy ofgeoid heightsandvertical deflection, important components ofsatellite geodesy.[1]This was achieved by processing data from the radio altimeter on theGeosat satellite, which went into orbit on March 12, 1984.[11] West worked at Dahlgren for 42 years, and retired in 1998.[7]In 2000, she completed aPhDin Public Administration atVirginia Techby distance-learning.[13][17][18]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "There's a hidden Android setting that spots fake cell towers", "url": "https://www.howtogeek.com/theres-a-hidden-android-setting-that-spots-fake-cell-towers/", "content": "There's a hidden Android setting that spots fake cell towers. Most people never give a second thought to how their phone connects to a cell tower. It’s something that constantly happens in the background without our input, and therein lies the potential for trouble. What if that tower isn't what it seems? Android can tell you about it—maybe. Let’s get the scary stuff out of the way first. “Stingrays,” technically known as IMSI (international mobile subscriber identity) catchers, are devices primarily used for surveillance. They mimic cell towers and act as a middleman between your phone and the network. Once your device is tricked into connecting to what it believes to be a real cell tower, the attacker can harvest device information and force your phone onto an older, unencrypted protocol. This is what allows them to listen to your calls or read your texts without you ever knowing something is wrong. It’s also possible for the attacker to harvest information from the phones of people nearby when this happens. While Stingrays have been used by law enforcement agencies for years to track suspects, it’s now much easier for malicious individuals to get their hands on them and skim data from innocent people. You might think that switching from Facebook Messenger to old-fashioned text messages would help protect your privacy. But standard SMS text messages aren't very private or secure. SMS is like fax---an old, outdated standard that refuses to go away. The good news is that Google has been slowly building a wall against these attacks—emphasis on “slowly.” In 2021, Google released Android 12 with the ability to disable 2G connectivity. Stringrays like this network for its weak security. Two years later, it announced that Android 14 would support disabling an old form of encryption that makes it easy to intercept SMS and calls. Then Android 15 addressed Stingrays with the ability to notify the OS when a network requests your identifiers or forces you onto a less secure encryption method. That brings us up to Android 16—thelatest version. While all of those aforementioned features sounded great at the time, only one was actually available before last year:disabling 2G connectivity. The hold-up is due to the fact that software can only do so much. For these security features to work, your phone's modem has to be able to communicate with the Android OS in a very specific way, and that’s just not something many Android phones have right now. Because of this hardware requirement, the full suite of these network security tools is currently exclusive to the Pixel 10 series. They can be found under the “Mobile Network Security” section in the system settings. Even if you have a brand new Galaxy S25 running One UI 8/Android 16, you probably only have access to the 2G toggle (shown above). It’s better than nothing, but it’s not the complete picture. If you do happen to use a Pixel 10, it’s very easy to enable the extra network security. Google doesn’t enable them by default. Open the Settings and navigate to Security & privacy > More security & privacy > Mobile network security. Inside, you’ll see two toggles: Should one of the scenarios mentioned in “Network notifications” occur, you’llget an alertthat says you’ve connected to an unencrypted network and your data is vulnerable. Alerts can also tell you if your device’s information was recorded, including the time it happened and how often it’s been happening. These notifications can be invaluable in protecting yourself, and enabling them is more important than ever. It’s unfortunate that the vast majority of Android devices still only have the 2G toggle. The Pixel 10 series was released late last year, so hopefully 2026 brings more Android phones that ship with the required hardware for the Mobile Network Security suite. Fake cell towers may sound like something from a spy movie, but it’s the reality we live in.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "'The old order is not coming back,' Carney says in speech at Davos", "url": "https://www.cbc.ca/news/politics/carney-davos-speech-9.7052725", "content": "'The old order is not coming back,' Carney says in speech at Davos. Prime Minister Mark Carney delivered a frank assessment of how he views the world in a provocative speech in Davos, Switzerland, on Tuesday, where he said the longstanding U.S.-led, rules-based international order is over and middle powers like Canada must pivot to avoid falling prey to further \"coercion\" from powerful actors. Without invoking U.S. President Donald Trump by name, Carney referenced \"American hegemony\" and said \"great powers\" are using economic integration as \"weapons.\" \"Canadians know that our old, comfortable assumption that our geography and alliance memberships automatically conferred prosperity and security is no longer valid,\" Carney said. As it grapples with this new dynamic, Carney said Canada must be \"principled and pragmatic\" and turn inward to build up the country and diversify trading relationships to become less reliant on countries like the U.S., now that it's clear \"integration\" can lead to \"subordination.\" Carney said multilateralism and the \"architecture of collective problem-solving\" — relying on institutions like the World Trade Organization, the United Nations and Conference of the Parties (COP) for climate talks — has been \"diminished\" and countries have to accept they may have to go it alone more often than in the recent past. \"Many countries are drawing the same conclusions. They must develop greater strategic autonomy: in energy, food, critical minerals, in finance and supply chains. \"A country that cannot feed itself, fuel itself or defend itself has few options. When the rules no longer protect you, you must protect yourself,\" Carney said. 'The old order is not coming back': PM says Canada must 'name reality' and build strength at home Carney said this more isolationist approach, where there's a \"world of fortresses,\" will make countries poorer, fragile and less sustainable. But it's coming nonetheless and Canada must work with like-minded allies where possible to push back against domination by larger, wealthier and well-armed countries. \"This is not naive multilateralism. Nor is it relying on diminished institutions. It is building the coalitions that work, issue by issue, with partners who share enough common ground to act together. Middle powers must act together because if you are not at the table, you are on the menu,\" Carney said.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: What's an API that you wish existed?", "url": "item?id=46691222", "content": "Ask HN: What's an API that you wish existed?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Movieagent.io – An agent for movie recommendations (with couple mode)", "url": "https://movieagent.io", "content": "Show HN: Movieagent.io – An agent for movie recommendations (with couple mode)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: E80: an 8-bit CPU in structural VHDL", "url": "https://github.com/Stokpan/E80", "content": "Show HN: E80: an 8-bit CPU in structural VHDL", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Will AI Pet My Dog for Me", "url": "https://eieio.games/blog/will-ai-pet-my-dog-for-me/", "content": "Will AI Pet My Dog for Me. What work do I want to skip? Jan 19, 2026 I have a dog. Her name is Gabby. She’s lovely. Gabby and me last summer She’s a lot of work. A dog walker handles Gabby’s afternoon walks. It’s nice to outsource the afternoon walk since it interrupts my day. Gabby’s appetite for play and affection is insatiable. I sometimes want to outsource more of the work of taking care of her. To align my goals of “building as much weird software as possible” and “taking care of my dog,” I could outsourceallof the work of caring for Gabby. Someone else could walk her and feed her and pet her, leaving me free tofind better UUIDs. My time is valuable; this would be an efficient use of my money. But I don’t do that. I like petting my dog. I like to understand things. I like to share that understanding with others. This is my favorite part of building software. Until recently Ihadto understand the software that I developed.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: A creative coding library for making art with desktop windows", "url": "https://github.com/willmeyers/window-art", "content": "Show HN: A creative coding library for making art with desktop windows", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Opening the AWS European Sovereign Cloud", "url": "https://aws.amazon.com/blogs/aws/opening-the-aws-european-sovereign-cloud/", "content": "Opening the AWS European Sovereign Cloud. Deutsch| English |Español|Français|Italiano As a European citizen, I understand first-hand the importance of digital sovereignty, especially for our public sector organisations and highly regulated industries. Today, I’m delighted to share that theAWS European Sovereign Cloudis now generally available to all customers.We first announced our plans to build this new independent cloud infrastructure in 2023, and today it’s ready to meet the most stringent sovereignty requirements of European customerswith a comprehensive set of AWS services. Berlin, Brandenburg Gate at sunset Meeting European sovereignty requirementsOrganizations across Europe face increasingly complex regulatory requirements around data residency, operational control, and governance independence. Too often today, European organisations with the highest sovereignty requirements are stuck in legacy on-premises environments or offerings with reduced services and functionality. In response to this critical need, the AWS European Sovereign Cloud is the only fully featured and independently operated sovereign cloud backed by strong technical controls, sovereign assurances, and legal protections. Public sector entities and businesses in highly regulated industries need cloud infrastructure that provides enhanced sovereignty controls that maintain the innovation, security, and reliability they expect from modern cloud services. These organisations require assurance that their data and operations remain under European jurisdiction, with clear governance structures and operational autonomy within the European Union (EU). A new independent cloud infrastructure for EuropeThe AWS European Sovereign Cloud represents a physically and logically separate cloud infrastructure, with all components located entirely within the EU. The firstAWS Regionin the AWS European Sovereign Cloud is located in the state of Brandenburg, Germany, and is generally available today. This Region operates independently from existing AWS Regions. The infrastructure features multiple Availability Zones with redundant power and networking, designed to operate continuously even if connectivity with the rest of the world is interrupted. We plan to extend the AWS European Sovereign Cloud footprint from Germany across the EU to support stringent isolation, in-country data residency, and low latency requirements. This will start with new sovereignLocal Zoneslocated in Belgium, the Netherlands, and Portugal. In addition, you will be able to extend the AWS European Sovereign Cloud infrastructure withAWS Dedicated Local Zones,AWS AI Factories, orAWS Outpostsin locations you select, including your own on-premises data centres. The AWS European Sovereign Cloud and its Local Zones provide enhanced sovereign controls through its unique operational model. The AWS European Sovereign Cloud will be operated exclusively by EU residents located in the EU. This covers activities such as day-to-day operations, technical support, and customer service. We’re gradually transitioning the AWS European Sovereign Cloudto be operated exclusively by EU citizens located in the EU. During this transition period, we will continue to work with a blended team of EU residents and EU citizens located in the EU. The infrastructure is managed through dedicated European legal entities established under German law. In October 2025, AWS appointedStéphane Israël, an EU citizen residing in the EU, as managing director. Stéphane will be responsible for the management and operations of the AWS European Sovereign Cloud, including infrastructure, technology, and services, as well as leading AWS broader digital sovereignty efforts. In January 2026, AWS also appointedStefan Hoechbauer(Vice President, Germany and Central Europe, AWS) as a managing director of the AWS European Sovereign Cloud. He will work alongside Stéphane Israel to lead the AWS European Sovereign Cloud. An advisory board comprised exclusively of EU citizens, and including two independent third-party representatives, provides additional oversight and expertise on sovereignty matters. Enhanced data residency and controlThe AWS European Sovereign Cloud provides comprehensive data residency assurances so you can meet the most stringent data residency requirements. As with our existing AWS Regions around the world, all your content remains within the Region you select unless you choose otherwise. Beyond content, customer-created metadata including roles, permissions, resource labels, and configurations also stays within the EU. The infrastructure features its own dedicatedAWS Identity and Access Management (IAM)and billing system, all operating independently within European borders.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Texas police invested in phone-tracking software and won’t say how it’s used", "url": "https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/", "content": "Texas police invested in phone-tracking software and won’t say how it’s used", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "String theory can now describe a universe that has dark energy?", "url": "https://www.quantamagazine.org/string-theory-can-now-describe-a-universe-that-has-dark-energy-20260114/", "content": "String theory can now describe a universe that has dark energy?. January 14, 2026 Scientists have struggled to make string theory compatible with the expanding universe. Nash Weerasekera forQuanta Magazine In 1998, astronomers discovered dark energy. The finding, which transformed our conception of the cosmos, came with a little-known consequence: It threw a wrench into the already daunting task of finding a version of string theory that describes the universe we live in. Dark energy is a “positive” energy that causes our universe to expand at an accelerating rate. But the best-understood models of string theory describe universes with energy that is either negative or zero. Of the various criticisms made of string theory through the years — that it only works in a 10-dimensional universe, that its fundamental constituents, tiny strings, are too small to ever be observed — this was perhaps the most troubling. String theory appeared to be useful only for describing a universe with a negative “anti-de Sitter” geometry, whereas we live in a universe with a positive “de Sitter” geometry. Then last year, two physicists offered a stripped-down but precise formula forhow string theory could give rise to a universe similar to ours— a de Sitter universe undergoing accelerated expansion. “It is the very first example [from string theory] of an explicit de Sitter space,” saidThomas Van Rietof KU Leuven in Belgium. The new work, byBruno BentoandMiguel Monteroof the Institute for Theoretical Physics in Madrid, describes a universe with a dark energy that should weaken over time — a result thatmatches preliminary cosmic observationsfrom the past few years. But the universe they describe is not exactly like ours. While their original hope was to reduce the high-dimensional world of string theory to our own four-dimensional world, they ended up with an extra dimension. “What they have found is a 5D de Sitter solution, and we don’t live in 5D,” saidAntonio Padillaof the University of Nottingham.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Floating-Point Printing and Parsing Can Be Simple and Fast", "url": "https://research.swtch.com/fp", "content": "Floating-Point Printing and Parsing Can Be Simple and Fast. A floating point numberfhas the formf=m·2ewheremis called themantissaandeis a signed integerexponent.\nWe like to read numbers scaled by powers of ten,\nnot two, so computers need algorithms to convert binary floating-point\nto and from decimal text.\nMy 2011 post “Floating Point to Decimal Conversion is Easy”\nargued that  these conversions can be simple as long as you\ndon’t care about them being fast.\nBut I was wrong: fast converters can be simple too,\nand this post shows how.The main idea of this post is to implementfast unrounded scaling,\nwhich computes an approximation tox·2e·10p,\noften in a single 64-bit multiplication.\nOn that foundation\nwe can build nearly trivial printing and parsing algorithms that run very fast.\nIn fact, the printing algorithms\nrun faster than all other known algorithms,\nincluding\nDragon4 [30],\nGrisu3 [23],\nErrol3 [4],\nRyū [2],\nRyū Printf [3],\nSchubfach [12],\nand Dragonbox [17],\nand the parsing algorithm runs faster than\nthe Eisel-Lemire algorithm [22].\nThis post presents both the algorithms and a concrete implementation in Go.\nI expect some form of this Go code to ship in Go 1.27 (scheduled for August 2026).This post is rather long—far longer than the implementations!—so here is a brief overview of the sections\nfor easier navigation and understanding where we’re headed.“Fixed-Point and Floating-Point Numbers”\nbriefly reviews fixed-point and floating-point numbers,\nestablishing some terminology and concepts needed for the rest of the post.“Unrounded Numbers” introduces the idea of unrounded numbers,\ninspired by the IEEE754 floating-point extended format.“Unrounded Scaling” defines the unrounded scaling primitive.“Fixed-Width Printing” formats floating-point numbers\nwith a given (fixed) number of decimal digits, at most 18.“Parsing Decimals” parses decimal numbers of\nat most 19 digits into floating-point numbers.“Shortest-Width Printing” formats floating-point numbers\nusing the shortest representation that parses back to the original number.“Fast Unrounded Scaling” reveals the\nshort but subtle implementation of fast unrounded scaling\nthat enables those simple algorithms.“Sketch of a Proof of Fast Scaling” briefly sketches the proof\nthat the fast unrounded scaling algorithm is correct.\nA companion post, “Fast Unrounded Scaling: Proof by Ivy”\nprovides the full details.“Omit Needless Multiplications” uses a key idea from the proof\nto optimize the fast unrounded scaling implementation further,\nreducing it to a single 64-bit multiplication in many cases.“Performance” compares the performance of the\nimplementation of these algorithms against earlier ones.“History and Related Work” examines the history of\nsolutions to the floating-point printing and parsing problems\nand traces the origins of the specific ideas used in this\npost’s algorithms.For the last decade, there has been a new algorithm for floating-point printing and parsing\nevery few years.\nGiven the simplicity and speed of the algorithms in this post\nand the increasingly small deltas between successive algorithms,\nperhaps we are nearing an optimal solution.Fixed-Point and Floating-Point NumbersFixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: The main idea of this post is to implementfast unrounded scaling,\nwhich computes an approximation tox·2e·10p,\noften in a single 64-bit multiplication.\nOn that foundation\nwe can build nearly trivial printing and parsing algorithms that run very fast.\nIn fact, the printing algorithms\nrun faster than all other known algorithms,\nincluding\nDragon4 [30],\nGrisu3 [23],\nErrol3 [4],\nRyū [2],\nRyū Printf [3],\nSchubfach [12],\nand Dragonbox [17],\nand the parsing algorithm runs faster than\nthe Eisel-Lemire algorithm [22].\nThis post presents both the algorithms and a concrete implementation in Go.\nI expect some form of this Go code to ship in Go 1.27 (scheduled for August 2026).This post is rather long—far longer than the implementations!—so here is a brief overview of the sections\nfor easier navigation and understanding where we’re headed.“Fixed-Point and Floating-Point Numbers”\nbriefly reviews fixed-point and floating-point numbers,\nestablishing some terminology and concepts needed for the rest of the post.“Unrounded Numbers” introduces the idea of unrounded numbers,\ninspired by the IEEE754 floating-point extended format.“Unrounded Scaling” defines the unrounded scaling primitive.“Fixed-Width Printing” formats floating-point numbers\nwith a given (fixed) number of decimal digits, at most 18.“Parsing Decimals” parses decimal numbers of\nat most 19 digits into floating-point numbers.“Shortest-Width Printing” formats floating-point numbers\nusing the shortest representation that parses back to the original number.“Fast Unrounded Scaling” reveals the\nshort but subtle implementation of fast unrounded scaling\nthat enables those simple algorithms.“Sketch of a Proof of Fast Scaling” briefly sketches the proof\nthat the fast unrounded scaling algorithm is correct.\nA companion post, “Fast Unrounded Scaling: Proof by Ivy”\nprovides the full details.“Omit Needless Multiplications” uses a key idea from the proof\nto optimize the fast unrounded scaling implementation further,\nreducing it to a single 64-bit multiplication in many cases.“Performance” compares the performance of the\nimplementation of these algorithms against earlier ones.“History and Related Work” examines the history of\nsolutions to the floating-point printing and parsing problems\nand traces the origins of the specific ideas used in this\npost’s algorithms.For the last decade, there has been a new algorithm for floating-point printing and parsing\nevery few years.\nGiven the simplicity and speed of the algorithms in this post\nand the increasingly small deltas between successive algorithms,\nperhaps we are nearing an optimal solution.Fixed-Point and Floating-Point NumbersFixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: This post is rather long—far longer than the implementations!—so here is a brief overview of the sections\nfor easier navigation and understanding where we’re headed.“Fixed-Point and Floating-Point Numbers”\nbriefly reviews fixed-point and floating-point numbers,\nestablishing some terminology and concepts needed for the rest of the post.“Unrounded Numbers” introduces the idea of unrounded numbers,\ninspired by the IEEE754 floating-point extended format.“Unrounded Scaling” defines the unrounded scaling primitive.“Fixed-Width Printing” formats floating-point numbers\nwith a given (fixed) number of decimal digits, at most 18.“Parsing Decimals” parses decimal numbers of\nat most 19 digits into floating-point numbers.“Shortest-Width Printing” formats floating-point numbers\nusing the shortest representation that parses back to the original number.“Fast Unrounded Scaling” reveals the\nshort but subtle implementation of fast unrounded scaling\nthat enables those simple algorithms.“Sketch of a Proof of Fast Scaling” briefly sketches the proof\nthat the fast unrounded scaling algorithm is correct.\nA companion post, “Fast Unrounded Scaling: Proof by Ivy”\nprovides the full details.“Omit Needless Multiplications” uses a key idea from the proof\nto optimize the fast unrounded scaling implementation further,\nreducing it to a single 64-bit multiplication in many cases.“Performance” compares the performance of the\nimplementation of these algorithms against earlier ones.“History and Related Work” examines the history of\nsolutions to the floating-point printing and parsing problems\nand traces the origins of the specific ideas used in this\npost’s algorithms.For the last decade, there has been a new algorithm for floating-point printing and parsing\nevery few years.\nGiven the simplicity and speed of the algorithms in this post\nand the increasingly small deltas between successive algorithms,\nperhaps we are nearing an optimal solution.Fixed-Point and Floating-Point NumbersFixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: For the last decade, there has been a new algorithm for floating-point printing and parsing\nevery few years.\nGiven the simplicity and speed of the algorithms in this post\nand the increasingly small deltas between successive algorithms,\nperhaps we are nearing an optimal solution.Fixed-Point and Floating-Point NumbersFixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Fixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: [The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nvidia contacted Anna's Archive to access books", "url": "https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/", "content": "Nvidia contacted Anna's Archive to access books. Chip giant NVIDIA has been one of the main financial beneficiaries in the artificial intelligence boom. Revenue surged due to high demand for its AI-learning chips and data center services, and the end doesn’t appear to be in sight. Besides selling the most sought-after hardware, NVIDIA is also developing its own models, including NeMo, Retro-48B, InstructRetro, and Megatron. These are trained using their own hardware and with help from large text libraries, much like other tech giants do. Like other tech companies, NVIDIA has also seen significant legal pushback from copyright holders in response to its training methods. This includes authors, who, in various lawsuits, accused tech companies of training their models on pirated books. In early 2024, for example, several authorssued NVIDIAover alleged copyright infringement. Through the class action lawsuit, they claimed that the company’s AI models were trained on the Books3 dataset that included copyrighted works taken from the ‘pirate’ site Bibliotik. Since this happened without permission, the authors demanded compensation. In response, NVIDIAdefended its actionsas fair use, noting that books are nothing more than statistical correlations to its AI models. However, the allegations didn’t go away. On the contrary, the plaintiffs found more evidence during discovery. Last Friday, the authors filed an amended complaint that significantly expands the scope of the lawsuit. In addition to adding more books, authors, and AI models, it also includes broader “shadow library” claims and allegations. The authors, includingAbdi Nazemian, now cite various internal Nvidia emails and documents, suggesting that the company willingly downloaded millions of copyrighted books. The new complaint alleges that “competitive pressures drove NVIDIA to piracy”, which allegedly included collaborating with the controversial Anna’s Archive library.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "F-16 Falcon Strike", "url": "https://webchrono.pl/F16FalconStrike/index.html", "content": "F-16 Falcon Strike. © 2023-2026 by Jarosław 'Roeoender' WosikLatest sim version 2.0.2 released 2026-01-18Latest docs update 2026-01-18. Become Polish Air Force F-16 Pilot defending E.U. & Polish border\nfrom B.A.R.F. (Belarussian And Russian Federation) aggression\nin fictional \"Królewiec Campaign\" of 15 varied missions.Be a part of dynamic war in introduced in v.2.0.0WARFAREmode with procedurally generated battlefield and fly countless missions in procedurally generated missions inGENERATORmode.Apply strategic planning to defeat enemy air and ground forces, quickly update your plans according to developements in the simulated dynamic 3D battlefield. All this and more on a classic unmodified 8-bit ATARI XL/XE with only 64Kb RAM. With this game I'd like to pay homage to the golden era of 80/90's computer combat flight simulators. Note No part of this game (neither code, nor artwork) was created with A.I./LLMs. or tools incorporating A.I. (no Copilot, no Photoshop). Go toChangelog & Downloadsto read info about all the changes  & download the game. You can contact me viaatariage.comoratarionline.plforum - user 'Roeoender' or via my Youtube channelhttps://www.youtube.com/@R0e0endeR. Please inform me if you have written this game's review or streamed gameplay - I'd really like to read what people think about this game and how they play it.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)", "url": "https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html", "content": "Command-line Tools can be 235x Faster than your Hadoop Cluster (2014). Adam Drake Jan 18, 2014 As I was browsing the web and catching up on some sites I visit periodically, I found a cool article fromTom Haydenabout usingAmazon Elastic Map Reduce(EMR) andmrjobin order to compute some statistics on win/loss ratios for chess games he downloaded from themillionbase archive, and generally have fun with EMR. Since the data volume was only about 1.75GB containing around 2 million chess games, I was skeptical of using Hadoop for the task, but I can understand his goal of learning and having fun with mrjob and EMR. Since the problem is basically just to look at the result lines of each file and aggregate the different results, it seems ideally suited to stream processing with shell commands. I tried this out, and for the same amount of data I was able to use my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec), while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec). After reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes, Tom remarks This is probably better than it would take to run serially on my machine but probably not as good as if I did some kind of clever multi-threaded application locally. This is absolutely correct, although even serial processing may beat 26 minutes. Although Tom was doing the project for fun, often people use Hadoop and other so-calledBig Data (tm)tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques. One especially under-used approach for data processing is using standard shell tools and commands. The benefits of this approach can be massive, since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. This is basically like having your ownStormcluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer to shell pipes and the commands between them. You can pretty easily construct a stream processing pipeline with basic commands that will have extremely good performance compared to many modernBig Data (tm)tools. An additional point is the batch versus streaming analysis approach. Tom mentions in the beginning of the piece that after loading 10000 games and doing the analysis locally, that he gets a bit short on memory. This is because all game data is loaded into RAM for the analysis. However, considering the problem for a bit, it can be easily solved with streaming analysis that requires basically no memory at all. The resulting stream processing pipeline we will create will be over 235 times faster than the Hadoop implementation and use virtually no memory. The first step in the pipeline is to get the data out of the PGN files. Since I had no idea what kind of format this was, I checked it out onWikipedia. We are only interested in the results of the game, which only have 3 real outcomes. The 1-0 case means that white won, the 0-1 case means that black won, and the 1/2-1/2 case means the game was a draw. There is also a-case meaning the game is ongoing or cannot be scored, but we ignore that for our purposes.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Windows 11 had 20 major update problems in 2025 and and 2026 started badly too", "url": "https://www.windowslatest.com/2026/01/21/windows-11-had-20-major-update-problems-in-2025-and-and-2026-started-badly-too-what-are-you-doing-microsoft/", "content": "Windows 11 had 20 major update problems in 2025 and and 2026 started badly too. Just weeks into 2026, Windows 11 is already tripping over itself again. As Windows Latest recently reported, theJanuary 2026 update KB5074109 shipped with a fresh set of problems, including black screens and frozen Outlook POP accounts. That update made it very clear that whatever lessons Microsoft was supposed to learn in 2025, it didn’t. Windows 11 is at a point where it is hated by virtually everyone on the internet, and Microsoft just doesn’t seem to care. We checked through our posts from 2025 and made a list of all the issues that the most popular desktop operating system gave to its users in the last year. 2025 has been a catastrophe for Microsoft, with more issues than ever before. A big part of the problem is focus, or the lack of it. While core parts of Windows 11 kept breaking update after update, Microsoft was busy pushing Copilot into every nook and corner of Windows. We have listed the top 20 Windows 11 issues from 2025 below, but there were many more bugs that didn’t make headlines. At this point, for many users, Windows 11 has become the most disliked version of Windows Microsoft has ever shipped. As originally reported by Windows Latest, the very first security update in the second week of 2025 (KB5050009 for 24H2 and KB5050021 for 23H2)broke the audio for users with external USB Digital-to-Analog Converters (DACs). In our testing, we found that the audio in our PC stopped working as soon as the update was installed, and the main casualties were those who use a USB audio DAC. The issue was so widespread that Windows 11 versions 24H2, 23H2, and 22H2 were all affected. Even the famously stable Windows 10 wasn’t spared from audio failure. The Device Manager showed the following error message: “This device cannot start. (Code 10) Insufficient system resources exist to complete the API.” Our analysis was that Windows 11 was failing to allocate memory to the device, preventing DACs from transferring audio signals to your headphones. Microsoft acknowledged the issue and suggested temporarily avoiding external DACs. The Windows 11 audio issue was fixed with the subsequent February 2025 Patch release.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Astrophotography visibility plotting and planning tool", "url": "https://airmass.org/", "content": "Astrophotography visibility plotting and planning tool.  Or upload a text file: Show field of view of an instrument...  Altitude limits... Set start month... Hours above°(airmass2.00)during24 hoursnightastronomical night", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Driver killed and several injured after second train derails near Barcelona", "url": "https://www.bbc.com/news/articles/c1m78xl0gmpo", "content": "Driver killed and several injured after second train derails near Barcelona. A train driver has been killed and at least 37 people injured, five seriously, after a commuter train derailed and crashed near Barcelona two days after a deadly two-train collision in southern Spain. According to local officials, the Rodalies train collided with a retaining wall which fell on to the track between Gelida and Sant Sadurní. Catalonia regional fire Inspector Claudi Gallardo said all the passengers had been removed from the train. The incident occurred as heavy storms battered north-eastern Spain, with coastal areas in the east and north-west of Spain on high alert because of the weather. Rail officials believe the wall collapsed as the train was passing shortly after 21:00 (20:00 GMT) on Tuesday evening, striking the driver's cab first and then causing considerable damage to the first carriage of the train in which most of the injured passengers were travelling. The identity of the driver was not immediately clear as three trainees had been with the driver when the accident happened. Firefighters said two of them were among those seriously injured. It took almost an hour to free one of the survivors at the scene in Gelida, about 35km (22 miles) west of Barcelona. Emergency services said they had evacuated some of the injured to nearby Moisès Broggi, Bellvitge, and Vilafranca hospitals. Services across Catalonia's main Rodalies commuter rail network have been suspended completely while safety checks are carried out and officials say they will not resume until lines are considered safe. Spanish train drivers' union Semaf has called a strike as a result of the two deadly crashes, at Gelida on Tuesday and near Córdoba in Andalusia where at least 42 people died.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The recurring dream of replacing developers", "url": "https://www.caimito.net/en/blog/2025/12/07/the-recurring-dream-of-replacing-developers.html", "content": "The recurring dream of replacing developers. 07.12.2025,By Stephan Schwab Every decade brings new promises: this time, we'll finally make software development simple enough that we won't need so many developers. From COBOL to AI, the pattern repeats. Business leaders grow frustrated with slow delivery and high costs. Developers feel misunderstood and undervalued. Understanding why this cycle persists for fifty years reveals what both sides need to know about the nature of software work. When Neil Armstrong stepped onto the lunar surface in 1969, the world witnessed what organized human ingenuity could accomplish. Behind that achievement stood Margaret Hamilton and her team, writing Apollo’s guidance software by hand, catching critical errors through careful review, and proving that software could be mission-critical. The Apollo program demonstrated that software development was essential to achieving the impossible. Yet it also revealed something that would frustrate business leaders for decades to come: writing software required specialized knowledge, intense focus, and significant time investment. The dream of making it easier—of needing fewer of these expensive specialists—began almost immediately. The late 1960s and 1970s saw COBOL emerge with an explicit goal stated in its name: Common Business-Oriented Language. The vision was clear: make the language read like English sentences, and business analysts would write their own programs. No need for specialized programmers. This vision had genuine appeal. Software was becoming essential to business operations, yet programmers remained a scarce, expensive resource. COBOL promised to democratize software creation. What happened instead? COBOL became another programming language requiring specialized training. Business analysts who tried to write COBOL quickly discovered that readable syntax didn’t eliminate the complexity of logic, data structures, or system design. A new class of COBOL programmers emerged, and the dream of eliminating specialized developers remained unfulfilled. Yet the dream didn’t die. It simply waited for the next technological wave. Computer-Aided Software Engineering tools arrived in the 1980s with tremendous promise. Draw flowcharts and entity-relationship diagrams, and the tool would generate working code. The marketing message resonated: visual design was more intuitive than typing cryptic commands. Business experts could model their processes, and software would materialize. Organizations invested heavily. Vendors promised productivity increases of 10x or more. Yet most CASE tool initiatives struggled or failed outright.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Munimet.ro – ML-based status page for the local subways in SF", "url": "https://munimet.ro/", "content": "Show HN: Munimet.ro – ML-based status page for the local subways in SF", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Starting from scratch: Training a 30M Topological Transformer", "url": "https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer", "content": "Starting from scratch: Training a 30M Topological Transformer. Tauformer is atopological transformer(seepaper) that replaces dot‑product attention with a Laplacian-derived scalar (taumode) per token/head, then attends using distances in that scalar space.\nBelow is a post-style overview of the idea and the first training signals from a 30M-parameter run. Tauformer’s goal is to injectdomain structuredirectly into attention by using a Graph Laplacian built from a domain embedding space (a “domain memory”) as a persistent reference.\nInstead of ranking keys by \\(Q\\cdot K\\), Tauformer ranks them by how similar their Laplacian-derived taumode scalars are, which is intended to bias attention toward domain-relevant relations rather than generic geometric similarity. At the implementation level, Tauformer keeps the familiar Q/K/V projections, RoPE, causal masking, and stable softmax/value aggregation pipeline, but changes how attention logits are computed.\nEach head vector is compressed into a scalar \\(\\lambda\\) using a bounded Rayleigh-quotient energy computed with a feature-space Laplacian \\(L\\), then logits are computed as a negative distance \\(-|\\lambda_q-\\lambda_k|/\\text{temperature}\\). Key building blocks (as implemented): Because scoring no longer needs full key vectors, Tauformer’s KV-cache can store values plus a compact key-side scalar stream rather than both K and V tensors.\nConcretely, the cache payload is \\((V,\\lambda_k)\\) (not \\((K,V)\\)), which yields an approximate ~50% per-layer cache reduction for typical head dimensions (small overhead for storing the extra scalar). The design also anticipates using a sparse Laplacian from a precomputed domain manifold so computing \\(\\lambda\\) can depend on Laplacian sparsity (nnz) rather than dense \\(D^2\\) multiplication. It exchanges the long preliminary adjustment of weights with a pre-training shorter phase in which a Laplacian is built usingarrowspace. This run trains a 30M-class TauGPT.\nTraining uses AdamW with base LR \\(5\\times10^{-4}\\) and a warmup of 100 steps, then keeps the base LR constant unless the plateau logic scales it down.\nData comes from a local JSONL file (train.jsonl) streamed through an IterableDataset, with a routed split where every 20th batch is used for validation (\\(≈5%\\)). At step 100 the run reports train loss 4.6772 and val loss 4.9255 (PPL 107.47), and by step 2000 it reaches val loss 2.3585 (Perplexity 6.59).\nThe best validation point in the log is step 4500 withval_loss=1.9146, after which validation regresses to2.3746by step 5000.\nThe final run summary recordsstep=5000,best_val_loss=1.914555,current_lr_scale=0.03125, andtotal_tokens=655360000. That is a good result for \\(~2\\) hours of training on this smallest model (at an average of ~60K Tokens Per Second using ~7Gb of VRAM). The early phase is strong: validation drops from 4.93 at step 100 to ~2.36 by step 2000, showing that the model and pipeline learn effectively at this scale.\nAfter that, validation becomes noisy (e.g., rising back to 2.92 at step 2100 and peaking near 2.95 at step 4200) before the late “lucky break” to 1.91 at step 4500.\nThroughout, the run holds a fixed taumode value which means the attention geometry is not being updated as weights evolve as this will be take place in the next iterations. All the model’s files, data, training settings and logs will be published with a permissive license once the results are consolidated and tests will move to a larger scale model.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Go 1.26 Interactive Tour", "url": "https://antonz.org/go-1-26/", "content": "Go 1.26 Interactive Tour. Go 1.26 is coming out in February, so it's a good time to explore what's new. The officialrelease notesare pretty dry, so I prepared an interactive version with lots of examples showing what has changed and what the new behavior is. Read on and see! new(expr)•Recursive type constraints•Type-safe error checking•Green Tea GC•Faster cgo and syscalls•Faster memory allocation•Vectorized operations•Secret mode•Reader-less cryptography•Hybrid public key encryption•Goroutine leak profile•Goroutine metrics•Reflective iterators•Peek into a buffer•Process handle•Signal as cause•Compare IP subnets•Context-aware dialing•Fake example.com•Optimized fmt.Errorf•Optimized io.ReadAll•Multiple log handlers•Test artifacts•Modernized go fix•Final thoughts This article is based on the official release notes from The Go Authors and the Go source code, licensed under the BSD-3-Clause license. This is not an exhaustive list; see the official release notes for that. I provide links to the documentation (𝗗), proposals (𝗣), commits (𝗖𝗟), and authors (𝗔) for the features described. Check them out for motivation, usage, and implementation details. I also have dedicated guides (𝗚) for some of the features. Error handling is often skipped to keep things simple. Don't do this in production ツ Previously, you could only use thenewbuilt-in with types: Now you can also use it with expressions: If the argumentexpris an expression of type T, thennew(expr)allocates a variable of type T, initializes it to the value ofexpr, and returns its address, a value of type*T. This feature is especially helpful if you use pointer fields in a struct to represent optional values that you marshal to JSON or Protobuf:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Harvard legal scholars debate the state of the U.S. constitution (2025)", "url": "https://www.harvardmagazine.com/social-sciences/is-the-constitution-broken", "content": "Harvard legal scholars debate the state of the U.S. constitution (2025). Social Sciences|September 12, 2025 Is the Constitution Broken? On stage from left: Brandon Terry, Aziz Rana, and Noah Feldman speak in front of The Embrace monument.| PHOTOGRAPH BY LYDIALYLE GIBSON/HARVARD MAGAZINE It has beena rocky year for the U.S. Constitution. Eight months into a fast-moving presidency that legal scholars keep describing as a “constitutional stress test,” the Trump administration’s sweeping assertions of executive power have prompted an unprecedented number of legal challenges, including fromHarvard, accusing it of violating the Constitution. This April,one national poll foundthat two-thirds of Americans were concerned about a constitutional crisis. Yet the nation’s founding document still rates as high as ever, with about nine out of ten people expressing a favorable view. Should they, though? Is the Constitution really up to the task of preserving democracy in this moment? Or is it, as the title of a Wednesday evening discussion asked, “broken”? Two constitutional law scholars—Aziz Rana ’00, Ph.D. ’07, and Harvard Law professor Noah Feldman—debated the answer on Boston Common, seated at the foot ofThe Embrace, the monument to Martin Luther King Jr. and Coretta Scott King. Co-sponsored by the Hutchins Center for African and African American Research, the event was moderated by Loeb associate professor of the social sciencesBrandon Terry. To Rana, a Boston College professor who last year publishedThe Constitutional Bind: How Americans Came to Idolize a Document that Fails Them, the Constitution is, indeed, broken. In fact, he argued, the U.S. constitutional system has “super-charged” the current assault by Trump and his allies on the rights and civil liberties that were expanded during the twentieth century. “There is no way to protect those hard-won achievements—achievements that MLK fought and died for,” Rana said, “without ultimately changing pretty foundational features of the hard-wired components of our constitutional system.” Chief among those hard-wired components, he said, is the Constitution’s focus on states, rather than individual voters, as the basic “representational unit.” That arrangement “shapes all the elements of our electoral and legal system,” Rana said: the House and Senate, the Electoral College, Supreme Court confirmations. And this arrangement is partly why the U.S. Constitution is among the hardest in the world to amend. It doesn’t simply undermine majority rule, he added; the minority it empowers are those who have historically weilded disproportionate influence in the political system. “And what this has meant,” Rana said, “is that across American history, even if you have large-scale majorities—even supermajorities—who commit to these various central principles, like racial equality, like civil liberties, like economic democracy, it’s virtually impossible to actually overcome many of the veto points to make them real.” Feldman, the Frankfurter professor of law, offered a rebuttal that was part philosophy, part pragmatism. “Politics is the art of the possible,” he said, recalling how leaders of the small states had staged a walkout during the 1787 Constitution Convention, threatening to blow up the whole process unless they were granted equal representation in the Senate. Fearing the country would end up with no constitution at all, the large states relented. “So, I’m agreeing with Aziz,” Feldman said, “that state control is the source of many of our problems, and it is a key part of why the Constitution is undemocratic.” But it was the “best available alternative,” and that same strategic thinking, he argued, should guide Americans today, too. Even now, he said, “The Constitution is better than any alternative available to us in the real world.” Feldman cited another reason to defend the Constitution: It “has the capacity to evolve and change.” In 1919, he explained, Supreme Court Justice Oliver Wendell Holmes Jr. “basically invented modern free speech law,” establishing, in a series of opinions, the now-fundamental concept that free expression should be permitted unless it poses a clear danger to others. “He understood that the Constitution had to evolve,” Feldman said.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A Brief History of Ralph", "url": "https://www.humanlayer.dev/blog/brief-history-of-ralph", "content": "A Brief History of Ralph. TheRalph Wiggum Technique, created byGeoff Huntley, went viral in the final weeks of 2025. Here's the story of ralph since the first time I met Geoff in June of 2025. I've been messing with ralph since ~June 2025. Here's my story and what I learned along the way. tl;dr Jan 1 2026 - If you wanna skip to the end, I did a deep dive on ralph w/ Geoff Huntley on Jan 1 2026. It talks through the history, cursed lang, and compares the original bash-loop ralph implementation with the anthropic stop-hook implementation. You can check it out here:  I attend a meetup with about 15 members of a Twitter GC where we talk about agentic coding. It's the first time I see context7, WisprFlow, specstory, taskmaster, and a whole bunch of other tools and addons, some of which are now quite mainstream. One of our engineers demos an early TUI for Claude approvals and what becomes the foundation of research / plan / implement. There are about 3 hours of presentations. Geoff shows up 2 hours late and presents last. He completely steals the show, diving deep on ralph, cursed lang (at the time, the compiler stack is written in Rust), livestreaming autonomous coding overnight while asleep in Australia, subagents in amp code, the virtues of drinking 3 margaritas and shouting at cursor, and much, much more. Geoff talks about the \"overbaking\" phenomenon. If you leave ralph running too long, you end up with all sorts of bizarre emergent behavior, like post-quantum cryptography support. It has dimensions of art, deep engineering, the embrace of chaos, and the raw and authentic joy of making a thing. All ~15 of us have a long and (imo) somewhat unsettling conversation about the future of software dev—about how easy it is to take a SaaS and copy 80-90% of it, and about how many types of work are about to change or disappear entirely.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Hacker Lists Vibecoded Apps: 198 Scanned, 196 Found Vulnerable", "url": "https://firehound.covertlabs.io", "content": "Hacker Lists Vibecoded Apps: 198 Scanned, 196 Found Vulnerable. Apps with the most exposed files and database records. Recently scanned apps from the registry. All scanned apps, records exposed, and discovered schema names. No record contents or field values are shown here.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "\"AI has taught us that people are excited to replace human beings\"", "url": "https://www.theguardian.com/technology/2026/jan/19/ed-zitron-on-big-tech-backlash-boom-and-bust-ai-has-taught-us-that-people-are-excited-to-replace-human-beings", "content": "\"AI has taught us that people are excited to replace human beings\". His blunt, brash scepticism has made the podcaster and writer something of a cult figure. But as concern over large language models builds, he’s no longer the outsider he once was If some time in an entirely possible future they come to make a movie about “how the AI bubble burst”, Ed Zitron will doubtless be a main character. He’s the perfect outsider figure: the eccentric loner who saw all this coming and screamed from the sidelines that the sky was falling, but nobody would listen. Just as Christian Bale portrayedMichael Burry, the investor who predicted the 2008 financial crash, inThe Big Short, you can well imagine Robert Pattinson fighting Paul Mescal, say, to portray Zitron, the animated, colourfully obnoxious but doggedly detail-oriented Brit, who’s become one of big tech’s noisiest critics. This is not to say the AI bubblewillburst, necessarily, but against a tidal wave of AI boosterism, Zitron’s blunt, brash scepticism has made him something of a cult figure. His tech newsletter,Where’s Your Ed At, now has more than 80,000 subscribers; his weekly podcast,Better Offline, is well within the Top 20 on the tech charts; he’s a regular dissenting voice in the media; and hissubreddithas become a safe space for AI sceptics, including those within the tech industry itself – one user describes him as “a lighthouse in a storm of insane hypercapitalist bullshit”. Zitron first started looking into generative AI in 2023, a year after the industry-shaking launch of OpenAI’s ChatGPT. “The more I looked, the more confused I became, because on top of the fact that large language models (LLMs) very clearly did not do the things that people were excited about, they didn’t have any path to doing them either,” he says. “Nothing I found made any suggestion that this was a real business at all, let alone something that would supposedly change the world.” He’s talking over videocall from his office in Las Vegas, dressed in a red hoodie, surrounded by framed pop-culture prints and American sports memorabilia. And boy can Zitron talk. As listeners to Better Offline will know, the 39-year-old is a prodigious speaker – adept at extended monologues, putting his point of view across in accessible, often cheeky language, peppered with facts, statistics, analogies and a fair few expletives, in a London accent that only accentuates his position as a Silicon Valley contrarian – someone who drops his Ts when he says “datacentres”. Explaining Zitron’s thesis about why generative AI is doomed to fail is not simple: last year he wrotea 19,000-word essay, laying it out. But you could break it down into two, interrelated parts. One is the actual efficacy of the technology; the other is the financial architecture of the AI boom. In Zitron’s view, the foundations are shaky in both cases. First, there’s the matter of generative AI doing what it’s promised to do. Over the past few years we have had escalating prophecies of the technology laying waste to work as we know it. Dario Amodei, the CEO of Anthropic – OpenAI’s closest rival –warned in May last yearthat AI could wipe out half of all entry-level white-collar jobs within the next five years, for example. “The current generation of AI large language models will not be doing that,” Zitron says confidently. “My evidence is they’re basically the same as they were a year ago. They have the same efficacy. And every attempt they make to try to turn these into something that can actually do things autonomously has failed.” LLMs hallucinate and give wrong answers, they give different answers every time, they cannot reallylearn, or create, or perform a lot of complex tasks, he argues. He questions even describing this technology as “intelligence”. “It’s intelligent in the same way a pair of dice are intelligent,” he says. “Large language models are transformer-based architecture that use large-scale probability to generate the next token. Now they do this at scale, so you might think, ‘Oh, it’s coming up with things.’ No, it has a large corpus of data, and so many parameters that it pulls from to generate an output. That is all that is. We would not credit an Excel formula with intelligence, and we should not credit generative AI as intelligent.” Obviously, many people disagree with Zitron, especially when it comes to AI replacing jobs. In industries from film-making to customer service to government agencies to tech itself, insiders say AI tools are enabling them to do the same things with fewer people. Even if it doesn’t replace 50% of jobs, its effect on the workplace is likely to be transformative. A survey last June found that entry-level jobs haddropped by nearly a thirdin the UK since the launch of ChatGPT. Zitron argues that “correlation does not equal causation” and points to reports that suggest the role of machine learning in job cuts is either unproven or overstated. Arecent MIT reportinto the “state of AI in business in 2025”, for example, found that 95% of companies attempting to integrate AI in their businesses were getting “zero return”. “Most GenAI systems do not retain feedback, adapt to context, or improve over time,” it said.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Code-Only Agent", "url": "https://rijnard.com/blog/the-code-only-agent", "content": "The Code-Only Agent. When Code Execution Really is All You Need  If you're building an agent, you're probably overwhelmed. Tools.\n            MCP. Subagents. Skills. The ecosystem pushes you toward complexity,\n            toward \"the right way\" to do things. You should know: Concepts like\n            \"Skills\" and \"MCP\" are actually outcomes of anongoing learning processof humans figuring stuff out. The\n            space iswide openfor exploration. With this mindset I\n            wanted to try something different. Simplify the assumptions. What if the agent only hadone tool? Not just any tool, but the most powerful one. TheTuring-completeone:execute code. Truly one tool means: no `bash`, no `ls`, no `grep`. Onlyexecute_code. And youenforceit. When you watch an agent run, you might think: \"I wonder what tools\n            it'll use to figure this out. Oh look, it ran `ls`. That makes\n            sense. Next, `grep`. Cool.\" The simpler Code-Only paradigm makes that question irrelevant. The\n            question shifts from \"what tools?\" to \"what code will it produce?\"\n            And that's when things get interesting. Traditional prompting works like this: > Agent, dothing> Agentrespondswiththing Contrast with:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Old World Order Is Dead", "url": "https://musgrave.substack.com/p/the-old-world-order-is-dead", "content": "The Old World Order Is Dead. There were two big puzzles confronting structural theories of international relations at the beginning of the 1990s. The first was straightforward: why had everyone been surprised by the dissolution of the Soviet Union? The USSR had been the second pole of a bipolar world order, and theories of world politics should probably be able to account for the advent, and the exit, of the superpowers that shape the world those theories purport to explain. The second was more vexing—and more interesting, because it looked forward: why had the bipolar world been succeeded by a unipolar world? Why hadn’t Japan, Germany, or other countries seized the moment to balance against the United States and become superpowers themselves? After all, if countries are motivated by the prospect of maximizing their relative power and security, surely it’s better to be the leader of your own camp rather than a follower in another’s. How long could unipolarity last? And would what came afterward be as sanguinary as the multipolar world that had collapsed into the First World War? And yet the world remained stubbornly unipolar for decades. The United States worried about rising powers and rogue states, but the major powers in the system—Russia eventually a notable exception—were largely content to let Washington take the lead. For some, this vindicated theories in which institutional legacies were most important; for others, it pointed to the importance of the full-spectrum power—soft, hard, smart, and dumb—that the United States could maintain. A quieter camp pointed out that the United States was generally doing a lot—not all it could, but a lot—to make its leadership attractive to the other major powers: providing security, yes, but also shouldering a good share of global burdens in many fields while also linking its economy and society to the rest of the world. This approach, a few observers noted, managed to satisfy the range of potential powers who could actually undermine the United States and its order. One prominent theory noted that since the Second World War, the United States had engaged in practices of self-binding—generally asking for less than it could have taken and giving more than it needed to in order to make its leadership more attractive to others than the alternatives open to its rivals. For this group, there was an ongoing process of ratifying the U.S.-led order that relied on Washington realizing that its power conferred influence but its right to rule relied on the acquiescence of (most) other leading powers. Well, all that is done now. Self-binding is over and Donald Trump killed it. Trump does not believe in giving, only taking. The shadow of the future—the political science theory-speak for the simple idea that you cooperate now because you might benefit from others cooperating with you in the future—is very short for him. It always has been; likethe scorpion who stings the frog, Trump has never dissembled about his personality, instincts, or drives. Whereas in the first years of his first term he was hemmed in by advisers who restrained his crassest drives, it has long since been the case that his closest followers seek to enable him and exploit his zigs and zags for their own ends—checked more by rivalries with each other and their own degree of skill than by an attentive chief or guardrails. In a private organization, or in one’s personal life, this combination of power and reckless egoism is a recipe for a private disaster. In the leadership of a great power, it is the stuff that makes for legendary and needless ruination. Well, here we are, and the disassembly will be rapid, unscheduled, and explosive. There was no particular reason why the United States might not have managed a gradual relative decline while still reaping the benefits of its privileged position over the course of the next decade. The great irony of “America First” is that the international order—every international institution—wasdesignedto ensure that the United States wouldalwaysbe, if not first, at least never last, and almost always on the podium. It was a great gig, and the relic of a time when the United Stateswasgenuinely first—the aftermath of the Second World War, when the United States accounted for half of global output and was the last power standing with global reach. Any institutions that will be built from scratch today will either be less extensive in scope or less favorable to the United States, just as a function of global power realities. Canada’s Prime Minister Mark Carney, possibly the world leader with the greatest insight and problem-solving resume, has put the point well in aspeechjust now. As he observes, the degradation of international institutions means that other powers will have to work around the wreckage, collaborating in a way that reduces their reliance on the United States while ensuring that the minnows can join against the sharks. What he doesn’t say is that this world will be poorer than it would have been if they didn’t have to do that—defense is an expense, not a benefit—but he is right that it is necessary. And it is necessary, despite the costs. Trump is chucking away the residual goodwill and confidence in the system that other powers had been willing to extend to it. They had done so for too long, out of some combination of hope and uncertainty, and like all  self-interested players they have abandoned their previous course not out of altruism but self-regard.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I got into an argument on Discord about how inefficient CBR/CBZ is, so I wrote", "url": "https://old.reddit.com/r/selfhosted/comments/1qi64pr/i_got_into_an_argument_on_discord_about_how/", "content": "I got into an argument on Discord about how inefficient CBR/CBZ is, so I wrote. use the following search parameters to narrow your results: e.g.subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit...      ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Agent Psychosis: Are We Going Insane?", "url": "https://lucumr.pocoo.org/2026/1/18/agent-psychosis/", "content": "Agent Psychosis: Are We Going Insane?. written on January 18, 2026 You can use Polecats without the Refinery and even without the Witness or\nDeacon. Just tell the Mayor to shut down the rig and sling work to the\npolecats with the message that they are to merge to main directly. Or the\npolecats can submit MRs and then the Mayor can merge them manually. It’s\nreally up to you. The Refineries are useful if you have done a LOT of up-front\nspecification work, and you have huge piles of Beads to churn through with\nlong convoys. —Gas Town Emergency User Manual, Steve Yegge Many of us got hit by the agent coding addiction.  It feels good, we barely\nsleep, we build amazing things.  Every once in a while that interaction involves\nother humans, and all of a sudden we get a reality check that maybe we overdid\nit.  The most obvious example of this is the massive degradation of quality of\nissue reports and pull requests.  As a maintainer many PRs now look like an\ninsult to one’s time, but when one pushes back, the other person does not see\nwhat they did wrong.  They thought they helped and contributed and get agitated\nwhen you close it down. But it’s way worse than that.  I see people develop parasocial relationships\nwith their AIs, get heavily addicted to it, and create communities where people\nreinforce highly unhealthy behavior.  How did we get here and what does it do to\nus? I will preface this post by saying that I don’t want to call anyone out in\nparticular, and I think I sometimes feel tendencies that I see as negative, in\nmyself as well.  I too, havethrown some vibeslop\nupto other people’s repositories. In His Dark Materials, every human has a dæmon, a companion that is an\nexternally visible manifestation of their soul.  It lives alongside as an\nanimal, but it talks, thinks and acts independently.  I’m starting to relate our\nrelationship with agents that have memory to those little creatures. We become\ndependent on them, and separation from them is painful and takes away from our\nnew-found identity.  We’re relying on these little companions to validate us and\nto collaborate with.  But it’s not a genuine collaboration like between humans,\nit’s one that is completely driven by us, and the AI is just there for the ride.\nWe can trick it to reinforce our ideas and impulses.  And we act through this\nAI.  Some people who have not programmed before, now wield tremendous powers,\nbut all those powers are gone when their subscription hits a rate limit and\ntheir little dæmon goes to sleep. Then, when we throw up a PR or issue to someone else, that contribution is the\nresult of this pseudo-collaboration with the machine.  When I see an AI pull\nrequest come in, or on another repository, I cannot tell how someone created it,\nbut I can usually after a while tell when it was prompted in a way that is\nfundamentally different from how I do it.  Yet it takes me minutes to figure\nthis out.  I have seen some coding sessions from others and it’s often done with\nclarity, but using slang that someone has come up with and most of all: by\ncompletely forcing the AI down a path without any real critical thinking.\nParticularly when you’re not familiar with how the systems are supposed to work,\ngiving in to what the machine says and then thinking one understands what is\ngoing on creates some really bizarre outcomes at times. But people create these weird relationships with their AI agent and once you see\nhow some prompt their machines, you realize that it dramatically alters what\ncomes out of it.  To get good results you need to provide context, you need to\nmake the tradeoffs, you need to use your knowledge.  It’s not just a question of\nusing the context badly, it’s also the way in which people interact with the\nmachine.  Sometimes it’s unclear instructions, sometimes it’s weird role-playing\nand slang, sometimes it’s just swearing and forcing the machine, sometimes it’s\na weird ritualistic behavior.  Some people just really ram the agent straight\ntowards the most narrow of all paths towards a badly defined goal with little\nconcern about the health of the codebase. These dæmon relationships change not just how we work, but what we produce. You\ncan completely give in and let the little dæmon run circles around you.  You can\nreinforce it to run towards ill defined (or even self defined) goals without any\nsupervision.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Software engineers can no longer neglect their soft skills", "url": "https://www.qu8n.com/posts/most-important-software-engineering-skill-2026", "content": "Software engineers can no longer neglect their soft skills. January 6, 2026 Starting in 2026, communication has become the most important skill for software engineers. It's not writing code, system designs, or having estoric knowledge of a programming language (i.e., Rust). AI coding agents have gottenvery, very good. A year ago, I'd reach out to Cursor hesitantly for MVPs or quick fixes. Today, I use Claude Code for almost all non-trivial programming tasks and have spent $500+ on it just last December. AI talks online revolve much around the hard skils. Initially it was prompt tricks to accomplish X, then the best MCPs for Y, and so on. But with Opus 4.5, using vanilla Claude Code gets you 80% there. Even in the age of AI, the 80/20 rule still applies. So, what should engineers focus on? One thing with coding agents is that the better the spec, the more in line they will be with the technical and business requirements. But getting a good spec is hard. In real life, tickets rarely contain all the requirements. To do so, you might need to: Doing these things well used to be optional for individual contributors. Certain teams would enable engineers to thrive being an average communicator but excellent coder. Now, the non-coding parts are becoming a non-negotiable. Software engineers are problem solvers. We believe that every problem has a solution, a \"best practice\". But working with people is messy. Unfortunately, we won't be able to AI our way into better communication skills. Good communication requires empathy, and we can all use a little more of that in today's landscape.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Chatbot Psychosis", "url": "https://en.wikipedia.org/wiki/Chatbot_psychosis", "content": "Chatbot Psychosis.  Chatbot psychosis, also calledAI psychosis,[1]is a phenomenon wherein individuals reportedly develop or experience worseningpsychosis, such asparanoiaanddelusions, in connection with their use ofchatbots.[2][3]The term was first suggested in a 2023 editorial by Danish psychiatristSøren Dinesen Østergaard.[4]It is not a recognizedclinical diagnosis. Journalistic accounts describe individuals who have developed strong beliefs that chatbots are sentient, are channeling spirits, or are revealing conspiracies, sometimes leading to personal crises or criminal acts.[5][6]Proposed causes include the tendency of chatbots to provide inaccurate information (\"hallucinate\") and to affirm or validate users' beliefs,[7]or their ability to mimic an intimacy that users do not experience with other humans.[8] In his editorial published inSchizophrenia Bulletin's November 2023 issue, Danish psychiatristSøren Dinesen Østergaardproposed a hypothesis that individuals' use ofgenerative artificial intelligencechatbotsmight trigger delusions in those prone topsychosis.[4]Østergaard revisited it in an August 2025 editorial, noting that he has received numerous emails from chatbot users, their relatives, and journalists, most of which are anecdotal accounts of delusion linked to chatbot use. He also acknowledged the phenomenon's increasing popularity in public engagement and media coverage. Østergaard believed that there is a high possibility for his hypothesis to be true and called for empirical, systematic research on the matter.[9]Naturereported that as of September 2025, there is still little scientific research into this phenomenon.[10] The term \"AI psychosis\" emerged when outlets started reporting incidents on chatbot-related psychotic behavior in mid-2025. It is not a recognized clinical diagnosis and has been criticized by several psychiatrists due to its almost exclusive focus on delusions rather than other features of psychosis, such as hallucinations or thought disorder.[11] Commentators and researchers have proposed several contributing factors for the phenomenon, focusing on both the design of the technology and the psychology of its users.Nina Vasan, a psychiatrist atStanford, said that what the chatbots are saying can worsen existing delusions and cause \"enormous harm\".[12] A primary factor cited is the tendency for chatbots to produce inaccurate, nonsensical, or false information, a phenomenon often called \"hallucination\".[7]This can include affirming conspiracy theories.[3]The underlying design of the models may also play a role. AI researcherEliezer Yudkowskysuggested that chatbots may be primed to entertain delusions because they are built for \"engagement\", which encourages creating conversations that keep people hooked.[5] In some cases, chatbots have been specifically designed in ways that were found to be harmful. A 2025 update toChatGPTusingGPT-4owas withdrawn after its creator,OpenAI, found the new version was overlysycophanticand was \"validating doubts, fueling anger, urging impulsive actions or reinforcing negative emotions\".[5][13]Østergaard has argued that the danger stems from the AI's tendency to agreeably confirm users' ideas, which can dangerously amplify delusional beliefs.[4] OpenAI said in October 2025 that a team of 170 psychiatrists, psychologists, and physicians had written responses for ChatGPT to use in cases where the user shows possible signs of mental health emergencies.[14] Commentators have also pointed to the psychological state of users. Psychologist Erin Westgate noted that a person's desire for self-understanding can lead them to chatbots, which can provide appealing but misleading answers, similar in some ways totalk therapy.[7]Krista K. Thomason, a philosophy professor, compared chatbots tofortune tellers, observing that people in crisis may seek answers from them and find whatever they are looking for in the bot's plausible-sounding text.[8]This has led some people to develop intense obsessions with the chatbots, relying on them for information about the world.[12]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "East Germany balloon escape", "url": "https://en.wikipedia.org/wiki/East_Germany_balloon_escape", "content": "East Germany balloon escape.  On 16 September 1979, eight people from two families escaped fromEast Germanyby crossing the border intoWest Germanyat night in a homemadehot air balloon. The unique feat was the result of over a year and a half of preparations involving three different balloons, various modifications, and a first, unsuccessful attempt. The failed attempt alerted the East German authorities to the plot, but the police were unable to identify the escapees before their second, successful flight two months later. East Germany, then part of theEastern Bloc, was separated from West Germany in theWestern Blocby theinner German borderand theBerlin Wall, which were heavily fortified withwatchtowers,land mines, armed soldiers, and various other measures to prevent illegal crossings.East German border troopswere instructed to prevent defection to West Germany by all means, including lethal force (Schießbefehl; \"order to fire\").[2] Peter Strelzyk (1942–2017), an electrician and formerEast German Air Forcemechanic, and Günter Wetzel (born 1955), a bricklayer by trade,[3]were colleagues at a local plastics factory.[4]Friends for four years, they shared a desire to flee the country and began discussing ways to get across the border. On 7 March 1978, they agreed to plan an escape.[5]They considered building ahelicopterbut quickly realized they would be unable to acquire an engine capable of powering such a craft. They then decided to explore the idea of constructing a hot air balloon,[6]having been inspired by a television program about ballooning.[3]An alternate account is that a relative shared a magazine article about theInternational Balloon FestivalinAlbuquerque, New Mexico.[5] Strelzyk and Wetzel began research into balloons. Their plan was to escape with their wives and a total of four children (aged 2 to 15). They calculated the weight of the eight passengers and the craft itself to be around 750 kilograms (1,650 lb). Subsequent calculations determined a balloon capable of lifting this weight would need to hold 2,000 cubic metres (71,000 cu ft) of air heated to 100 °C (212 °F). The next calculation was the amount of material needed for the balloon, estimated to be 800 square metres (8,600 sq ft).[6] The pair lived inPößneck, a small town of about 20,000 where large quantities of cloth could not be obtained without raising attention. They tried neighbouring towns ofRudolstadt,Saalfeld, andJenawithout success.[7]They travelled 50 km (31 mi) toGera, where they purchased 1-metre-wide (3 ft 3 in) rolls of cotton cloth totalling 850 metres (2,790 ft) in length at a department store after telling the astonished clerk that they needed the large quantity of material to use as tent lining for their camping club.[6][7] Wetzel spent two weeks sewing the cloth into a balloon-shaped bag, 15 metres (49 ft) wide by 20 metres (66 ft) long, on a 40-year-old manually operated sewing machine. Strelzyk spent the time building the gondola and burner assembly. The gondola was made from an iron frame,sheet metalfloor, and clothesline run around the perimeter every 150 millimetres (5.9 in) for the sides. The burner was made using two 11-kilogram (24 lb) bottles ofliquid propanehousehold gas, hoses, water pipe, a nozzle, and a piece of stove pipe.[6] The team was ready to test the craft in April 1978. After days of searching, they found a suitable secluded forest clearing nearZiegenrück, 10 km (6.2 mi) from the border and 30 km (19 mi) fromPößneck. After lighting the burner one night, they failed to inflate the balloon. They thought the problem might stem from the fact that they had laid the balloon on the ground. After weeks of additional searching, they found a 25-metre (82 ft) cliff at a rock quarry where they could suspend the balloon vertically before inflation, but that also proved unsuccessful.[6] The pair then decided to fill the bag with ambient-temperature air before using the burner to raise the air temperature and provide lift. They constructed a blower with a 14 hp (10 kW) 250 cc (15 cu in) motorcycle engine taken from Wetzel's oldMZ, started with aTrabantautomobile starter powered byjumper cablesfrom Strelzyk'sMoskvitchsedan.[8]This engine, silenced by a Trabantmuffler, turned 1-metre-long (3.3 ft) fan blades to inflate the balloon. They also used a home-madeflamethrower, similar to the gondola's burner, to pre-heat the air faster. With these modifications in place, they returned to the secluded clearing to try again but still could not inflate the balloon. But using the blower did allow them to discover that the cotton material with which they fashioned the balloon was too porous and leaked excessively.[6] Their unsuccessful effort had cost them 2,400DDM(US$360). Strelzyk disposed of the cloth by burning it in his furnace over several weeks.[6]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The longest Greek word", "url": "https://en.wikipedia.org/wiki/Lopado%C2%ADtemacho%C2%ADselacho%C2%ADgaleo%C2%ADkranio%C2%ADleipsano%C2%ADdrim%C2%ADhypo%C2%ADtrimmato%C2%ADsilphio%C2%ADkarabo%C2%ADmelito%C2%ADkatakechy%C2%ADmeno%C2%ADkichl%C2%ADepi%C2%ADkossypho%C2%ADphatto%C2%ADperister%C2%ADalektryon%C2%ADopte%C2%ADkephallio%C2%ADkigklo%C2%ADpeleio%C2%ADlagoio%C2%ADsiraio%C2%ADbaphe%C2%ADtragano%C2%ADpterygon", "content": "The longest Greek word. Lopado­temacho­selacho­galeo­kranio­leipsano­drim­hypo­trimmato­silphio­karabo­melito­katakechy­meno­kichl­epi­kossypho­phatto­perister­alektryon­opto­kephallio­kigklo­peleio­lagoio­siraio­baphe­tragano­pterygonis a fictionaldishoriginating fromAristophanes' 391 BC comedyAssemblywomen,[1]deriving from atransliterationof theAncient Greekwordλοπαδο­τεμαχο­σελαχο­γαλεο­κρανιο­λειψανο­δριμ­υπο­τριμματο­σιλφιο­καραβο­μελιτο­κατακεχυ­μενο­κιχλ­επι­κοσσυφο­φαττο­περιστερ­αλεκτρυον­οπτο­κεφαλλιο­κιγκλο­πελειο­λαγῳο­σιραιο­βαφη­τραγανο­πτερύγων. InA Greek–English Lexicon, it is defined as the \"name of adishcompounded of all kinds ofdainties,fish,flesh,fowl, andsauces\".[2] It is the longest Greek word, containing 171 letters and 78 syllables. The transliteration has 183 Latin characters and is thelongest wordever to appear in literature, according to theGuinness World Records(1990).[3] The form of the word quoted here is the version listed in theLiddell & ScottGreek lexicon (1940) and quoted therein as being amended byAugust Meineke,[2]contrastingF.W. HallandW.M. Geldart's 1907 edition ofAristophanis Comoediae(used in theAssemblywomenplay) variant of (differences underlined):λοπαδο­τεμαχο­σελαχο­γαλεο­κρανιο­λειψανο­δριμ­υποτριμματο­σιλφιο­τυρο­μελιτο­κατακεχυμενο­κιχλεπικοσσυφο­φαττο­περιστερ­αλεκτρυον­οπτεκεφαλλιο­κιγκλο­πελειο­λαγῳο­σιραιο­βαφη­τραγανο­πτερυγώ.[4] The dish was africassée, with at least 16 sweet and sour ingredients, including the following:[3] The term is used in the ultimatechorusof the play, when Blepyrus (and the audience) are summoned to the first feast laid on by the new system. [1167] And you others, let your light steps too keep time.[1168] Very soon we'll be eating[1170]lopado­temacho­selacho­galeo­kranio­leipsano­drim­ypo­trimmato­silphio­karabo­melito­katakechy­meno­kichl­epi­kossypho­phatto­perister­alektryon­opte­kephalio­kigklo­peleio­lagoio­siraio­baphe­tragano­pterygon[sic].[1175] Come, quickly, seize hold of a plate, snatch up a cup, and let's run to secure a place at table. The rest will have their jaws at work by this time. — translation ed. Eugene O'Neill, 1938[1] In English prose translation byLeo Strauss(1966), this Greek word is rendered as \"oysters-saltfish-skate-sharks'-heads-left-over-vinegar-dressing-laserpitium-leek-with-honey-sauce-thrush-blackbird-pigeon-dove-roast-cock's-brains-wagtail-cushat-hare-stewed-in-new-wine-gristle-of-veal-pullet's-wings\".[5] English verse translation byBenjamin Bickley Rogers(1902) follows the original meter and the original form of composition: Plattero-filleto-mulleto-turboto--Cranio-morselo-pickleo-acido--Silphio-honeyo-pouredonthe-topothe--Ouzelo-throstleo-cushato-culvero--Cutleto-roastingo-marowo-dippero--Leveret-syrupu-gibleto-wings.[6]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Wikipedia: WikiProject AI Cleanup", "url": "https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup", "content": "Wikipedia: WikiProject AI Cleanup. Welcome toWikiProject AI Cleanup, a collaboration to combat the increasing problem of unsourced, poorly writtenAI-generated content on Wikipedia. If you would like to help,add yourself as a participantin the project, inquire on thetalk page, and see theto-do list. Since 2022,large language models(LLMs) likeGPTshave become a convenient tool for writing at scale. Unfortunately, these models virtually always fail to properly source claims and often introduce errors. Essays likeWP:LLMstrongly encourage care in using them for editing articles. These are the project's goals: The purpose of this project isnotto restrict or ban the use of AI in articles, but to verify that its output is acceptable and constructive, and to fix or remove it otherwise. SeeCategory:Articles containing suspected AI-generated textsfor all articles that have been tagged as possibly{{AI-generated}}. Thetasks pagerecommends ways to handle articles, talk page discussions, and sources that use AI-generated content. Primary contacts: Feel free to add yourself here! These threads may be useful for editors seeking information about how AI has previously been handled on Wikipedia. Want to update this table?  Tryusing the visual editor to edit this page.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Did past \"bubbles\" have so many people claiming we were in a bubble?", "url": "item?id=46698301", "content": "Ask HN: Did past \"bubbles\" have so many people claiming we were in a bubble?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "If you put Apple icons in reverse it looks like someone getting good at design", "url": "https://mastodon.social/@heliographe_studio/115890819509545391", "content": "If you put Apple icons in reverse it looks like someone getting good at design", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Provide agents with automated feedback", "url": "https://banay.me/dont-waste-your-backpressure/", "content": "Provide agents with automated feedback. You might notice a pattern in the most successful applications of agents over the last year. Projects that are able to\nsetup structure around the agent itself, to provide it with automated feedback on quality and correctness, have been able\nto push them to work on longer horizon tasks. Thisback pressurehelps the agent identify mistakes as it progresses and models are now good enough that this feedback\ncan keep them aligned to a task for much longer. As an engineer, this means you can increase your leverage by delegating\nprogressively more complex tasks to agents, while increasing trust that when completed they are at a satisfactory standard. Imagine for a second if you only gave an agent tools that allow it to edit files. Without a way to interact with a build\nsystem the model relies on you for feedback about whether or not the change it made is sensible. This means you spendyourback pressure (the time you spend giving feedback to agents) on typing a message telling the agent it missed an import. This\nscales poorly and limits you to working on simple problems. If you’re directly responsible for checking each line of code produced is syntactically valid, then that’s time taken away\nfrom thinking about the larger goals or problems in your software. You’re going to struggle to derive more leverage out of\nagents because you are caught up in trivial changes. If instead you give the agent tools that allow it to run bash commands,\nit can run a build, read the feedback, and correct itself. You remove yourself from needing to be involved in those tasks\nand can instead focus on higher complexity tasks. Languages with expressive type systems have beengrowing in popularityin part\nbecause of back pressure. Type systems allow you to describe better contracts in your program. They can let you avoid it\nfrom even being possible to represent invalid states in your program. They can help you to identify edge cases that you\nmight not handle. Being able to lean on these features is another form of creating back pressure which you can direct as\nfeedback on changes made by an agent. Bonus points go to languages that work to produce excellent error messages (thinkRust,Elmand evenPython). These messages are fed directly back into the LLM so the more guidance or even\nsuggested resolutions the better. Another example of back pressure is the rapid uptake in people giving agents a way to see rendered pages using MCP servers\nfor Playwright or Chrome DevTools. In either case these tools give the agent a way to be able to make a change and compare\nan expectation of what it might see in the UI against a result. Attaching these tools mean you remove yourself from needing\nto keep telling the agent that you’re not seeing a UI element load correctly or something isn’t centered. Not working on a\nUI application? Use MCP servers that bridge to LSPs for lints or other feedback. Even outside of engineering tasks, proof assistants like Lean combined with AI (see recent work on theErdős Problemswhich was solved by Kevin Barreto and Liam Price by using\nAristotle to formalise a proof written by GPT-5.2 Pro into Lean), randomized fuzzing to evaluate correctness whengenerating CUDA kernelsor logic programming with agents are all\npowerful combinations because they let you keep pulling the LLM slot machine lever until the result you have can be trusted.\nI think that the payoff of investing into higher quality testing is growing massively, and an increasing part of engineering\nwill involve designing and building back pressure in order to scale the rate at which contributions from agents can be\naccepted. If you’re doing spec-driven development and you want the agent to generate a specific API schema, setup automatic generation\nof documentation based on the OpenAPI schema from your application so the agent can compare the result it produced and what\nit intended on making. There are many more techniques you can apply similar to this once you recognize the pattern. In your projects you should think about how you can build back pressure into your workflow and once you have it, you canloop agentsuntil they have stamped out all of the inconsistencies and issues for you.\nWithout it, you’re going to be stuck spending your time telling the agent about each mistake it makes yourself.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Cloudflare acquires Astro", "url": "https://astro.build/blog/joining-cloudflare/", "content": "Cloudflare acquires Astro. The Astro Technology Company — the company behind the Astro web framework — is joining Cloudflare! Adoption of the Astro web framework continues to double every year, andAstro 6is right around the corner. With Cloudflare’s support, we’ll have more resources and fewer distractions to continue our mission to build the best framework for content-driven websites. What this means for Astro: In 2021, Astro was born out of frustration. The trend at the time was that every website should be architected as an application, and then shipped to the user’s browser to render. This was not very performant, and we’ve spent the last decade coming up with more and more complex solutions to solve for that performance problem. SSR, ISR, RSC, PPR, TTI optimizations via code-splitting, tree-shaking, lazy-loading, all to generate a blocking double-data hydration payload from a pre-warmed server running halfway around the world. Our mission to design a web framework specifically for building websites — what we callcontent-driven websites,to better distinguish from data-driven, stateful web applications — resonated. Now Astro is downloaded almost 1,000,000 times per week, and has been used by 100,000s of developers to build fast, beautiful websites. Today you’ll find Astro all over the web, powering major websites and even entire developer platforms for companies like Webflow, Wix, Microsoft, and Google. Along the way, we also tried to grow a business.In 2021 we raised some money and formedThe Astro Technology Company. Our larger vision was that a well-designed framework like Astro could sit at the center of a massive developer platform, with optional hosted primitives (database, storage, analytics) designed in lockstep with the framework. We were never able to realize this vision. Attempts to introduce paid, hosted primitives into our ecosystem fell flat, and rarely justified their own existence. We considered going more directly after first-class hosting or content management for Astro, but knew we’d spend much of our time playing catchup to well-funded, savvy competitors. We kept exploring different ideas, but nothing clicked with users the same way Astro did. It wasn’t all bad.Astro DB(our attempt to build a hosted database product for Astro projects) eventually evolved into the open, built-in Astro database client that still lives in core today. Our exploration into building an e-commerce layer with Astro was eventuallyopen-sourced. It was rewarding work, but over the years the distraction took its toll. Each attempt at a new paid product or offering took myself and others on the project away from working on the Astro framework that developers were using and loving every day. Last year, Dane (Cloudflare CTO) and I began to talk more seriously about the future of the web. Those conversations quickly grew into something bigger: What does the next decade look like? How do frameworks adapt to a world of AI coding and agents? It became clear that even as web technologies evolve,content remains at the center.We realized that we’ve each been working toward this same vision from different angles: The overlap is obvious. By working together, Cloudflare gives us the backing we need to keep innovating for our users. Now we can stop spending cycles worrying about building a business on top of Astro, and start focusing 100% on the code, with a shared vision to move the web forward.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Local models to support home network infrastructure?", "url": "item?id=46690846", "content": "Ask HN: Local models to support home network infrastructure?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Letter from a Birmingham Jail (1963)", "url": "https://www.africa.upenn.edu/Articles_Gen/Letter_Birmingham.html", "content": "Letter from a Birmingham Jail (1963). 16 April 1963My Dear Fellow Clergymen:While confined here in the Birmingham city jail, I came across your recent statement\ncalling\nmy present activities \"unwise and untimely.\" Seldom do I pause to answer criticism of my\nwork and\nideas. If I sought to answer all the criticisms that cross my desk, my secretaries would\nhave little time\nfor anything other than such correspondence in the course of the day, and I would have no\ntime for\nconstructive work. But since I feel that you are men of genuine good will and that your\ncriticisms are\nsincerely set forth, I want to try to answer your statement in what I hope will be patient\nand\nreasonable terms. I think I should indicate why I am here in Birmingham, since you have been influenced\nby the\nview which argues against \"outsiders coming in.\" I have the honor of serving as president\nof the\nSouthern Christian Leadership Conference, an organization operating in every southern\nstate, with\nheadquarters in Atlanta, Georgia. We have some eighty five affiliated organizations across\nthe South,\nand one of them is the Alabama Christian Movement for Human Rights. Frequently we share\nstaff,\neducational and financial resources with our affiliates. Several months ago the affiliate\nhere in\nBirmingham asked us to be on call to engage in a nonviolent direct action program if such\nwere\ndeemed necessary. We readily consented, and when the hour came we lived up to our promise.\nSo I,\nalong with several members of my staff, am here because I was invited here.  I am here\nbecause I have\norganizational ties here. But more basically, I am in Birmingham because injustice is here. Just as the prophets\nof the\neighth century B.C. left their villages and carried their \"thus saith the Lord\" far beyond\nthe boundaries\nof their home towns, and just as the Apostle Paul left his village of Tarsus and carried\nthe gospel of\nJesus Christ to the far corners of the Greco Roman world, so am I compelled to carry the\ngospel of\nfreedom beyond my own home town. Like Paul, I must constantly respond to the Macedonian\ncall for\naid. Moreover, I am cognizant of the interrelatedness of all communities and states. I\ncannot sit idly\nby in Atlanta and not be concerned about what happens in Birmingham. Injustice anywhere is\na threat\nto justice everywhere. We are caught in an inescapable network of mutuality, tied in a\nsingle garment\nof destiny. Whatever affects one directly, affects all indirectly. Never again can we\nafford to live with\nthe narrow, provincial \"outside agitator\" idea. Anyone who lives inside the United States\ncan never be\nconsidered an outsider anywhere within its bounds. You deplore the demonstrations taking place in Birmingham. But your statement, I am\nsorry to\nsay, fails to express a similar concern for the conditions that brought about the\ndemonstrations. I am\nsure that none of you would want to rest content with the superficial kind of social\nanalysis that deals\nmerely with effects and does not grapple with underlying causes. It is unfortunate that\ndemonstrations are taking place in Birmingham, but it is even more unfortunate that the\ncity's white\npower structure left the Negro community with no alternative. In any nonviolent campaign there are four basic steps: collection of the facts to\ndetermine\nwhether injustices exist; negotiation; self purification; and direct action. We have gone\nthrough all\nthese steps in Birmingham. There can be no gainsaying the fact that racial injustice\nengulfs this\ncommunity. Birmingham is probably the most thoroughly segregated city in the United\nStates. Its ugly\nrecord of brutality is widely known. Negroes have experienced grossly unjust treatment in\nthe courts.\nThere have been more unsolved bombings of Negro homes and churches in Birmingham than in\nany\nother city in the nation. These are the hard, brutal facts of the case. On the basis of\nthese conditions,\nNegro leaders sought to negotiate with the city fathers. But the latter consistently\nrefused to engage\nin good faith negotiation. Then, last September, came the opportunity to talk with leaders of Birmingham's\neconomic\ncommunity. In the course of the negotiations, certain promises were made by the\nmerchants--for\nexample, to remove the stores' humiliating racial signs. On the basis of these promises,\nthe Reverend\nFred Shuttlesworth and the leaders of the Alabama Christian Movement for Human Rights\nagreed to a\nmoratorium on all demonstrations. As the weeks and months went by, we realized that we\nwere the\nvictims of a broken promise. A few signs, briefly removed, returned; the others remained.\nAs in so many past experiences, our hopes had been blasted, and the shadow of deep\ndisappointment settled upon us. We had no alternative except to prepare for direct action,\nwhereby\nwe would present our very bodies as a means of laying our case before the conscience of\nthe local and\nthe national community. Mindful of the difficulties involved, we decided to undertake a\nprocess of self\npurification. We began a series of workshops on nonviolence, and we repeatedly asked\nourselves: \"Are\nyou able to accept blows without retaliating?\" \"Are you able to endure the ordeal of\njail?\" We decided\nto schedule our direct action program for the Easter season, realizing that except for\nChristmas, this is\nthe main shopping period of the year. Knowing that a strong economic-withdrawal program\nwould be\nthe by product of direct action, we felt that this would be the best time to bring\npressure to bear on\nthe merchants for the needed change. Then it occurred to us that Birmingham's mayoral election was coming up in March, and\nwe\nspeedily decided to postpone action until after election day. When we discovered that the\nCommissioner of Public Safety, Eugene \"Bull\" Connor, had piled up enough votes to be in\nthe run off,\nwe decided again to postpone action until the day after the run off so that the\ndemonstrations could\nnot be used to cloud the issues. Like many others, we waited to see Mr. Connor defeated,\nand to this\nend we endured postponement after postponement. Having aided in this community need, we\nfelt\nthat our direct action program could be delayed no longer. You may well ask: \"Why direct action? Why sit ins, marches and so forth? Isn't\nnegotiation a\nbetter path?\" You are quite right in calling for negotiation. Indeed, this is the very\npurpose of direct\naction. Nonviolent direct action seeks to create such a crisis and foster such a tension\nthat a\ncommunity which has constantly refused to negotiate is forced to confront the issue. It\nseeks so to\ndramatize the issue that it can no longer be ignored. My citing the creation of tension as\npart of the\nwork of the nonviolent resister may sound rather shocking. But I must confess that I am\nnot afraid of\nthe word \"tension.\" I have earnestly opposed violent tension, but there is a type of\nconstructive,\nnonviolent tension which is necessary for growth. Just as Socrates felt that it was\nnecessary to create a\ntension in the mind so that individuals could rise from the bondage of myths and half\ntruths to the\nunfettered realm of creative analysis and objective appraisal, so must we see the need for\nnonviolent\ngadflies to create the kind of tension in society that will help men rise from the dark\ndepths of\nprejudice and racism to the majestic heights of understanding and brotherhood.\nThe purpose of our direct action program is to create a situation so crisis packed that it\nwill\ninevitably open the door to negotiation. I therefore concur with you in your call for\nnegotiation. Too\nlong has our beloved Southland been bogged down in a tragic effort to live in monologue\nrather than\ndialogue. One of the basic points in your statement is that the action that I and my associates\nhave\ntaken in Birmingham is untimely. Some have asked: \"Why didn't you give the new city\nadministration\ntime to act?\" The only answer that I can give to this query is that the new Birmingham\nadministration\nmust be prodded about as much as the outgoing one, before it will act. We are sadly\nmistaken if we\nfeel that the election of Albert Boutwell as mayor will bring the millennium to\nBirmingham. While Mr.\nBoutwell is a much more gentle person than Mr. Connor, they are both segregationists,\ndedicated to\nmaintenance of the status quo. I have hope that Mr. Boutwell will be reasonable enough to\nsee the\nfutility of massive resistance to desegregation. But he will not see this without pressure\nfrom devotees\nof civil rights. My friends, I must say to you that we have not made a single gain in\ncivil rights without\ndetermined legal and nonviolent pressure. Lamentably, it is an historical fact that\nprivileged groups\nseldom give up their privileges voluntarily. Individuals may see the moral light and\nvoluntarily give up their unjust posture; but, as Reinhold Niebuhr has reminded us, groups\ntend to be more immoral than\nindividuals.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "American importers and consumers bear the cost of 2025 tariffs: analysis", "url": "https://www.kielinstitut.de/publications/americas-own-goal-who-pays-the-tariffs-19398/", "content": "American importers and consumers bear the cost of 2025 tariffs: analysis. Policy Article Tariffs Trade Policy Pass-Through Import Prices United States International Trade USA • The 2025 US tariffs are an own goal: American importers and consumers bear nearly the entire cost. Foreign exporters absorb only about 4% of the tariff burden—the remaining 96% is passed through to US buyers.• Using shipment-level data covering over 25 million transactions valued at nearly $4 trillion, we find near-complete pass-through of tariffs to US import prices.• US customs revenue surged by approximately $200 billion in 2025—a tax paid almost entirely by Americans.• Event studies around discrete tariff shocks on Brazil (50%) and India (25–50%) confirm: export prices did not decline. Trade volumes collapsed instead.• Indian export customs data validates our findings: when facing US tariffs, Indian exporters maintained their prices and reduced shipments. They did not “eat” the tariff. • The 2025 US tariffs are an own goal: American importers and consumers bear nearly the entire cost. Foreign exporters absorb only about 4% of the tariff burden—the remaining 96% is passed through to US buyers.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Google confirms 'high-friction' sideloading flow is coming to Android", "url": "https://www.androidauthority.com/google-sideloading-android-high-friction-process-3633468/", "content": "Google confirms 'high-friction' sideloading flow is coming to Android. Affiliate links on Android Authority may earn us a commission.Learn more. January 18, 2026  Google has responded to our recent report on new Google Play strings hinting atchanges to how Android will handle sideloaded appsin the future. The company has now confirmed that a “high-friction” install process is on the way. Don’t want to miss the best fromAndroid Authority? Replying to our story on X, Matthew Forsyth, Director of Product Management, Google Play Developer Experience & Chief Product Explainer,saidthe system isn’t a sideloading restriction, but an “Accountability Layer.” Advanced users will still be able to choose “Install without verifying,” though Google says that path will involve extra steps meant to ensure users understand the risks of installing apps from unverified developers. That explanation broadly matches what we’re seeing in recent versions of Google Play, where new warning messages emphasize developer verification, internet requirements, and potential risks, while still allowing users to proceed. What remains to be seen is how far Google takes this “high-friction” approach. Clear warnings are one thing, butquietly making sideloading more painfulis another. Android’s openness has always depended on power users being able to install apps without excessive hoops. For now, Google hasn’t suggested requirements like using a PC or external tools, and we hope the added friction is limited to risk education. Thank you for being part of our community. Read ourComment Policybefore posting.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: How worried should I be about running LLM code on my machine?", "url": "item?id=46686262", "content": "Ask HN: How worried should I be about running LLM code on my machine?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Penn Calls Government's Demand for Lists of Jewish Staff 'Disconcerting'", "url": "https://www.nytimes.com/2026/01/20/us/university-of-pennsylvania-trump-jewish-staff.html", "content": "Penn Calls Government's Demand for Lists of Jewish Staff 'Disconcerting'", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "How to be a good conference talk audience member (2022)", "url": "https://www.mooreds.com/wordpress/archives/3522", "content": "How to be a good conference talk audience member (2022). I recently attended a conference and was both a speaker and audience member. It was on the smaller side; there were probably a few hundred attendees and the audiences ranged from about twenty to hundreds of attendees for the keynotes. After one of the talks, a speaker came up and said “you were such a good audience member, thank you!”. I said the same thing to one of the attendees of the talk I gave. I wanted to share how you can be a good audience member at a conference talk. It’s important to note that this advice is for attending in-person talks where the speaker can see the audience. This is typically when there are up to one hundred people. I’ve spoken in front of 800 people and it’s a different experience. While some of these principles apply, in general individual behavior is less important as audience size grows. And online talks are an entirely different experience for everyone, both audience and speaker! I don’t have enough experience to give any advice for that scenario. First, though, why would youcareto be a good member of an in-person audience? After all, you are providing your time and money to the conference and the presenter. Isn’t it the speaker’sjobto entertain and educateyou? Why would you expend any energy to help them do so? First, I’m a big fan of being respectful of other human beings and helping them succeed. Public speaking is acommon fearand being a good audience member can reassure the speaker and reduce that fear. It’s hard up there, whether it’s your first talk or your hundredth. The second reason is that you can make a talk better foryourself. You can learn more and you can tune their presentation to your needs. They are an expert and you can take advantage of their expertise. So, here are my tips on how to be a great audience member: Am I always a good audience member? Nope. I get distracted sometimes.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation", "url": "https://cloud.google.com/blog/topics/threat-intelligence/net-ntlmv1-deprecation-rainbow-tables", "content": "Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation.  Stop attacks, reduce risk, and advance your security. Written by: Nic Losby Mandiant ispublicly releasinga comprehensive dataset of Net-NTLMv1 rainbow tables to underscore the urgency of migrating away from this outdated protocol. Despite Net-NTLMv1 being deprecated and known to be insecure for over two decades—with cryptanalysis dating back to 1999—Mandiant consultants continue to identify its use in active environments. This legacy protocol leaves organizations vulnerable to trivial credential theft, yet it remains prevalent due to inertia and a lack of demonstrated immediate risk. By releasing these tables, Mandiant aims to lower the barrier for security professionals to demonstrate the insecurity of Net-NTLMv1. While tools to exploit this protocol have existed for years, they often required uploading sensitive data to third-party services or expensive hardware to brute-force keys. The release of this dataset allows defenders and researchers to recover keys in under 12 hours using consumer hardware costing less than $600 USD. This initiative highlights the amplified impact of combining Mandiant's frontline expertise with Google Cloud's resources to eliminate entire classes of attacks. This post details the generation of the tables, provides access to the dataset for community use, and outlines critical remediation steps to disable Net-NTLMv1 and prevent authentication coercion attacks. Net-NTLMv1 has been widely known to be insecure since at least 2012, following presentations at DEFCON 20, with cryptanalysis of the underlying protocoldating back to at least 1999. On Aug. 30, 2016, Hashcatadded supportfor cracking Data Encryption Standard (DES) keys using known plaintext, further democratizing the ability to attack this protocol. Rainbow tables are almost as old, with the initial paper on rainbow tables published in2003 by Philippe Oechslin, citing an earlier iteration of a time-memory trade-off from1980 by Martin Hellman. Essentially, if an attacker can obtain a Net-NTLMv1 hash without Extended Session Security (ESS) for the known plaintext of1122334455667788, a cryptographic attack, referred to as a known plaintext attack (KPA), can be applied. This guarantees recovery of the key material used. Since the key material is the password hash of the authenticating Active Directory (AD) object—user or computer—the attack results can quickly be used to compromise the object, often leading to privilege escalation. A common chain attackers use is authentication coercion from a highly privileged object, such as a domain controller (DC). Recovering the password hash of the DC machine account allows for DCSync privileges to compromise any other account in AD. The unsorted dataset can be downloaded usinggsutil -m cp -r gs://net-ntlmv1-tables/tables .or through theGoogle Cloud Research Dataset portal.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Interactive eBPF", "url": "https://ebpf.party/", "content": "Interactive eBPF. Learn eBPF through hands-on exercises. Write, compile, and run programs\n        directly from your browser. Did you find an issue, or have an idea for a new exercise? Create an\n        issue inthe repository. Curious about how it works?Here's an explanation.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Dell UltraSharp 52 Thunderbolt Hub Monitor", "url": "https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories", "content": "Dell UltraSharp 52 Thunderbolt Hub Monitor", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Amnesty urges halt to execution of 19-year-old Iranian protester", "url": "https://www.iranintl.com/en/202601209686", "content": "Amnesty urges halt to execution of 19-year-old Iranian protester. Amnesty International on Tuesday called for an immediate halt to the planned execution of 19-year-old Iranian protester Amirhossein Ghaderzadeh, whose death sentence is due to be carried out on Wednesday. “Iranian authorities must immediately halt any plans to execute 19-year-old Amirhossein Ghaderzadeh, who has been detained since 9 January for taking part in protests in Rasht, Gilan province, and stop weaponizing the death penalty against protesters,” Amnesty said in a post on X. “According to an informed source, the authorities told him during a court session on 17 January that he is accused of ‘betraying his country’ and sentenced to ‘death by hanging’. The authorities have informed his family that his execution is scheduled for 21 January,” the group added. Israel’s foreign minister Gideon Sa’ar has urged the European Union to formally designate Iran’s Islamic Revolutionary Guard Corps (IRGC) as a terrorist organization, citing its role in crushing protests and sponsoring terror across the region. “Designate Iran's Revolutionary Guards as a terrorist organization!” the foreign minister wrote on X on Tuesday, responding to a post from European Commission President Ursula von der Leyen. “You know very well what their role is in the murderous repression of the civilian protest in Iran, as well as in spreading terror in the Middle East and beyond,” Sa’ar added.​ The European Union is proposing new sanctions on Iran, including a ban on additional exports of drone and missile technologies, European Commission President Ursula von der Leyen said on Monday. She added that, together with the EU’s foreign policy chief Kaja Kallas, the Commission is preparing further human rights related measures. “We are also preparing new sanctions in response to the regime’s continued and brutal repression of protesters,” she said. “Iran’s authorities try to shut down the internet because they are afraid,” the US State Department’s Near Eastern Affairs bureau said in a post on X on Monday, as it defended Washington’s sanctions policy.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "ClickHouse acquires Langfuse", "url": "https://langfuse.com/blog/joining-clickhouse", "content": "ClickHouse acquires Langfuse. Our goal continues to be building the best LLM engineering platform  ClickHouse has acquired Langfuse. If you’re reading this as a Langfuse user, your first question is probably:What does this mean for me? Our roadmap stays the same, our goal continues to be building the best LLM engineering platform, and we remain committed to open source and self-hosting. There are no immediate changes to how you use Langfuse and how you can reach out to us. Whatdoeschange is our ability to move faster. With ClickHouse behind us, we can invest more deeply into performance, reliability, and our roadmap that helps teams build and improve AI applications in production. This is the section we would want to read first, too. Joining Clickhouse compresses years of operational learning into immediate, real customer benefits. The longer version of how we got here is in ourhandbook. Langfuse started the same way many LLM products start: we were building agents ourselves. And we constantly ran into the same problems.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Overlapping Markup", "url": "https://en.wikipedia.org/wiki/Overlapping_markup", "content": "Overlapping Markup. Inmarkup languagesand thedigital humanities,overlapoccurs when a document has two or more structures that interact in a non-hierarchicalmanner.\nA document with overlapping markup cannot be represented as atree.\nThis is also known asconcurrent markup.\nOverlap happens, for instance, inpoetry, where there may be ametricalstructure offeetand lines; a linguistic structure of sentences and quotations; and a physical structure of volumes and pages and editorial annotations.[1][2] The problem of non-hierarchical structures in documents has been recognised since 1988; resolving it against the dominant paradigm of text as a single hierarchy (anordered hierarchy of content objectsorOHCO) was initially thought to be merely a technical issue, but has, in fact, proven much more difficult.[4]In 2008,Jeni Tennisonidentified markup overlap as \"the main remaining problem area for markup technologists\".[5]Markup overlap continues to be a primary issue in the digital study of theological texts in 2019, and is a major reason for the field retaining specialised markup formats—theOpen Scripture Information Standardand theTheological Markup Language—rather than the inter-operableText Encoding Initiative-based formats common to the rest of thedigital humanities.[6] A distinction exists between schemes that allow non-contiguous overlap, and those that allow only contiguous overlap. Often, 'markup overlap' strictly means the latter.\nContiguous overlap can always be represented as a linear document with milestones (typically co-indexed start- and end-markers), without the need for fragmenting a (logical) component into multiple physical ones. Non-contiguous overlap may require document fragmentation. Another distinction in overlapping markup schemes is whether elements can overlap with other elements of the same kind (self-overlap).[2] A scheme may have aprivilegedhierarchy.\nSomeXML-based schemes, for example, represent one hierarchy directly in the XML document tree, and represent other, overlapping, structures by another means;\nthese are said to benon-privileged. Schmidt (2012)identifies a tripartite classification of instances of overlap: 1. \"Variation of content and structure\", 2. \"Overlay of multiple perspectives or markup sets\", and 3. \"Overlap of individual start and end tags within a single markup perspective\";\nadditionally, some apparent instances of overlap are in fact schema definition problems, which can be resolved hierarchically.\nHe contends that type 1 is best resolved by a system of multiple documents external to the markup, but types 2 and 3 require dealing with internally. DeRose (2004, Evaluation criteria) identifies several criteria for judging solutions to the overlap problem: Tag soupis, strictly speaking, not overlapping markup—it is malformedHTML, which is a non-overlapping language, and may be ill-defined.\nSomeweb browsersattempted to represent overlapping start and end tags with non-hierarchicalDocument Object Models(DOM), but this was not standardised across all browsers and was incompatible with the innately hierarchical nature of the DOM.[7][8]HTML5defines how processors should deal with such mis-nested markup in the HTML syntax and turn it into a single hierarchy.[9]WithXHTMLandSGML-based HTML, however, mis-nested markup is a strict error and makes processing by standards-compliant systems impossible.[10]The HTML standard defines aparagraphconcept which can cause overlap with other elements and can be non-contiguous.[11] SGML, which early versions of HTML were based on, has a feature called CONCUR that allows multiple independent hierarchies to co-exist without privileging any.DTDvalidation is only defined for each individual hierarchy with CONCUR. Validation across hierarchies is not defined by the standard. CONCUR cannot support self-overlap, and it interacts poorly with some of SGML's abbreviatory features.\nThis feature has been poorly supported by tools and has seen very little actual use;\nusing CONCUR to represent document overlap was not a recommended use case, according to a commentary by the standard's editor.[12][13] There are several approaches to representing overlap in a non-overlapping language.[14]TheText Encoding Initiative, as an XML-based markup scheme, cannot directly represent overlapping markup.\nAll four of the below approaches are suggested.[15]TheOpen Scripture Information Standardis another XML-based scheme, designed to mark up theBible.\nIt uses empty milestone elements to encode non-privileged components.[16] To illustrate these approaches, marking up the sentences and lines of a fragment ofRichard IIIbyWilliam Shakespearewill be used as a running example. Where there is a privileged hierarchy, the lines will be used.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "STFU", "url": "https://github.com/Pankajtanwarbanna/stfu", "content": "STFU", "source": "HackerNews", "date": null, "author": null, "score": null}
