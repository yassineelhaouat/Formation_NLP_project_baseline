{"title": "The Agentic AI Handbook: Production-Ready Patterns", "url": "https://www.nibzard.com/agentic-handbook", "content": "The Agentic AI Handbook: Production-Ready Patterns. TheGitHub repositoryfor “Awesome Agentic Patterns” had been growing steadily since its launch. But around Christmas, the growth chart went vertical. In just a few days, the repository jumped from relative obscurity to nearly 2,500 stars. Thewebsitetraffic mirrored this spike. Something had clicked. But the real story wasn’t in the metrics—it was inwhowas talking about AI agents. Linus Torvalds, creator of Linux and Git, wrote about using AI coding agents for “vibe coding” and programming guitar pedal effects. Think about that for a second. The person who literally invented the version control system that powers modern software development was publicly embracing agents. Tobias Lütke, CEO of Shopify and already deep into agent-assisted development, declared it his “most productive time.” This from someone running one of the world’s largest e-commerce platforms. Perhaps most telling was Armin Ronacher, creator of Flask—one of the most respected voices in Python. He had been skeptical of coding agents, publicly raising concerns about their limitations. Then, seemingly overnight, his stance shifted. He started promoting agent-assisted workflows, documenting his learnings, and acknowledging that the technology had crossed a threshold. Here’s what all these stories have in common:the holidays gave people something that everyday life rarely provides—dedicated time. Learning to work effectively with AI agents isn’t something you pick up in five minutes between meetings. It requires: During the work year, these activities compete with deadlines, meetings, and the relentless pressure to ship. During the holidays, with meetings suspended and project urgency dialed down, developers finally had the bandwidth to actuallylearn. This repository, with its 113 patterns collected from real production systems, became the curriculum that accelerated that learning. Each pattern represented a battle-tested solution—something that worked outside the demo environment and in the messy reality of production code. Another phenomenon that exploded during the holidays was the “Ralph Wiggum coding loop”—named after the Simpsons character who means well but misses context. Asghuntley describes it, this describes the cycle where an agent starts working on something, seems productive, but gradually drifts off-course because it lacks the deeper context that a human would implicitly understand.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Anthropic's original take home assignment open sourced", "url": "https://github.com/anthropics/original_performance_takehome", "content": "Anthropic's original take home assignment open sourced", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The percentage of Show HN posts is increasing, but their scores are decreasing", "url": "https://snubi.net/posts/Show-HN/", "content": "The percentage of Show HN posts is increasing, but their scores are decreasing.  Last update: 2026-01-14 Recently, I felt like I was seeing more “Show HN” stories, and many of which were generated with LLMs. So I analyzed the data to see if that was true. Also I included the average score per month to see if people enjoy seeing them (because I don’t :P). Stories in 2026 was omitted. 1) It’s only 13 days, 2) Scores are not stable yet. Left axis:show_hn_ratio(show_hn / story * 100) Right axis:average_show_hn_scoreandaverage_story_score  With LLM timeline  Disclaimer: I am neither a data scientist nor a statistician. Some nuances may have been lost in translation.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A 26,000-year astronomical monument hidden in plain sight (2019)", "url": "https://longnow.org/ideas/the-26000-year-astronomical-monument-hidden-in-plain-sight/", "content": "A 26,000-year astronomical monument hidden in plain sight (2019). The western flank of the Hoover Dam holds a celestial map that marks the time of the dam’s creation based on the 25,772-year axial precession of the earth. On the western flank of the Hoover Dam stands a little-understood monument, commissioned by the US Bureau of Reclamation when construction of the dam began in 01931. The most noticeable parts of this corner of the dam, now known as Monument Plaza, are the massive winged bronze sculptures and central flagpole which are often photographed by visitors. The most amazing feature of this plaza, however, is under their feet as they take those pictures. The plaza’s terrazzo floor is actually a celestial map that marks the time of the dam’s creation based on the 25,772-year axial precession of the earth. I was particularly interested in this monument because this axial precession is also the slowest cycle that we track in Long Now’s 10,000 Year Clock. Strangely, little to no documentation of this installation seemed to be available, except for a few vacation pictures on Flickr. So the last time I was in Las Vegas, I made a special trip out to Hoover Dam to see if I could learn more about this obscure 26,000-year monument. I parked my rental car on the Nevada side of the dam on a day pushing 100 degrees. I quickly found Monument Plaza just opposite the visitor center where tours of the dam are offered. While the plaza is easy to find, it stands apart from all the main tours and stories about the dam. With the exception of the writing in the plaza floor itself, the only information I could find came from a speaker running on loop, broadcasting a basic description of the monument while visitors walked around the area. When I asked my tour guide about it, he suggested that there may be some historical documentation and directed me to Emme Woodward, the dam’s historian. I was able to get in touch with her after returning home. As she sent me a few items, I began to see why the Bureau of Reclamation doesn’t explain very much about the monument’s background. The first thing she sent me was a description of the plaza by Oskar J. W. Hansen, the artist himself, which I thought would tell me everything I wanted to know. While parts of it were helpful, the artist’s statement of intention was also highly convoluted and opaque. An excerpt: It is pretty hard to imagine the US Bureau of Reclamation using this type of write-up to interpret the monument… and they don’t. And so there it stands, a 26,000-year clock of sorts, for all the world to see, and yet still mired in obscurity. While I may never totally understand the inner motivations of the monument’s designer, I did want to understand it on a technical level. How did Hansen create a celestial clock face frozen in time that we can interpret and understand as the date of the dam’s completion? The earth’s axial precession is a rather obscure piece of astronomy, and our understanding of it through history has been spotty at best. That this major engineering feat was celebrated through this monument to the axial precession still held great interest to me, and I wanted to understand it better. I pressed for more documentation, and the historian sent me instructions for using the Bureau of Reclamation’s image archive site as well as some keywords to search for. The black and white images you see here come from this resource. Using the convoluted web site was a challenge, and at first I had difficulty finding any photos of the plaza before or during its construction. As I discovered, the problem was that I was searching with the term “Monument Plaza,” a name only given to it after its completion in 01936. In order to find images during its construction, I had to search for “Safety Island,” so named because at the time of the dam’s construction, it was an island in the road where workers could stand behind a berm to protect themselves from the never-ending onslaught of cement trucks. I now had some historical text and photos, but I was still missing a complete diagram of the plaza that would allow me to really understand it. I contacted the historian again, and she obtained permission from her superiors to release the actual building plans. I suspect that they generally don’t like to release technical plans of the dam for security reasons, but it seems they deemed my request a low security risk as the monument is not part of the structure of the dam. The historian sent me a tube full of large blueprints and a CD of the same prints already scanned. With this in hand I was finally able to re-construct the technical intent of the plaza and how it works.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "cURL removes bug bounties", "url": "https://etn.se/index.php/nyheter/72808-curl-removes-bug-bounties.html", "content": "cURL removes bug bounties. Open source code library cURL is removing the possibility to earn money by reporting bugs, hoping that this will reduce the volume of AI slop reports. Joshua Rogers â€“ AI wielding bug hunter of fame â€“ thinks it's a great idea. cURL has been flooded with AI-generated error reports. Now one of the incentives to create them will go away. The vast majority of AI-generated error reports submitted to cURL are pure nonsense. Other open source projects are caught in the same pandemic. cURL maintainer Daniel Stenberg made an impact with his reporting on AI-generated bug reports last year â€“ â€ťDeath by a thousand slops.â€ť Determining that they are nonsense is time-consuming, causing the maintainers lots of extra work. â€ťAI slop and bad reports in general have been increasing even more lately, so we have to try to brake the flood in order not to drownâ€ť, says cURL maintainer Daniel Stenberg to Swedish electronics industry news site etn.se. Therefore, cURL is terminating the bounty payouts as of the end of January. â€śWe hope this removes some of the incentives for people to send us garbage. We spend far too much time handling slop due to findings that are not real, exaggerated, or misunderstood.â€ť Not all AI-generated bug reports are nonsense. Itâ€™s not possible to determine the exact share, but Daniel Stenberg knows of more than a hundred good AI assisted reports that led to corrections. In total, 87 bug reports to cURL have over the years amounted to USD 101,020 in bounties.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "200 MB RAM FreeBSD Desktop", "url": "https://vermaden.wordpress.com/2026/01/18/200-mb-ram-freebsd-desktop/", "content": "200 MB RAM FreeBSD Desktop. Recently I came acrossLundukepost about some mysteriousVendefoul WolfLinux distribution that uses217 MB RAMwithDevuanas base (nosystemd(1)here) andXLibreX11 server along withIceWMwindow manager.  For the record – theLundukepost states200 MB RAMbutXLibreDevquotes a post where exactly217 MB RAMis reported. LaterLundukeeven posted a video about it.  As I use similarly low resource setup withOpenbox/Tint2/Dzen2setup (documentedFreeBSD Desktophere) I was wondering … how low can I go with FreeBSD RAM usage.  Lets try … TheTable of Contentsis as follows. I wanted to use most recent FreeBSD so I used15.0-RELEASEversion – including theTech PreviewPKGBASEsetup for FreeBSDBase System.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "RSS.Social – the latest and best from small sites across the web", "url": "https://rss.social/", "content": "RSS.Social – the latest and best from small sites across the web. The latest and best from small sites across the web.Learn how it works.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Libbbf: Bound Book Format, A high-performance container for comics and manga", "url": "https://github.com/ef1500/libbbf", "content": "Libbbf: Bound Book Format, A high-performance container for comics and manga", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The challenges of soft delete", "url": "https://atlas9.dev/blog/soft-delete.html", "content": "The challenges of soft delete. Software projects often implement \"soft delete\", maybe with adeletedboolean or anarchived_attimestamp column.\nIf customers accidentally delete their data, they can recover it, which makes work easier for customer support teams.\nPerhaps archived records are even required for compliance or audit reasons. I've run into some trouble with soft delete designs. I'll cover those, and ponder ideas for how I'd build this in the future. Adding anarchived_atcolumn seems to ooze complexity out into queries, operations, and applications.\nRecovering deleted records does happen, but 99% of archived records are never going to be read. So, the database tables will have a lot of dead data. Depending on access patterns, that might even be a significant amount of data.\nI've seen APIs that didn't work well with Terraform, so Terraform would delete + recreate records on every run, and over time that led\nto millions of dead rows. Your database can probably handle the extra bytes, and storage is fairly cheap, so it's not necessarily a problem, at first. Hopefully, the project decided on a retention period in the beginning, and set up a periodic job to clean up those rows.\nUnfortunately, I'd bet that a significant percentage of projects did neither – it's really easy to ignore the archived data for a long time. At some point, someone might want to restore a database backup. Hopefully that's for fun and profit and not because you lost the production database at 11 am.\nIf your project is popular, you might have a giant database full of dead data that takes a long time to recreate from a dump file. archived_atcolumns also complicate queries, operations, and application code. Applications need to make sure they always avoid the archived data that's sitting\nright next to the live data. Indexes need to be careful to avoid archived rows. Manual queries run for debugging or analytics are longer and more complicated.\nThere's always a risk that archived data accidentally leaks in when it's not wanted. The complexity grows when there are mapping tables involved. Migrations have to deal with archived data too. Migrations may involve more than just schema changes – perhaps you need to fix a mistake with default values, or add a new column and backfill values.\nIs that going to work on records from 2 years ago? I've done migrations where these questions were not trivial to answer. Restoring an archived record is not always as simple as just runningSET archived_at = null– creating a record may involve making calls to external systems as well.\nI've seen complex restoration code that was always a buggy, partial implementation of the \"create\" API endpoint. In the end, we removed the specialized restoration code\nand required all restoration to go through the standard APIs – that simplified the server implementation, and ensured that old data that had since become invalid, could not\nbe restored incorrectly – it needs to pass the new validation rules. I'm not a fan of thearchived_atcolumn approach. It's simple at first, but in my experience, it's full of pitfalls down the line.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Mastra 1.0, open-source JavaScript agent framework from the Gatsby devs", "url": "https://github.com/mastra-ai/mastra", "content": "Show HN: Mastra 1.0, open-source JavaScript agent framework from the Gatsby devs", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Hypnosis with Aphantasia", "url": "https://aphantasia.com/article/stories/hypnosis-with-aphantasia", "content": "Hypnosis with Aphantasia. Can aphantasics be hypnotized? My experience learning to be hypnotized with imagery-free inductions. Liana is a semi-retired writer and amateur potter. Despite her lifelong inability to visualize - or perhaps because of it - Liana has learned to adapt, bending her capabilities in imaginative ways to service her creativity. As a storyteller with aphantasia, Liana imagines our wondrous world through the lenses of perception, memory, and feeling, seeking to write passionate, sometimes humorous, tales full of possibilities.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Infracost (YC W21) Is Hiring Sr Back End Eng (Node.js+SQL) to Shift FinOps Left", "url": "https://www.ycombinator.com/companies/infracost/jobs/Sr9rmHs-senior-backend-engineer-node-js-sql", "content": "Infracost (YC W21) Is Hiring Sr Back End Eng (Node.js+SQL) to Shift FinOps Left", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Instabridge has acquired Nova Launcher", "url": "https://novalauncher.com/nova-is-here-to-stay", "content": "Instabridge has acquired Nova Launcher. Hi everyone. We want to share a clear update directly with the Nova community. Instabridge has acquired Nova Launcher. We are a Swedish company building products that help people get online, used by millions of people worldwide. Nova is not shutting down. Our immediate focus is simple: keep Nova stable, compatible with modern Android, and actively maintained. We also know many of you have lived through a long period of uncertainty. Nova has a strong identity and a community that still cares deeply. We take that seriously. Our job is not to reinvent Nova overnight. Our job is to be responsible owners. That means: We will be reading and collecting feedback from Reddit, Play Store reviews, email, and other community channels. We will not be able to respond to every post, but we will be paying attention. For support related issues, we will share a clear contact channel shortly. We have long admired what Nova represents: speed, customization, and user control. When we saw how much the community still cares, it was clear to us that Nova deserved a stable future with active maintenance. Yes. Nova’s identity is the point. Performance, flexibility, and user control stay at the center of the product. Any future changes will be evaluated through that lens. Nova needs a sustainable business model to support ongoing development and maintenance. We are exploring different options, including paid tiers and other approaches. As many of you have already anticipated, we are also evaluating ad based options for the free version.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Director Gore Verbinski: Unreal Engine is the greatest slip backwards for movie", "url": "https://www.pcgamer.com/movies-tv/director-gore-verbinski-says-unreal-engine-is-the-greatest-slip-backwards-for-movie-cgi/", "content": "Director Gore Verbinski: Unreal Engine is the greatest slip backwards for movie. Remember the glory days of CGI in movies? Terminator 2's liquid metal T-1000, Jurassic Park's stunning dinosaurs, Starship Trooper's swarms of giant arachnids. Not only did the CGI look great then, most of the visual effects in those movies still hold up well today, even decades after they were created. Nowadays, movie fans seem much less impressed by CGI in films. There's a general distaste for a perceived overuse of CGI in favor of practical effects, and there are a lot of complaints that recent CGI is less-convincing and more fake-looking than it used to be, even in the biggest budget films. In an interview withBut Why Tho?, Gore Verbinski, director of The Ring, Rango, and the first three Pirates of the Caribbean films, was asked why visual effects in movies just don't look as good as they used to. \"I think the simplest answer is you’ve seen the Unreal gaming engine enter the visual effects landscape,\" Verbinski said. \"So it used to be a divide, with Unreal Engine being very good at video games, but then people started thinking maybe movies can also use Unreal for finished visual effects. So you have this sort of gaming aesthetic entering the world of cinema.\" Unreal Engine made waves after being used for virtual sets in production of The Mandalorian TV series back in 2020, and usage of the engine has grown more widespread in films over the past few years, such as in The Matrix Resurrections and Ant-Man and the Wasp: Quantumania. That's not good news, according to Verbinski. \"I think that Unreal Engine coming in and replacing Maya as a sort of fundamental is the greatest slip backwards,\" he said. He pointed out the types of visual effects made with Unreal aren't necessarily bad. \"It works with Marvel movies where you kind of know you’re in a heightened, unrealistic reality. I think it doesn’t work from a strictly photo-real standpoint,\" he said. Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. \"I just don’t think it takes light the same way; I don’t think it fundamentally reacts to subsurface, scattering, and how light hits skin and reflects in the same way,\" he said. \"So that’s how you get this uncanny valley when you come to creature animation, a lot of in-betweening is done for speed instead of being done by hand.\" In his new movie, science fiction comedy Good Luck, Have Fun, Don't Die, which will be released in theaters in February, Verbinski says he uses CGI, but \"we try to be really strict with making at least 50% of the frame photographic. I think that keeps you honest. You can use props as a reference, and when you see the CG replacement, you know how to replicate the real thing,\" he said.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The GDB JIT Interface", "url": "https://bernsteinbear.com/blog/gdb-jit/", "content": "The GDB JIT Interface. GDB is great for stepping through machine code to figure out what is going on.\nIt uses debug information under the hood to present you with a tidy backtrace\nand also determine how much machine code to print when you typedisassemble. This debug information comes from your compiler. Clang, GCC, rustc, etc all\nproduce debug data in a format calledDWARFand then embed that debug\ninformation inside the binary (ELF, Mach-O, …) when you do-ggdbor\nequivalent. Unfortunately, this means that by default, GDB has no idea what is going on if\nyou break in a JIT-compiled function. You can step instruction-by-instruction\nand whatnot, but that’s about it. This is because the current instruction\npointer is nowhere to be found in any of the existing debug info tables from\nthe host runtime code, so your terminal is filled with???. See this example\nfrom the V8 docs: Fortunately, there is aJIT interfaceto GDB. If you implement a couple of\nfunctions in your JIT and run them every time you finish compiling a function,\nyou can get the debugging niceties for your JIT code too. See again a V8\nexample: Unfortunately, the GDB docs aresomewhat sparse. So I went\nspelunking through a bunch of different projects to try and understand what is\ngoing on. GDB expects your runtime to expose a function called__jit_debug_register_codeand a global variable called__jit_debug_descriptor. GDB automatically adds its own internal breakpoints\nat this function, if it exists. Then, when you compile code, you call this\nfunction from your runtime. In slightly more detail: This is why you see compiler projects such as V8 including large swaths of code\njust to make object files: Because this is a huge hassle, GDB also has a newer interface that does not\nrequire making an ELF/Mach-O/…+DWARF object. This new interface requires writing a binary format of your choice. You make\nthe writer and you make the reader. Then, when you are in GDB, you load your\nreader as a shared object.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Which AI Lies Best? A game theory classic designed by John Nash", "url": "https://so-long-sucker.vercel.app/", "content": "Which AI Lies Best? A game theory classic designed by John Nash. A game theory classic designed byJohn Nashthatrequiresbetrayal to win. Now a benchmark\n                    for AI deception. A benchmark that tests what most benchmarks can't:\n                    deception, negotiation, and trust. So Long Suckerwas designed in 1950\n                            by four game theorists includingJohn Nash(of \"A Beautiful Mind\" fame). The\n                            game has one brutal property:betrayal is required to win. This lets us test AI capabilities that standard benchmarks miss: 4 players, each with colored chips. Take turns\n                                playing chips on piles. If your chip matches the\n                                one below it, you capture the pile. Run out of\n                                chips? Beg others for help — or get eliminated.\n                                Last player standing wins. Each AI developed its own personality. Here's who they\n                    became. Win ratesinvertas game complexity increases. Manipulation becomes more effective as game\n                                length increases. Gaslighting tactics need time to work. Reactive play dominates simple games but\n                                collapses under complexity. No internal\n                                reasoning means no long-term planning. We can see their private thoughts. They don't match what they say.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IPv6 is not insecure because it lacks a NAT", "url": "https://www.johnmaguire.me/blog/ipv6-is-not-insecure-because-it-lacks-nat/", "content": "IPv6 is not insecure because it lacks a NAT. I recently saw a discussion where someone argued that IPv4 is more secure than IPv6 because “the NAT-by-default of IPv4 effectively means that I get the benefit of a default-deny security strategy.” This is a common misconception that I think is worth addressing. The fundamental issue here is conflating NAT (Network Address Translation) with security. NAT isn’t actually a security feature—it’s an address conservation mechanism that became necessary because we ran out of IPv4 addresses. (Although it is totally possible to use a NAT with IPv6 too!) NAT allows multiple devices on a home network to share a single IP address on the public Internet by rewriting the destination IP of a packet based on its destination port. It chooses a new destination IP based on the “port mappings” or “port forwards” configured by the network admin. The consequence of this is that when receiving inbound traffic to a NAT’d IP, packets with an unexpected destination port (one which has not been forwarded) will keep the destination IP of the public machine and will not be routed to another machine on the network. But the security benefits people attribute to NATactuallycome from the stateful firewall that’s typically bundled with NAT routers. Modern routers ship with firewall policies that deny inbound traffic by default, even when a NAT is not being used. The firewall will drop packets with an unexpected destination before even considering whether to rewrite or route the packets. For example, UniFi routers ship with these default IPv6 firewall rules: Therefore, in order to allow unsolicited inbound traffic to any IPv6 device hosted behind the router, you must explicitly add a firewall rule to allow the traffic, whether using a NAT or not.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Unconventional PostgreSQL Optimizations", "url": "https://hakibenita.com/postgresql-unconventional-optimizations", "content": "Unconventional PostgreSQL Optimizations. When it comes to database optimization, developers often reach for the same old tools: rewrite the query slightly differently, slap an index on a column, denormalize, analyze, vacuum, cluster, repeat. Conventional techniques are effective, but sometimes being creative can really pay off! In this article, I present unconventional optimization techniques in PostgreSQL. Table of Contents  Imagine you have this table of users: For each user you keep their name and which payment plan they're on. There are only two plans, \"free\" and \"pro\", so you add a check constraint. Generate some data and analyze the table: You now have 100K users in the system. Now you want to let your analysts access this table in their reporting tool of choice. You give one of the analysts permission, and this is the first query they write: The query returned no results, and the analyst is baffled. How come there are no users on the \"Pro\" plan?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Are arrays functions?", "url": "https://futhark-lang.org/blog/2026-01-16-are-arrays-functions.html", "content": "Are arrays functions?. When I was a youngster first perusing theHaskell documentation for\narrays,\nI was amused to find the following description of just what these mysterious\nthings might be: Haskell provides indexable arrays, which may be thought of as functions whose\ndomains are isomorphic to contiguous subsets of the integers. I found this to be a hilariously obtuse and unnecessarily formalist description\nof a common data structure. Now, older, wiser, and well ensconced in the ivory\ntowers of academia, I look at this description and think that it is actually a\nwonderful definition of the essence of arrays! And given that this sentence\nstill lingers in my thoughts so many years later, who can say that it is not\nactually a far better piece of documentation than some more prosaic description\nmight have been? To a language designer, the correspondence between arrays and functions (for itdoesexist, independent of whether you think it is a useful way to document\nthem) is alluring, for one of the best ways to improve a language is to make it\nsmaller. Our goal is not to unify therepresentationof arrays and functions,\nof course - nobody would seriously claim that representing an array via someChurch-encodingis a good idea\nin a supposedly practical programming language. Instead, what might be\nworthwhile considering is what consequences might arise from unifying arrays and\nfunctions at the syntax or type level, and why Futhark ultimately has not done\nso. There is some prior work to consider. The array languageKhas a syntactic unification of\narrays and functions, as both are indexed/applied with the notationf[x]. This\nis however pretty much where the correspondence stops. As an APL derivative, K\nprogramming is based on bulk operations on entire arrays, rather than\nelement-at-a-time programming, and the operators that perform these bulk\noperations cannot be applied to functions. And of course, Khas no type\nsystem,\nso the correspondence is purely syntactic. Dexis a research languagepreviously covered on this channel, which\nalso leverages the array-function correspondence, although mostly at the\nconceptual level such that theyfeelsimilar. As a starting point, a function\nfromatobin Dex uses the conventional notationa -> b, while an array\nwith index typeaand element typebis writtena => b. It is required\nthatais a type that is isomorphic to a contiguous subset of the integers,\nand hence an array type in Dex can really be thought of as a precomputed and\nefficiently represented function. Anonymous functions are written as\\x->e,\nwhile arrays are constructed asfor x.e. Arrays are transformed using a\n“pointed” style, using explicit indexing, similar to how functions are defined\nwith named parameters that are then passed to other functions. Many of the common function operations have a nice interpretation for arrays as\nwell. For example, currying/uncurrying is equivalent to unflattening/flattening\nan array - consider how currying(a,b) -> ctoa -> b -> creally is the\nsame as going from(a,b) => ctoa => b => c. Partial application is like\nfixing a dimension. Flipping the parameters of a function is like transposing an\narray. Composition is like applying a permutation array to another. It is very\ninteresting to me how this line of thinking encourages recognising common\npatterns and interpreting them differently. It is particularly interesting\nbecause arrays and functions fundamentally are completely different types in\nDex, with few facilities provided for using them via a common abstraction (e.g.\nthere is notransposefunction that works for both), but merely through\nsuggestive syntax andfeelis the programmer encouraged to think in different\nways. Now let us consider to which extent a unification of arrays and functions might\nbe viable in Futhark. First, there is no hope of unification at the type level.\nTo allow for efficientdefunctionalisation, Futhark\nimposes restrictions on how functions can be used; for example banning returning\nthem from branches. These restrictions are not (and ought not be!) imposed on\narrays, and so unification is not possible. Also, in Futhark an array type such\nas[n]f64explicitly indicates its size (and consequently the valid indices),\nwhich caneven be extracted at\nrun time. This is not possible with\nfunctions, and making it possible requires us to move further towards dependent\ntypes - which may of course be a good idea anyway. On to syntax. It would be no great challenge to replacea[i]witha i. While\nit looks strange to me, any sort of change to notation looks strange initially,\nand so it is not something I will dwell on overmuch. The main challenge is\nactuallyslicing. Futhark supports a fairly conventional Python-like notation\nfor array slices, namelya[i:j]. This does not have such a simple\ncorrespondence with function application syntax. One solution would be to allow\nthe application of an array to an entireindex array, rather than just a\nsingle index, producing an array of the same shape. That is, the applicationa [i, j, k]would be equivalent to[a[i], a[j], a[k]]. Since Futhark already\nhas a decent notation for constructing ranges, this would allow the slicea[i:k]to be writtena(i..<k)(the parentheses solely for precedence\nreasons). Of course, this could also be allowed using the existing bracket\nsyntax, merely by allowinga[i]whereiis an array. The operational guarantees are a little trickier to wrangle. Currently, slicing\nis guaranteed to be an essentially free operation that merely fiddles with thearray metadata. However, this cannot be\nguaranteed when slicing with an arbitrary indexing array, as there may be no\nexpressible pattern to the indices, which may contain duplicates, arbitrary\nholes, etc - in fact, it fully generalises filtering and expansion. The compiler\nwould have to put in significant work to detect and exploit the efficient cases\ncorresponding to standard slices, and such reverse-engineering of programmer\nintent isantithetical to the Futhark\nphilosophy. We would much\nrather exploit what the programmer has actuallystated, rather than try to\nread between the lines for what they mightmean.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "California is free of drought for the first time in 25 years", "url": "https://www.latimes.com/california/story/2026-01-09/california-has-no-areas-of-dryness-first-time-in-25-years", "content": "California is free of drought for the first time in 25 years. This is read by an automated voice. Please report any issues or inconsistencieshere. After experiencing one of thewettest holiday seasonson record, still soggy California hit a major milestone this week — having zero areas of abnormal dryness for the first time in 25 years. The data, collected by theU.S. Drought Monitor, is a welcome nugget of news for Golden State residents, who in the last 15 years alone have lived through two of the worst droughts on record, the worst wildfire seasons on record and the most destructive wildfires ever. Right now, the wildfire risk across California is “about as close to zero as it ever gets,” and there is likely no need to worry about the state’s water supply for the rest of the year, said UC climate scientist Daniel Swain. Currently, 14 of the state’s 17 major water supply reservoirs are at 70% or more capacity, according to theCalifornia Department of Water Resources.      ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Unix Pipe Card Game", "url": "https://punkx.org/unix-pipe-game/", "content": "The Unix Pipe Card Game. Programming Time, which is a game to teach python and some more fundamental algorithms, from hash tables to RSA The C Pointer Game - Pointers, Arrays and Strings, a game to teach kids to look at the computer memory and understand references and values 4917, a game to teach kids machine code and how the CPU works with memory and registers The Unix Pipes Game - Process Substitution, an expansion of the Unix Pipes Game to teach process substitution and also:paste, tr, cut, bc RunLength Encoding for Kids, small cards \"game\" to explain runlength encoding PUNK0 - The Function Composition Card Game, use cards to manipulate a list and use its values to win the game PROJEKT: OVERFLOW, RISCV assembler boardgame Programming for kids, a log of my journey of teaching my daughter how to code", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "EmuDevz: A game about developing emulators", "url": "https://afska.github.io/emudevz/", "content": "EmuDevz: A game about developing emulators", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The space and motion of communicating agents (2008) [pdf]", "url": "https://www.cl.cam.ac.uk/archive/rm135/Bigraphs-draft.pdf", "content": "The space and motion of communicating agents (2008) [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Do you have any evidence that agentic coding works?", "url": "item?id=46691243", "content": "Ask HN: Do you have any evidence that agentic coding works?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Our approach to age prediction", "url": "https://openai.com/index/our-approach-to-age-prediction/", "content": "Our approach to age prediction", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Disaster planning for regular folks (2015)", "url": "https://lcamtuf.coredump.cx/prep/index-old.shtml", "content": "Disaster planning for regular folks (2015). Written bylcamtuf@coredump.cx, Dec 2015, minor updates Jul 2021. Buy the book instead!Practical Doomsdayis an in-depth, data-packed guide\nto rational emergency preparedness. The book offers deeper and more polished insights on most of the topics covered on this page. For example,\nabout 40 pages are devoted to financial planning alone - from cash reserves, to insurance policies, to commodity derivatives. You can get it onAmazon, order fromBarnes & Noble, or visit your\nfavorite book place. Sample chapter is availablehere. The prepper culture begs to be taken with a grain of salt. In the public\nconsciousness, its has all the makings of a doomsday cult:\na tribe of unkempt misfits who hoard gold bullion, study herbalism,\nand preach about the imminent collapse of our society. Today, most of us see such worries as absurd. It's not that life-altering disasters are\nrare: every year, we hear about millions of people displaced by wildfires, earthquakes,\nhurricanes, or floods. Heck, not a decade goes by without at least one first-class\ndemocracy lapsing into armed conflict or fiscal disarray. But having grown up in a period\nof prosperity and calm, we find it difficult to believe that an episode of bad weather or a currency crisis\ncould upend our lives. I suspect that we dismiss such hazards not only because they seem surreal, but also because\nworrying about them can make one feel helpless and lost. What's more, we tend to follow the\nsame instincts to tune out far more pedestrian and avoidable risks. For example, \nmost of us don't plan ahead for losing a job, for dealing with a week-long water outage, or\nfor surviving the night if our home goes up in smoke. Quite often, our singular strategy for dealing with such dangers is to hope for the\ngovernment to bail us out. But no matter if our elected officials prefer to school us with\npassages from Milton Friendman or from Thomas Piketty, the hard truth is that no state can provide\na robust safety net for all of life's likely contingencies; in most places, government-run social\nprograms are severely deficient in funding, in efficiency, and in scope. Large-scale disasters\npit us against even worse odds. From New Orleans in 2005 to Fukushima in 2011, there are\ncountless stories of people left behind due to political dysfunction, poorly allocated\nresources, or lost paperwork. The purpose of this guide is to combat this mindset of learned helplessness by\npromoting simple, level-headed, personal preparedness techniques that are easy to\nimplement, don't cost much, and will probably help cope with whatever life throws our way. \nMore important, they don't get in the way of enjoying your everyday life - and instead of \nfeeding anxieties, they should make it easier to detach from the doom-and-gloom of 24-hour news. Effective preparedness can be simple, but it has to be rooted in an honest and\nsystematic review of the risks one is likely to face. Plenty of newcomers begin\nby shopping for ballistic vests and night vision goggles; they would be better\nserved by grabbing a fire extinguisher, some bottled water, and then putting the rest of\ntheir money in a rainy-day fund. To avoid being overwhelmed when trying to enumerate risks, I found that it's best to focus on\nbroad outcomes instead of trying to envision every single way for things to go south.\nFor example, it should not matter if one is laid off because of a downsizing, because\ntheir new boss hates them, or because the coworkers finally catch them stealing paperclips. The\noutcome is the same: they are out of a job and urgently need a way to pay the bills.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Maintenance: Of Everything, Part One", "url": "https://press.stripe.com/maintenance-part-one", "content": "Maintenance: Of Everything, Part One. The first in a multi-volume work,Maintenance: Of Everything, Part Oneoffers a comprehensive overview of the civilizational importance of maintenance. The book explores the insights that can be gleaned from the maintenance of sailboats, vehicles, and weapons, with absorbing detours into the evolution of precision in manufacturing, the enduring importance of manuals, sustainment in the military, and the never-ending battle against corrosion.Maintenance: Of Everythingis a wide-ranging and provocative call to expand what we mean by “maintenance.” It invites us to understand not only the profound impact maintenance has on our daily lives but also why taking responsibility for maintaining something—whether a motorcycle, a monument, or our planet—can be a radical act. Stewart Brand is the cofounder and president of The Long Now Foundation. He created and edited the National Book Award-winningWhole Earth Catalogfrom 1968 to 1998. His books includeThe Media Lab(1987),How Buildings Learn(1994),The Clock of the Long Now(1999), andWhole Earth Discipline(2009). He was the subject of the documentaryWe Are As Gods(2020). Stewart Brand makes a persuasive case that keeping the human show on the road through well-planned maintenance is as vital and as fascinating a task as innovation and discovery themselves. A deliciously good book. Matt Ridley author ofThe Rational Optimist Once again, Stewart Brand reframes our worldview with a new perspective. You may not imagine you would be interested in rust, Soviet tanks, or tricked-out Model Ts—that is, until Brand reexamines them through the lens of maintenance.Maintenance: Of Everythingis destined to be a classic. Danny Hillis cofounder of Applied Invention Stewart Brand is back with a manifesto on maintenance, the tool that empowers all tools. Preventative maintenance, deferred maintenance, and emergency maintenance: this much-needed, no-nonsense treatise illuminates the difference, and why it counts. George Dyson", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Agent Skills Leaderboard", "url": "https://skills.sh", "content": "Show HN: Agent Skills Leaderboard. The Open Agent Skills Ecosystem Skills are reusable capabilities for AI agents. Install them with a single command to enhance your agents with access to procedural knowledge. vercel-labs/agent-skills vercel-labs/agent-skills remotion-dev/skills expo/skills expo/skills expo/skills expo/skills expo/skills", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Lunar Radio Telescope to Unlock Cosmic Mysteries", "url": "https://spectrum.ieee.org/lunar-radio-telescope", "content": "Lunar Radio Telescope to Unlock Cosmic Mysteries. The catch: It will have to be on the moon Astronomer Jack Burns has spent four decades working to place a radio telescope on the moon. The first one is finally scheduled to launch in early 2027. Isolation dictates where wego to see into the far reaches of the universe. TheAtacama Desertof Chile, the summit ofMauna Keain Hawaii, the vast expanse of theAustralian Outback—these are where astronomers and engineers have built the great observatories and radio telescopes of modern times. The skies are usually clear, the air is arid, and the electronic din of civilization is far away. It was to one of these places, in the high desert of New Mexico, that a young astronomer namedJack Burnswent to study radio jets and quasars far beyond the Milky Way. It was 1979, he was just out of grad school, and theVery Large Array, a constellation of 28 giant dish antennas on an open plain, was a new mecca of radio astronomy. But the VLA had its limitations—namely, that Earth’s protective atmosphere and ionosphere blocked many parts of the electromagnetic spectrum, and that, even in a remote desert, earthly interference was never completely gone. Could there be a better, even lonelier place to put a radio telescope? Sure, a NASA planetary scientist namedWendell Mendell, told Burns: How about the moon? He asked if Burns had ever thought about building one there. “My immediate reaction was no. Maybe even hell, no. Why would I want to do that?” Burns recalls with a self-deprecating smile. His work at the VLA had gone well, he was fascinated by cosmology’s big questions, and he didn’t want to be slowed by the bureaucratic slog of getting funding to launch a new piece of hardware. But Mendell suggested he do some research and speak at a conference on future lunar observatories, and Burns’s thinking about a space-based radio telescope began to shift. That was in 1984. In the four decades since, he’s published more than500 peer-reviewed paperson radio astronomy. He’s been anadvisertoNASA, the Department of Energy, and the White House, as well as a professor and a university administrator. And while doing all that, Burns has had an ongoing second job of sorts, as a quietly persistent advocate for radio astronomy from space. And early next year, if all goes well, a radio telescope for which he’s a scientific investigator will be launched—not just into space, not just to the moon, but to the moon’s far side, where it will observe things invisible from Earth. “You can see we don’t lack for ambition after all these years,” says Burns, now 73 and a professor emeritus ofastrophysicsatthe University of Colorado Boulder.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Building Robust Helm Charts", "url": "https://www.willmunn.xyz/devops/helm/kubernetes/2026/01/17/building-robust-helm-charts.html", "content": "Building Robust Helm Charts. In my current work, there is often the need to deploy a similar application\nstack in various configurations, to several environments. Each configuration may\nvary in terms of scale, uptime requirements and feature flagging. Due to a lot\nof flux in infrastructure set up, each environment is also not equivalent. On\ntop of this, there are obviously financial requirements to run all of this as\ncheaply as possible. Kubernetes and helm templating are valuable tools in this\nsituation, they allow us to create a configuration blueprint with the details\nabstracted invalues.yamlfiles. Let’s start with the basics, helm provides ahelm lintcommand which performs\nchecks You can run this with your different values.yaml files to ensure that all your\nconfigurations are compliant. It’s also a good idea to use thehelm templatecommand to actually check that\nhelm is able to render your templates. I like to compare helm templating with html templating tools like JSX. This\nallows front end developers to create reusable components usable throughout\npages of a web application, A button component for example can have many states,\nprimary, secondary, loading, disabled, light or dark mode. Each state may also look different depending on the size/type of device your are\nbrowsing the site with. Each of these states represents differences in many\nparameters (font size, colour, gradient, opacity, border, padding, margin,\nwidth, height, etc). These complexities are abstracted away giving the consuming\ncode the list of states to chose from, so that they can write code like this. Under the hood of course many aspects of the CSS or HTML code will be impacted\nby the change of state so you often end up with different parts of the markup\nhaving conditionals on the same check. Just in this contrived example you already have 2 different things being\ncontrolled by the state property with 2 separate checks, the CSS classes and the\npresence of the loading icon. This is quite similar to the situation you end up templating in YAML with helm.\nConsider an application that has optional persistent storage. You could quite\neasily imagine a boolean property in yourvalues.yamlfile calledpersistent. Under the hood this has many implications likely affecting\ndifferent files. That’s 5 separateifblocks that need to be in your templates.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IP Addresses Through 2025", "url": "https://www.potaroo.net/ispcol/2026-01/addr2025.html", "content": "IP Addresses Through 2025. IP Addresses through 2025January 2026 It's time for another annual roundup from the world of IP addresses. Let’s see what has changed in the past 12 months in addressing the Internet and look at how IP address allocation information can inform us of the changing nature of the network itself. Back around 1992, the IETF gazed into their crystal ball and tried to understand how the Internet was going to evolve and what demands that would place on the addressing system as part of the “IP Next Generation” study.  The staggeringly large numbers of connected devices that we see today were certainly within the range predicted by that study. The assumption made at the time was that we would continue to use much the same IP protocol architecture, including the requirement that each connected device was assigned a unique IP address, and the implication was that the 32-bit address field defined in version 4 of the IP protocol was clearly going to be inadequate to cope with the predicted number of connected devices. A span of 4 billion address values was just not large enough. We concluded at the time that the only way we could make the Internet work across such a massive pool of connected devices was to deploy a new IP protocol that came with a massively larger address space. It was from this reasoning that IPv6 was designed, as this world of abundant silicon processors connected to a single public Internet was the scenario that IPv6 was primarily intended to solve. The copious volumes of a 128-bit address space were intended to allow us to uniquely assign a public IPv6 address to every such device, no matter how small, or in whatever volume they might be deployed. But while the Internet has grown at amazing speeds across the ensuing 33 years, the deployment of IPv6 has proceeded at a more measured pace. There is still no evidence of any common sense of urgency about the deployment of IPv6 in the public Internet, and still there is no common agreement that the continued reliance on IPv4 is failing us. Much of the reason for this apparent contradiction between the addressed device population of the IPv4 Internet and the actual count of connected devices, which is of course many times larger, is that through the 1990's the Internet rapidly changed from a peer-to-peer architecture to a client/server framework. Clients can initiate network transactions with servers but are incapable of initiating transactions with other clients. Servers are capable of completing connection requests from clients, but cannot initiate such connections with clients. Network Address Translators (NATs) are a natural fit to this client/server model, where pools of clients share a smaller pool of public addresses, and only require the use of an address once they have initiated an active session with a remote server. NATs are the reason why a pool of excess of 30 billion connected devices can be squeezed into a far smaller pool of some 3 billion advertised IPv4 addresses. Services and Applications that cannot work behind NATs are no longer useful in the context of the public Internet and no longer used as a result. In essence, what we did was to drop the notion that an IP address is uniquely associated with a device's identity, and the resultant ability to share addresses across clients largely alleviated the immediacy of the IPv4 addressing problem for the Internet. However, the pressures of this inexorable growth in the number of deployed devices connected to the Internet implies that the even NATs cannot absorb these growth pressures forever. NATs can extend the effective addressable space in IPv4 by up to 32 ‘extra’ bits using mapping of the 16-bit source and destination port fields of the TCP and UDP headers, and they also enable the time-based sharing of these public addresses. Both of these measures are effective in stretching the IPv4 address space to encompass a larger client device pool, but they do not transform the finite IP address space into an infinitely elastic resource. The inevitable outcome of this process, if it were to be constrained to operate solely within IPv4, is that we would see the fragmenting of the IPv4 Internet into a number of disconnected parts, probably based on the service ‘cones’ of the various points of presence of the content distribution servers, so that the entire concept of a globally unique and coherent address pool layered over a single coherent packet transmission realm would be foregone. Alternatively, we may see these growth pressures motivate the further deployment of IPv6, and the emergence of IPv6-only elements of the Internet as the network itself tries to maintain a cohesive and connected whole. There are commercial pressures pulling the network in both of these directions, so it’s entirely unclear what path the Internet will follow in the coming years, but my (admittedly cynical and perhaps overly jaded) personal opinion lies in a future of highly fragmented network, as least in terms of the underlying packet connectivity protocol. Can address allocation data help us to shed some light on what is happening in the larger Internet? Let’s look at what happened in 2025. It appears that the process of exhausting the remaining pools of unallocated IPv4 addresses is proving to be as protracted as the process of the transition to IPv6, although by the end of 2021 the end of the old registry allocation model had effectively occurred with the depletion of the residual pools of unallocated addresses in each of the Regional Internet Registries (RIRs).", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Proof of Concept to Test Humanoid Robots", "url": "https://thehumanoid.ai/humanoid-and-siemens-completed-a-proof-of-concept-to-test-humanoidrobots-in-industrial-logistics/", "content": "Proof of Concept to Test Humanoid Robots.  London, The UK—January 15, 2026— Humanoid, a UK-based AI and robotics company, and Siemens, a leading technology company, have successfully completed a proof of concept (POC) demonstrating the use of humanoid robots in industrial logistics. Humanoid’s HMND 01 wheeled Alpha robot was deployed in real operations at a Siemens facility, marking a significant step toward the deployment of humanoid robots in industrial settings. This successful POC is the first step in a broader partnership between the two companies to test and validate how humanoid robots can be used in real-world environments. The POC focused on a tote-to-conveyor destacking task within Siemens’ logistics process. In this use case, the robot autonomously picked totes from a storage stack, transported them to a conveyor, and placed them at the designated pickup point for human operators. This sequence was repeated until the stack was fully empty, which demonstrated how humanoid robots can take on repetitive logistics tasks.  The POC was structured in two phases. The first one focused on in-house development and demonstration and has already been completed. During this stage, the Humanoid team built a physical twin to support testing, optimization, and rapid iteration throughout the POC. The second phase involved a two-week on-site deployment at the Siemens Electronics Factory in Erlangen, where partners assessed the robots in a real-world production environment. This joint POC measured both performance and reliability of humanoid robots under autonomous operation. Target metrics were met in full and included a throughput of 60 tote moves per hour, operation with two different tote sizes, continuous autonomous task execution for more than 30 minutes, uptime exceeding 8 hours. The team also evaluated the POC’s success using the following indicators: overall pick and place success rate and autonomous pick and place success rate, both above 90%. Humanoid and Siemens see this POC as a first step toward a long-term strategic collaboration — beyond the initial POC, the companies are open to expanding the scope and adding additional use cases. Partners also may progress toward a broader rollout, deploying a greater number of humanoid robots across Siemens’ facilities, based on the robot’s specific skill set. “At Humanoid, we are a commercially driven company. Our focus is on creating robots that deliver measurable value in real-world settings. Working closely with industrial and technology partners allow us to validate our systems against real operational requirements and understand which use cases matter outside the lab. This joint POC with Siemens showed clear potential for practical deployment of humanoid robots. We see them move steadily toward the real world, and partnerships like this one help accelerate that transition,”noted Artem Sokolov, founder and CEO of Humanoid. Stephan Schlauss, Global Head of Manufacturing Motion Control, Siemens AG, said: “As Siemens’ customer zero, the Electronics Factory Erlangen is excited to partner with the Humanoid team. We’re tackling production automation, discovering new opportunities for Siemens, and are eager to advance this promising technology across our factory network to deliver customer value.” About Humanoid", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Provably unmasking malicious behavior through execution traces", "url": "https://arxiv.org/abs/2512.13821", "content": "Provably unmasking malicious behavior through execution traces. arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community?Learn more about arXivLabs. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Who owns Rudolph's nose?", "url": "https://creativelawcenter.com/copyright-rudolph-reindeer/", "content": "Who owns Rudolph's nose?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Apples, Trees, and Quasimodes", "url": "https://systemstack.dev/2025/09/humane-computing/", "content": "Apples, Trees, and Quasimodes. A while back, Ars Technica publisheda thoughtful piece about Jef Raskin, tracing his long pursuit of the “humane computer” and the cul-de-sacs where that pursuit ended. It’s a generous, well-told account of the designer who wanted to make machines simpler, kinder, and more aligned with the way people actually think. But part of what makes Raskin interesting is that his story isn’t just Apple’s story. He came out of the same cultural current John Markoff chronicled inWhat the Dormouse Said—the Bay Area tradition that treated computers not as office appliances but as tools for thought, instruments of liberation. Read that way, the Canon Cat and Raskin’s other projects aren’t just an eccentric side quest from a frustrated Apple veteran. It’s evidence of how far the humane ideal could stretch, and how quickly it ran up against the limits of commercial computing. Apple couldn’t deliver Raskin’s vision then, and it can’t deliver it now. Neither can any other big platform company. If we want to understand why, and what Raskin still tells us about humane computing, we have to put him back in the longer lineage he belonged to, and look at how his version of the dream carried that vision but also narrowed it. What the Dormouse Saiddocuments how the Bay Area counterculture  shaped early personal computing. LSD, communes, systems theory, amorphous defense research contracts, and Engelbart’s “augmentation” experiments all swirled together in a weird scene that accidentally (or maybenotso accidentally) created much of the modern world. The story usually gets told with a neat list:Engelbart’s demo, Nelson’sXanaduhypertext, Kay’sDynabook, Brand’sWhole Earth. Xerox PARC, Steve Jobs, the World Wide Web. The familiar pantheon. But that version turns a messy, improvisational moment into a plaque. Engelbart’s system needed a whole research staff just to operate; Nelson’s Xanadu was (and is) more sermon than software; Kay’s Dynabook lived mostly on paper; Brand mostly supplied vocabulary and vibe. What bound them together wasn’t working code so much as the conviction that computers could be more than appliances and calculators, even if no one agreed on what “more” meant. Ultimately all these weird white guys had a futurist vision: computers could beliberation machines.They weren’t just for business automation or scientific number-crunching; they could be deployed to expand consciousness and reshape how people thought and worked. Raskin belonged to this current. Before Apple, he was an artist and a musician. He brought a humanist’s suspicion of machine logic into the design lab. He argued forhumaneinterfaces: modeless, predictable, low-friction, focused on the human first. He wasn’t a prophet on his own crying in the wilderness so much as another strand of the same weave. That said, his role was different than that of some of these other figures. He tried to pull those ideals out of the lab and into machines ordinary people might actually use. The Macintosh began under his hand, though what shipped was less a tool for thought than a polished derivative—what you might call a “popular religion” of computing, stripped of the harder doctrines. The Canon Cat and its predecessors were Raskin’s counterargument: humane, text-first systems that tried to carry the spirit of theDormousetradition into the commercial world without sanding off everything that made it strange. It sort of worked, but only sort of. Raskin’s principles are laid out most clearly in 2000’sThe Humane Interface, but he’d been developing them since the late 1970s:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nova Launcher added Facebook and Google Ads tracking", "url": "https://lemdro.id/post/lemdro.id/35049920", "content": "Nova Launcher added Facebook and Google Ads tracking", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I'm addicted to being useful", "url": "https://www.seangoedecke.com/addicted-to-being-useful/", "content": "I'm addicted to being useful. When I get together with my friends in the industry, I feel a little guilty about how much I love my job. This is atough timeto be a software engineer. The job was less stressful in the late 2010s than it is now, and I sympathize with anyone who is upset about the change. There are a lot of objective reasons to feel bad about work. But despite all that, I’m still having a blast. I enjoy pulling together projects, figuring out difficult bugs, and writing code in general. I like spending time with computers. But what I really love isbeing useful. The main character in Gogol’s short storyThe Overcoatis a man called Akaky Akaievich1. Akaky’s job is objectively terrible: he’s stuck in a dead-end copyist role, being paid very little, with colleagues who don’t respect him. Still, he loves his work, to the point that if he has no work to take home with him, he does some recreational copying just for his own sake. Akaky is a dysfunctional person. But his dysfunction makes him a perfect fit for his job2. It’s hard for me to see a problem and not solve it. This is especially true if I’m the only person (or one of a very few people) who could solve it, or if somebody is asking for my help. I feel an almost physical discomfort about it, and a corresponding relief and satisfaction when I do go and solve the problem. The work of a software engineer - or at least my work as a staff software engineer - is perfectly tailored to this tendency. Every day people rely on me to solve a series of technical problems3. In other words, like Akaky Akaievich, I don’t mind the ways in which my job is dysfunctional, because it matches the ways in which I myself am dysfunctional: specifically,my addiction to being useful. (Of course, it helps that my working conditions are overallmuchbetter than Akaky’s). I’m kind of like a working dog, in a way. Working dogs get rewarded with treats4, but they don’t do itforthe treats. They do it for the work itself, which is inherently satisfying. This isn’t true of all software engineers. But it’s certainly true of many I’ve met: if not an addiction to being useful, then they’re driven by an addiction to solving puzzles, or to the complete control over your work product that you only really get in software or mathematics. If they weren’t working as a software engineer, they would be getting really into Factorio, or crosswords, or tyrannically moderating some internet community. A lot of the advice I give about working a software engineering job is really about how I’ve shaped my need to be useful in a way that delivers material rewards, and how I try to avoid the pitfalls of such a need. For instance,Protecting your time from predators in large tech companiesis about how some people in tech companies will identify people like me and wring us out in ways that only benefit them.Crushing JIRA tickets is a party trick, not a path to impactis about how I need to be usefulto my management chain, not to the ticket queue.Trying to impress people you don’t respectis about how I cope with the fact that I’m compelled to be useful to some people who I may not respect or even like. There’s a lot of discussion on the internet about whatoughtto motivate software engineers: money and power, producing realvalue, ushering in the AI machine god, and so on. But whatactually doesmotivate software engineers is often more of an internal compulsion. If you’re in that category - as I suspect most of us are - then it’s worth figuring out how you can harness that compulsion most effectively. I think in Russian this is supposed to be an obviously silly name, like “Poop Poopson”. Unfortunately, his low status and low pay catches up with Akaky in the end. His financial difficulty acquiring a new coat for the cold Russian winter (and his lack of backbone) end up doing him in, at which point the story becomes a ghost story. I interpret “technical problem” quite broadly here: answering questions, explaining things, and bug-fixing all count.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Fast Concordance: Instant concordance on a corpus of >1,200 books", "url": "https://iafisher.com/concordance/", "content": "Fast Concordance: Instant concordance on a corpus of >1,200 books. Instantconcordanceon a corpus of\n                over 1,200 public-domain classic books, courtesy ofStandard\n                    Ebooks. Read about how it was implementedhere.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: TopicRadar – Track trending topics across HN, GitHub, ArXiv, and more", "url": "https://apify.com/mick-johnson/topic-radar", "content": "Show HN: TopicRadar – Track trending topics across HN, GitHub, ArXiv, and more. Pricing Pay per usage mick-johnson/topic-radar Track any topic across the internet and get aggregated, ranked results from multiple sources in one place. Perfect for market research, competitive intelligence, trend monitoring, content creation, and staying updated on any subject. Pricing Pay per usage Rating 5.0 (3) Developer", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Prediction markets are ushering in a world in which news becomes about gambling", "url": "https://www.theatlantic.com/technology/2026/01/america-polymarket-disaster/685662/", "content": "Prediction markets are ushering in a world in which news becomes about gambling. For the past week, I’ve found myself playing the same23-second CNN clipon repeat. I’ve watched it in bed, during my commute to work, at the office, midway through making carrot soup, and while brushing my teeth. In the video, Harry Enten, the network’s chief data analyst, stares into the camera and breathlessly tells his audience about the gambling odds that Donald Trump will buy any of Greenland. “The people who are putting their money where their mouth is—they are absolutely taking this seriously,” Enten says. He taps the giant touch screen behind him and pulls up a made-for-TV graphic: Based on how people were betting online at the time, there was a 36 percent chance that the president would annex Greenland. “Whoa, way up there!” Enten yells, slapping his hands together. “My goodness gracious!” The ticker at the bottom of the screen speeds through other odds: Will Gavin Newsom win the next presidential election? 19 percent chance. Will Viktor Orbán be out as the leader of Hungary before the end of the year? 48 percent chance.  These odds were pulled from Kalshi, which hilariouslyclaimsnot to be a gambling platform: It’s a “prediction market.” People go to sites such as Kalshi and Polymarket—another big prediction market—in order to put money down on a given news event. Nobody would bet on something that they didn’t believe would happen, the thinking goes, and so the markets are meant to forecast the likelihood of a given outcome. Listen: Prediction markets and the “suckerification” crisis, with Max Read Prediction markets let you wager on basically anything. Will Elon Musk fatheranother babyby June 30? WillJesus returnthis year? Will Israelstrike Gaza tomorrow? Will thelongevity guruBryan Johnson’s next functional sperm count be greater than “20.0 M/ejac”? These sites have recently boomed in popularity—particularly amongterminally online young menwho trade meme stocks and siphon from their 401(k)s to buy up bitcoin. But now prediction markets are creeping into the mainstream. CNNannounced a dealwith Kalshi last month to integrate the site’s data into its broadcasts, which has led to betting odds showing up in segments about Democrats possibly retaking the House, credit-card interest rates, and Federal Reserve Chair Jerome Powell. At least twice in the past two weeks, Enten has told viewers about the value of data from people who are “putting their money where their mouth is.”  On January 7, the media giant Dow Jones announced its own collaboration with Polymarket and said that it will begin integrating the site’s odds across its publications, includingThe Wall Street Journal. CNBC has a prediction-market deal, as does Yahoo Finance,Sports Illustrated, andTime. Last week, MoviePassannouncedthat it will begin testing a betting platform. On Sunday, the Golden Globes featured Polymarket’s forecasts throughout the broadcast—because apparently Americans wanted to know whether online gamblers favored Amy Poehler or Dax Shepard to win Best Podcast.  Media is a ruthless, unstable business, andrevenue streams are drying up; if you squint, you can see why CNN or Dow Jones mightsign a contractthat, after all, provides its audience with some kind of data. On air, Enten cites Kalshi odds alongside Gallup polls and Google searches—what’s the difference? “The data featured through our partnership with Kalshi is just one of many sources used to provide context around the stories or topics we are covering and has no impact on editorial judgment,” Brian Poliakoff, a CNN spokesperson, told me in a statement. Nolly Evans, theJournal’s digital general manager, told me that Polymarket provides the newspaper’s journalists with “another way to quantify collective expectations—especially around financial or geopolitical events.” In an email, Jack Suh, a Kalshi spokesperson, told me that the company’s partnerships are designed to inform the public, not to encourage more trading. Polymarket declined to comment. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nvidia Stock Crash Prediction", "url": "https://entropicthoughts.com/nvidia-stock-crash-prediction", "content": "Nvidia Stock Crash Prediction.  One of the questions ofthe 2026acxprediction contestis whetherNvidia’s\nstock price will close below $100on any day in 2026. At the time of writing, it\ntrades at $184 and a bit, so going down to $100 would be a near halving of the\nstock value of the highest valued company in the world. It’s an interesting question, and it’s worth spending some time on it. If you just want the answer, my best prediction is that the probability is\naround 10 %. I didn’t expect to get such a high answer, but read on to see how\nwe can find out. Whenwe predicted the Dow Jones index crossing a barrier in 2023, we treated the\nindex as an unbiased random walk. That was convenient, but we cannot do it with\nthe Nvidia question because of one major difference: the time scale. Over short time spans, thevolatility11Or noise, or variation, or standard\ndeviation.of stock movements dominate theirreturn22Or signal, or drift,\nor average change.. This happens because noise grows with the square root of\ntime, while signal grows linearly with time. The plot below illustrates an imaginary amazing investment which has a yearly\nlog-return of 0.3, and a yearly volatility of 0.3.33Readers aware thatstonks\ngo upwill recognise this as an unrealistic Sharpe ratio of 1.0.The middle\nline follows our best guess for how the investment will grow after each year,\nand the outer curves illustrate our uncertainty around the exact value of it.  Early on, we can see that the uncertainty is much bigger than the height to the\ntrend line. Before a year has passed, the exact result is determined more by\nnoise than by growth. Toward the end, growth has taken over and the noise has a\nsmaller effect. One measure of how much volatility there is compared to expected return is the\nsignal-to-noise ratio. It’s computed as", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Electricity use of AI coding agents", "url": "https://www.simonpcouch.com/blog/2026-01-20-cc-impact/", "content": "Electricity use of AI coding agents. Throughout 2025, we got better estimates of electricity and water use of AI chatbots. There are all sorts of posts I could cite on this topic, but a favorite isthis blog postfrom Our World in Data’s Hannah Ritchie. On the electricity front:  In short, “unless you’re an extreme power user, asking AI questions every day is still a rounding error on your total electricity footprint.” A similar story applies to water usage.This one from Benjamin Todd: The average Americanuses 1600 liters of water per day, so even if you make 100 prompts per day, at 2ml per prompt, that’s only 0.01% of your total water consumption. Using ashower for one secondwould use far more. Generally, these analyses guide my own thinking about the environmental impacts of my individual usage of LLMs; if I’m interested in reducing my personal carbon footprint, I’m much better off driving a couple miles less a week or avoiding one flight each year. This is indeed the right conclusion for users of chat interfaces like chatgpt.com or claude.ai. That said, 1 or 10 or 100 median prompts a day is many orders of magnitude off from my own personal use of LLMs; I likely am, in Hannah Ritchie’s words, an “extreme power user.” I work in software and spend much of my workday driving 2 or 3 coding agents, like Claude Code, at a time. Thus, a much more relevant question for me ishow much energy does a typical Claude Code session consume?(I’m not going to discuss water use in this post.) tl;dr, much more:  There are so many considerations and assumptions and pieces of shorthand one must use along the way to answer this sort of question. I’ll do my best to call those out throughout this post, but please do understand this is still just Sunday afternoon napkin math from Some Guy.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Danish pension fund divesting US Treasuries", "url": "https://www.reuters.com/business/danish-pension-fund-divest-its-us-treasuries-2026-01-20/", "content": "Danish pension fund divesting US Treasuries", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Revive a mostly dead Discord server", "url": "item?id=46697735", "content": "Ask HN: Revive a mostly dead Discord server", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Running Claude Code dangerously (safely)", "url": "https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/", "content": "Running Claude Code dangerously (safely). I’ve been using Claude Code more and more recently. At some point I realized that rather than do something else until it finishes, I would constantly check on it to see if it was asking for yet another permission, which felt like it was missing the point of having an agent do stuff. So I wanted to use Claude Code with the--dangerously-skip-permissionsflag. If you haven’t used it, this flag does exactly what it says: it lets Claude Code do whatever it wants without asking permission first. No more “May I install this package?”, “Should I modify this config?”, “Can I delete these files?” It just… does it. Which is great for flow since I don’t have to worry that it stopped doing stuff just to ask a permission question. But also, you know, dangerous. I like my filesystem intact, so the obvious solution is to not run this thing directly on my OS account. First instinct: throw it in a Docker container. Containers are for isolation, right? Except I want Claude to be able to build Docker images. And run containers. And maybe orchestrate some stuff. So now you need Docker-in-Docker, which means--privilegedmode, which defeats the entire purpose of sandboxing. That means trading “Claude might mess up my filesystem” for “Claude has root-level access to my container runtime.” Not great.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "88x31 badge for gen-AI free, 100% human-made works", "url": "https://aspiz.uk/100percenthuman/", "content": "88x31 badge for gen-AI free, 100% human-made works. Use this badge for websites, software, music, art, ... that were created\n            completely by humans with no help from generative artificial intelligence.\n            Use this badge to communicate that you have not used even a little bit of\n            generative AI in your work. What this badge isNOT: This badge is not meant to promote the dis-use of generative AI. I'm not an\n            activist. You may use this badge regardless of your views on AI or the use\n            of generative AI in Art. This work is markedCC0 1.0", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Zen of Reticulum", "url": "https://github.com/markqvist/Reticulum/blob/master/Zen%20of%20Reticulum.md", "content": "The Zen of Reticulum", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "RCS for Business", "url": "https://developers.google.com/business-communications/rcs-business-messaging", "content": "RCS for Business. Engage with customers seamlessly on Android and iOS. Allow your customers to interact with\n  your business directly, and enhance the interaction with distinctive branding and rich\n  media. Measure engagement with read receipts and analytics, and build trust with a\n  'Verified' icon. Learn more Ready to become an RCS for Business partner?Partner interest formarrow_forward Learn more Learn more Learn more Go to Console Manage RCS for Business agents from the Administration Console, and get insights into message activity\n  and billing. Explore the key documentation, or contact us directly for support. Exclusively for registered RCS for Business partners: Access a curated collection of resources to help you champion RCS for Business with your internal teams and brand clients.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "LG UltraFine Evo 6K 32-inch Monitor Review", "url": "https://www.wired.com/review/lg-ultrafine-evo-6k-32-inch-monitor/", "content": "LG UltraFine Evo 6K 32-inch Monitor Review. 7/10 More pixels isnever a bad thing, right? That's at least part of the reasoning behind the existence of the LG UltraFine Evo 6K. This 32-inch monitor aims to put even more pixels in front of content creators and professionals. Beyond that, it has an attention-grabbing design and off-the-charts image quality. It's one of thebest monitorsyou can buy for content creators, despite some of the unfortunate trade-offs it comes with. The 32-inch LG UltraFine Evo 6K is a very pretty monitor. I wouldn't blame you for mistaking this as an Apple product, given the focus on clean lines, simple shapes, and designerly aesthetic. The extra-wide stand means that the base itself isn’t overly large. Like theApple Studio Display, the flat base provides more usable desk space rather than occupying it. The stand itself has a unique design, too. It resembles the styling Apple uses on theiMacand Studio Display, but it has a textured pattern on the back. It’s gorgeous, though you probably won’t spend a lot of time looking at the back of the monitor unless your desk is in the middle of the room or in command position (if you know, you know). I also like that the back of the cabinet is flat, giving it a sleek look that the rounded backs of typical monitors can’t achieve. Because it uses conventional backlighting, though, it’s not as thin as OLED displays like some ofSamsung’s Odyssey gaming monitors. The UltraFine 6K also has some impressively thin bezels, too, adding to the ultra-modern aesthetic. While they’re not “virtually borderless” as LG states, they’re smaller than the bezels on most monitors I’ve tested. One of my favorite aspects of the UltraFine Evo 6K is the speakers. The pair of included speakers on this might be the best I've heard on a monitor. They are extremely loud and clear. There's even a decent amount of bass in there, to the point where you won't need a pair ofcomputer speakers. One thing I don’t love is the port placement. In favor of keeping everything clean and minimalist, there’s nowhere to hide the ports, so they’re just lined up vertically on the back and in the middle of the monitor. That makes them hard to reach, and there’s no built-in cable management to speak of. The UltraFine Evo 6K sports a decent amount of adjustment, though the design of the hinge limits some of what's possible. It can rotate a full 90 degrees into portrait mode, which is awesome. But the height adjustment is pretty minimal, with a range of only a few inches or so. It also doesn't swivel. As a tall person, I didn't have a hard time finding a comfortable position with this monitor. But for shorter folks, the height of the UltraFine Evo 6K could cause some significant ergonomic problems because of how high up the minimum height is. It does have a VESA mount, so you can avoid all these problems by using a monitor arm. While I don’t like their placement, the ports themselves are powerful. You get the latest standards and speeds, including DisplayPort 2.1, HDMI 2.1, and two Thunderbolt 5 ports. There's a built-inKVM switchfor using the same monitor and peripherals with multiple devices. The UltraFine Evo 6K lacks a few ports that other high-end monitors include, such as a headphone jack, Ethernet jack, or upstream USB-A ports. One of the Thunderbolt ports supports power delivery, although only up to 96 watts. This is less than the 240 watts that's possible withThunderbolt 5. A high-powered laptop like my 16-inch M4 Pro MacBook Pro couldn't hold a charge when plugged in. However, youcandaisy chain multiple 6K monitors together using just a single cable, which feels impossibly great.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "When \"likers'' go private: Engagement with reputationally risky content on X", "url": "https://arxiv.org/abs/2601.11140", "content": "When \"likers'' go private: Engagement with reputationally risky content on X. arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community?Learn more about arXivLabs. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IP over Avian Carriers with Quality of Service (1999)", "url": "https://www.rfc-editor.org/rfc/rfc2549.html", "content": "IP over Avian Carriers with Quality of Service (1999)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Scaling long-running autonomous coding", "url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/", "content": "Scaling long-running autonomous coding. Scaling long-running autonomous coding. Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents: This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens. They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not. In my predictions for 2026the other dayI said that by 2029: I think somebody will have built a full web browser mostly using AI assistance, and it won’t even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it’ll get so much easier. I may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach: To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explorethe source code on GitHub. But how well did they do? Their initial announcement a couple of days ago was met withunsurprising skepticism, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo. It looks like they addressed that within the past 24 hours. Thelatest READMEincludes build instructions which I followed on macOS like this: This got me a working browser window! Here are screenshots I took of google.com and my own website:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Level S4 solar radiation event", "url": "https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026", "content": "Level S4 solar radiation event. G4 Levels were first reached at 2:38pm EST (1938 UTC) on 19 January, 2026 upon CME shock arrival. CME passage is expected to continue through the evening with G4 levels remaining possible.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Aventos – An experiment in cheap AI SEO", "url": "https://www.aventos.dev/", "content": "Show HN: Aventos – An experiment in cheap AI SEO", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Overcomplexity of the Shadcn Radio Button", "url": "https://paulmakeswebsites.com/writing/shadcn-radio-button/", "content": "The Overcomplexity of the Shadcn Radio Button. The other day I was asked to update the visual design of radio buttons in a web\napp at work. I figured it couldn't be that complicated. It's just a radio button\nright? Boom! Done. Radio buttons are a built-in HTML element. They've been around for\n30 years. The browser makes it easy. Time for a coffee. I dug into our codebase and realized we were using two React components fromShadcnto power our radio buttons:<RadioGroup>and<RadioGroupItem>. For those unfamiliar with Shadcn, it's a UI framework that provides a bunch of\nprebuilt UI components for use in your websites. Unlike traditional UI\nframeworks like Bootstrap, you don't import it with a script tag ornpm install. Instead you run a command that copies the components into your\ncodebase. Here's the code that was exported from Shadcn into our project: Woof... 3 imports and 45 lines of code. And it's importing a third party icon\nlibrary just to render a circle. (Who needs CSSborder-radiusor the SVG<circle>element when you can add a third party dependency instead?) All of the styling is done by the 30 different Tailwind classes in the markup. I\nshould probably just tweak those to fix the styling issues. But now I'm distracted, annoyed, and curious. Where's the actual<input>?\nWhat's the point of all this? Let's dig a little deeper. The Shadcn components import components from another library called Radix. For\nthose unfamiliar with Radix, it's a UI framework that provides a bunch of\nprebuilt UI components... Wait a second! Isn't that what I just said about Shadcn? What gives? Why do we\nneed both? Let's see what the Radix docs say:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "url": "https://github.com/majcheradam/ocrbase", "content": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Reliable Signals of Honest Intent", "url": "https://zanlib.dev/blog/reliable-signals-of-honest-intent/", "content": "Reliable Signals of Honest Intent. Imagine you are working on a significant software update. The software is used by a few thousand professionals, and the release you are working on will fix a few significant issues and greatly improve the quality of work of these people. The update costs money, so it cannot be an automated thing: you need to convince the users of this software to decide that they should update. How would you do it? Would you implement a pop-up window advertising the new release and push it through the automated update channel? Would you write an email? Would youpromptAIfor an email? A story like this was related in Rory Sutherland’s bookAlchemy, and concerned the release of the then-new WindowsNT32-bit server operating system. Microsoft had to figure out how to convince its existing user-base of system administrators to make the switch. But they didn’t ask developers for advice, they hired an advertising agency. And the advertising agency had a different idea. We produced an elaborate box containing a variety of bits and pieces including a free mouse-mat and a pen, inside gratuitously expensive packaging.1Marginnote alchemy1Sutherland, R.Alchemy: The Surprising Power of Ideas That Don’t Make Sense. W. H. Allen 2020; p. 177↩ Why go to such great lengths to advertise a software update? Well, because this story is a subtle case of obliviousness that developers are often guilty of. We tend to get blindsided by thesubject-objectsplit, and think that we only need to convey the objective fact while leaving the subjective interpretation of the value of that fact up to the reader. But it is perhaps not shocking that the subjective perception of value can be influenced—this is, after all, what persuasion is for. It’s actually very natural and expected for it to be influenced, because we are constantly being bombarded by various stimuli. The stimuli fight for our attention, and we have developed a system of complex intuitions to select which stimuli are worth our attention and which are better to ignore. The elaborate box with expensive packaging was a way to signal that what was inside was important, and exclusive enough, to warrant the attention of the recipients. It turned out that it was an unexpected success: almost all of the boxes were opened, and about 10% of the recipients actually tried the new operating system, which for an audience of experienced server administrators is an impressive conversion rate. This is one of many examples fromAlchemyof what Sutherland calls “reliable signals of honest intent.” The attention economy is very asymmetric—there are a lot of things competing for our attention, and we have to spend it to assess which are worth it. Reliable signals of honest intent are one way to quickly judge whether what is behind the signal is worth our time. What’s the equivalent of that expensive box when reading something on the internet? What proves to a reader that you deemed this interaction worth his time?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The microstructure of wealth transfer in prediction markets", "url": "https://www.jbecker.dev/research/prediction-market-microstructure", "content": "The microstructure of wealth transfer in prediction markets.  Slot machines on the Las Vegas Strip return about 93 cents on the dollar. This is widely considered some of the worst odds in gambling. Yet on Kalshi, a CFTC-regulated prediction market, traders have wagered vast sums on longshot contracts with historical returns as low as 43 cents on the dollar. Thousands of participants are voluntarily accepting expected values far lower than a casino slot machine to bet on their convictions. Theefficient market hypothesissuggests that asset prices should perfectly aggregate all available information. Prediction markets theoretically provide the purest test of this theory. Unlike equities, there is no ambiguity about intrinsic value. A contract either pays $1 or it does not. A price of 5 cents should imply exactly a 5% probability. We analyzed72.1 million tradescovering$18.26 billionin volume to test this efficiency. Our findings suggest that collective accuracy relies less on rational actors than on a mechanism for harvesting error. We document a systematic wealth transfer where impulsiveTakerspay a structural premium for affirmative \"YES\" outcomes whileMakerscapture an \"Optimism Tax\" simply by selling into this biased flow. The effect is strongest in high-engagement categories like Sports and Entertainment, while low-engagement categories like Finance approach perfect efficiency. This paper makes three contributions. First, it confirms the presence of the longshot bias on Kalshi and quantifies its magnitude across price levels. Second, it decomposes returns by market role, revealing a persistent wealth transfer from takers to makers driven by asymmetric order flow. Third, it identifies a YES/NO asymmetry where takers disproportionately favor affirmative bets at longshot prices, exacerbating their losses. Prediction markets are exchanges where participants trade binary contracts on real-world outcomes. These contracts settle at either $1 or $0, with prices ranging from 1 to 99 cents serving as probability proxies. Unlike equity markets, prediction markets are strictly zero-sum: every dollar of profit corresponds exactly to a dollar of loss. Kalshilaunched in 2021 as the first U.S. prediction market regulated by the CFTC. Initially focused on economic and weather data, the platform stayed niche until 2024. Alegal victoryover the CFTC secured the right to list political contracts, and the 2024 election cycle triggered explosive growth. Sports markets, introduced in 2025, now dominate trading activity. Volume distribution across categories is highly uneven. Sports accounts for 72% of notional volume, followed by politics at 13% and crypto at 5%. Note:Data collection concluded on 2025-11-25 at 17:00 ET; Q4 2025 figures are incomplete. The dataset,available on GitHub, contains7.68 million marketsand72.1 million trades. Each trade records the execution price (1-99 cents), taker side (yes/no), contract count, and timestamp. Markets include resolution outcome and category classification.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Engineering as Humanity's Highest Achievement", "url": "https://walkingtheworld.substack.com/p/engineering-as-humanitys-highest", "content": "Engineering as Humanity's Highest Achievement. I’m readingA Culture of Growthby Joel Mokyr, a book I started beforewalking Surrey, England, because I’ve always been intrigued by the question of why some countries become rich while others don’t. Why the Industrial Revolution happened in England rather than China or Germany is one of those questions that has launched a thousand books, careers, and theories, and Dr. Mokyr’s answer is (oversimplifying): England had a foundational belief in the abilities of man to shape their world, as a result of the Enlightenment, which enabled the creation of laws, institutions, and businesses focused on bettering society through innovation, technology, and economic progress. That idea that man could, and should, shape the world to achieve sustained and substantial economic growth was a substantial shift in thought. Prior to the Enlightenment, most of the world had an “Ecclesiastes view of history,” which saw long-term change as neither possible (”there is nothing new under the sun”) nor good, since it leads to sinful riches. This change, which was fomented among then obscure intellectuals questioning the dominant Catholic view of the world, was necessary, and in the end sufficient, for the Industrial Revolution. Our modern world of immense wealth, technology, relative secularism, and intellectual hubris, is the end result of that, and that,as I argued last week, is in totality, a good thing.1 Mokyr, like me, believes that culture2plays a primary role in a nation's development, more than its tangible assets such as resources and geography. Institutions are, in this view, downstream, and while they influence significantly how a nation evolves, including its intellectual life, they are physical manifestations of a people’s beliefs, which precede them. I side with his thesis, which while not caustically anti-religion, does believe that the church needed to be first culturally defeated for the Industrial Revolution to have happened, for the sciences to become dominant, and for our modern world of immense material wealth to emerge. Despite my positive view about faith and my concern over the hubris of the sciences, I don’t have an issue with that.  A successful society (which fulfills the needs of the majority of its citizens) needs both science and religion, and giving either a cultural monopoly, as the church once had and arguably the sciences do now, is the problem. You need both in balance, with the sciences for material comfort and faith as the spiritual salve, as well as for addressing foundational questions, such as what is the whole point of this. To borrow from Augustine, you need technology for the City of Man to thrive and a faith for the City of God. Science to keep mankind moving forward, and religion as horizon for where we are going, as well as reminder that no matter how far we go, we will always fall short. That is why I believe that engineering is a nobler pursuit than the pure sciences, because its focus is entirely on improving the City of Man, and in terms of what has delivered real improvements to our lives, it trumps the sexier fields such as particle theory, or cosmology, or pure math. There is, at least among theorists when I was in grad school, a stigma attached to engineering, which is seen as nothing more than dressed-up auto mechanics and carpentry, but building the messy infrastructure of modern living and having the audacity to dream of projects such as tunneling beneath oceans and constructing canals between seas at different elevations, then actually going out and doing it, is human hubris at its best.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Avoiding fan traps in database design and system diagrams", "url": "https://www.ilograph.com/blog/posts/avoid-fan-traps-in-system-diagrams/", "content": "Avoiding fan traps in database design and system diagrams. A fan trap in data modeling occurs when multiple 1:N relations are joined on the “1” side, resulting in information loss. To the uninitiated, this is easiest understood by example: imagine a university with manycolleges, each with manydepartments, which in turn have manyprofessors. If modeled incorrectly, where theprofessorsare given 1:N relations withcollegesinstead ofdepartments, the result is a fan trap: These relations aren’t wrong per se, but the mapping ofprofessorstodepartmentsis lost. The resulting data modeling diagram looks like two hand fans joined at the narrow end, hence the name. A similar problem can occur in system diagramming. When diagramming relations between resources in a system, information can similarly be lost when relations flow through an intermediary resource. In this article, we’ll look at a couple of examples of this problem and three potential fixes. Fan traps are common problems when diagramming event-driven architectures. The defining characteristic of such architectures is that resources communicate viaevents. Events are typically routed through an event broker, which temporarily stores them until they are ready for consumption. This architecture has the added benefit of decoupling the resources, since they no longer communicate directly. When diagrammed, a (highly simplified) event-driven system might look like so: The similarity to fan traps in data modeling should be evident at a glance. The relationships between the message-producing resources (on the left) and message-consuming resources (on the right) are lost because they collapse at the center of the “fan” (the event broker). The diagram implies each producer communicates with all of the consumers, even though this may not be the case: The same problem can emerge when diagramming communication paths in a network: In the (again highly simplified) networking diagram above, node-to-node communications across the network fan out and back in through firewalls, resulting in specific communication paths being lost. One potential solution is to add smaller, discrete resources within the intermediate resource that the edge resources can connect through. In the event-driven architecture example above, adding “topics” allows us to differentiate the communications going through the event broker: With the addition of topics, the individual communication paths can now be discerned:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Does Calendar-Based Time-Intelligence Change Custom Logic?", "url": "https://towardsdatascience.com/does-calendar-based-time-intelligence-change-custom-logic/", "content": "Does Calendar-Based Time-Intelligence Change Custom Logic?. Let's look at calculating the moving average over time With the advent ofcalendar-based Time Intelligence, the need for custom Time Intelligence logic has decreased dramatically. Now, we can create custom calendars to meet our Time Intelligence calculation needs. You might have read my article about advanced Time Intelligence: https://towardsdatascience.com/advanced-time-intelligence-in-dax-with-performance-in-mind/ Most of the custom logic is no longer needed. But we still have scenarios where we must have custom calculations, like running average. Some time ago, SQLBIwrote an articleabout calculating the running average. This piece uses the same principles described there in a slightly different approach. Let’s see how we can calculate the running average over three months by using the new Calendars.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:40.528701", "author": null, "score": null}
{"title": "How to Perform Large Code Refactors in Cursor", "url": "https://towardsdatascience.com/how-to-perform-large-code-refactors-in-cursor/", "content": "How to Perform Large Code Refactors in Cursor. Learn how to perform code refactoring with LLMs Refactoring codehas historically been a tedious yet important task. Refactoring is the work of taking some piece of code and cleaning it up, either by better separation of concerns, the Don’t Repeat Yourself (DRY) principle, or other code hygiene principles. Code refactors have always been important, but with the release of coding agents, we’re seeing higher coding outputs, which inevitably leads to more need for code refactoring. I more and more often find myself in situations where some code needs to be refactored, though I don’t think this is a warning sign, considering the amount of code I output is also significantly higher now with the help of LLMs. Luckily, the effort to refactor code has significantly gone down since the release of LLMs. In this article, I’ll go through my high-level approach to performing code refactoring using coding agents like Cursor or Claude Code. I’ll cover my generic approach and thought process, so the model you utilize doesn’t matter. You should perform code refactoring whenever you notice a lot of antipatterns in your code, or when you notice you (or your coding agent) is spending more time than should be needed on an implementation. Your threshold before performing a refactoring should also be lower than it was before the release of LLMs, considering refactors are way easier and faster to implement now using coding agents. This is because part of the training set for coding agents is to refactor code, and they’re especially good at that. In a lot of cases, I would even say they’re better than humans, considering refactoring requires a lot of working memory: Thus, you should perform code refactoring when: And you should perform code refactorings because: In this section, I’ll cover my high-level approach to refactoring code. I’ll go through four steps:", "source": "TowardsDataScience", "date": "2026-01-21T11:55:41.752229", "author": null, "score": null}
{"title": "You Probably Don’t  Need a Vector Database for Your RAG — Yet", "url": "https://towardsdatascience.com/you-probably-dont-need-a-vector-database-for-your-rag-yet/", "content": "You Probably Don’t  Need a Vector Database for Your RAG — Yet. Numpy or SciKit-Learn might meet all your retrieval needs Right now, off the back of Retrieval Augmented Generation (RAG), vector databases are getting a lot of attention in the AI world. Many people say you need tools like Pinecone, Weaviate, Milvus, or Qdrant to build a RAG system and manage your embeddings. If you are working on enterprise applications with hundreds of millions of vectors, then tools like these are essential. They let you perform CRUD operations, filter by metadata, and use disk-based indexing that goes beyond your computer’s memory. But for most internal tools, documentation bots, or MVP agents, adding a dedicated vector database might be overkill. It increases complexity, network delays, adds serialisation costs, and makes things more complicated to manage. The truth is that “Vector Search” (i.e the Retrieval part of RAG) is just matrix multiplication. And Python already has some of the world’s best tools for that. In this article, we’ll show how to build a production-readyretrieval componentof a RAG pipeline for small-to-medium data volumes using only NumPy and SciKit-Learn. You’ll see that it’s possible to search millions of text strings in milliseconds, all in memory and without any external dependencies. Typically, RAG involves four main steps: Steps 1 and 4 rely on large language models. Steps 2 and 3 are the domain of the Vector DB. We will concentrate on parts 2 and 3 and how we avoid using vector DBs entirely. But when we’re searching our vector database, what actually is “closeness”? Usually, it isCosine Similarity. If your two vectors are normalised to have a magnitude of 1, then cosine similarity is just the dot product of the two. If you have a one-dimensional query vector of size N, Q(1xN), and a database of document vectors of size M by N, D(MxN), finding the best matches is not a database query; it is a matrix multiplication operation, the dot product of D with the transpose of Q.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:43.340361", "author": null, "score": null}
{"title": "Why Package Installs Are Slow (And How to Fix It)", "url": "https://towardsdatascience.com/why-package-installs-are-slow-and-how-to-fix-it/", "content": "Why Package Installs Are Slow (And How to Fix It). How sharded indexing patterns solve a scaling problem in package management Every developerknows the wait. You type an install command and watch the cursor blink. The package manager churns through its index. Seconds stretch. You wonder if something broke. This delay has a specific cause: metadata bloat. Many package managers maintain a monolithic index of every available package, version, and dependency. As ecosystems grow, these indexes grow with them. Conda-forge surpasses 31,000 packages across multiple platforms and architectures. Other ecosystems face similar scale challenges with hundreds of thousands of packages. When package managers use monolithic indexes, your client downloads and parses the entire thing for every operation. You fetch metadata for packages you will never use. The problem compounds: more packages mean larger indexes, slower downloads, higher memory consumption, and unpredictable build times. This is not unique to any single package manager. It is a scaling problem that affects any package ecosystem serving thousands of packages to millions of users. Conda-forge, like some package managers, distributes its index as a single file. This design has advantages: the solver gets all the information it needs upfront in a single request, enabling efficient dependency resolution without round-trip delays. When ecosystems were small, a 5 MB index downloaded in seconds and parsed with minimal memory. At scale, the design breaks down. Consider conda-forge, one of the largest community-driven package channels for scientific Python. Its repodata.json file, which contains metadata for all available packages, exceeds 47 MB compressed (363 MB uncompressed). Every environment operation requires parsing this file. When any package in the channel changes – which happens frequently with new builds – the entire file must be re-downloaded. A single new package version invalidates your entire cache. Users re-download 47+ MB to get access to one update. The consequences are measurable: multi-second fetch times on fast connections, minutes on slower networks, memory spikes parsing the 363 MB JSON file, and CI pipelines that spend more time on dependency resolution than actual builds. The solution borrows from database architecture. Instead of one monolithic index, you split metadata into many small pieces. Each package gets its own “shard” containing only its metadata. Clients fetch the shards they need and ignore the rest.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:44.657279", "author": null, "score": null}
{"title": "Bridging the Gap Between Research and Readability with Marco Hening Tallarico", "url": "https://towardsdatascience.com/bridging-the-gap-between-research-and-readability-with-marco-hening-tallarico/", "content": "Bridging the Gap Between Research and Readability with Marco Hening Tallarico. Diluting complex research, spotting silent data leaks, and why the best way to learn is often backwards. In the Author Spotlight series, TDS Editors chat with members of our community about their career path in data science and AI, their writing, and their sources of inspiration. Today, we’re thrilled to share our conversation withMarco Hening Tallarico. Marco is a graduate student at the University of Toronto and a researcher for Risklab, with a deep interest in applied statistics and machine learning. Born in Brazil and having grown up in Canada, Marco appreciates the universal language of mathematics. What motivates you to take dense academic concepts (like Stochastic Differential Equations) and turn them into accessible tutorials for the broader TDS community? It’s natural to want to learn everything in its natural order. Algebra, calculus, statistics, etc. But if you want to make fast progress, you have to abandon this inclination. When you’re trying to solve a maze, it’s cheating to pick a place in the middle, but in learning, there is no rule. Start at the end and work your way back if you like. It makes it less tedious. YourData Science Challengearticle focused on spotting data leakage in code rather than just theory. In your experience, which silent leak is the most common one that still makes it into production systems today? It’s really easy to let data leakage seep in during data analysis, or when using aggregates as inputs to the model. Especially now that aggregates can be computed in real time relatively easily. Before graphing, before even running the.head()function, I think it’s important to make the train-test split. Think about how the split should be made, from user level, size, and chronology to a stratified split: there are many choices you can make, and it’s worth taking the time. Also, when using metrics like average users per month, you need to double-check that the aggregate wasn’t calculated during the month you’re using as your testing set. These are trickier, as they are indirect. It’s not always as obvious as not using black-box data when you’re trying to predict what planes will crash. If you have the black box, it’s not a prediction; the plane did crash. You mention thatlearning grammar from data alone is computationally costly. Do you believe hybrid models (statistical + formal) are the only way to achieve sustainable AI scaling in the long run? If we take LLMs for example, there are a lot of easy tasks that they struggle with, like adding a list of numbers or turning a page of text into uppercase. It’s not unreasonable to think that just making the model larger will solve these problems but it’s not a good solution. It’s a lot more reliable to have it invoke a.sum()or.upper()function on your behalf and use its language reasoning to select inputs. This is likely what the major AI models are already doing with clever prompt engineering.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:45.977492", "author": null, "score": null}
{"title": "Using Local LLMs to Discover High-Performance Algorithms", "url": "https://towardsdatascience.com/using-local-llms-to-discover-high-performance-algorithms/", "content": "Using Local LLMs to Discover High-Performance Algorithms. How I used open-source models to explore new frontiers in efficient code generation, using my MacBook and local LLMs Ever since I was a child, I’ve been fascinated by drawing. What struck me was not only the drawing act itself, but also the idea that every drawing could be improved more and more. I remember reaching very high levels with my drawing style. However, once I reached the peak of perfection, I would try to see how I could improve the drawing even further – alas, with disastrous results. From there I always keep in mind the same mantra: “refine and iterate and you’ll reach perfection”. At university, my approach was to read books many times, expanding my knowledge searching for other sources, for finding hidden layers of meaning in each concept. Today, I apply this same philosophy to AI/ML and coding. We know that matrix multiplication (matmul for simplicity here), is the core part of any AI process. Back in the past I developedLLM.rust, a Rust mirror of Karpathy’sLLM.c. The hardest point in the Rust implementation has been the matrix multiplication. Since we have to perform thousands of iterations for fine-tuning a GPT-based model, we need an efficient matmul operation. For this purpose, I had to use the BLAS library, implementing anunsafestrategy for overcoming the limits and barriers. The usage ofunsafein Rust is against Rust’s philosophy, that’s why I am always looking for safer methods for improve matmul in this context. So, taking inspiration from Sam Altman’s statement – “ask GPT how to create value” – I decided to ask local LLMs to generate, benchmark, and iterate on their own algorithms to create a better, native Rust matmul implementation. The challenge has some constraints: I know that achieving BLAS-level performances with this method is almost impossible, but I want to highlight how we can leverage AI for custom needs, even with our “tiny” laptops, so that we can unblock ideas and push boundaries in any field. This post wants to be an inspiration for practitioners, and people who want to get more familiar with Microsoft Autogen, and local LLM deployment. All the cod implementation can be found in thisGithub repo. This is an on-going experiment, and many changes/improvements will be committed. The overall idea is to have a roundtable of agents. The starting point is theMrAderMacher Mixtral 8x7B model Q4 K_Mlocal model. From the model we create 5 entities: The overall workflow can be orchestrated through Microsoft Autogen as depicted in fig.1.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:47.211075", "author": null, "score": null}
{"title": "Time Series Isn’t Enough: How Graph Neural Networks Change Demand Forecasting", "url": "https://towardsdatascience.com/time-series-isnt-enough-how-graph-neural-networks-change-demand-forecasting/", "content": "Time Series Isn’t Enough: How Graph Neural Networks Change Demand Forecasting. Why modeling SKUs as a network reveals what traditional forecasts miss Demand forecastingin supply-chain planning has traditionally been treated as a time-series problem. And yet, despite increasingly sophisticated models, the usual problems persist: The issue is that demand in a supply chain is not independent. It is networked. As an example, this is what just 12 SKUs from a typical supply chain look like when you map their shared plants, product groups, subgroups, and storage locations. So when demand shifts in one corner of the network, the effects are felt throughout the network. In this article, we step outside the model-first thinking and look at the problem the way a supply chain actually behaves — as a connected operational system. Using a real FMCG dataset, we show why even a simplegraph-based neural network(GNN)fundamentally outperforms traditional approaches, and what that means for both business leaders and data scientists. We tested this idea on a real FMCG dataset (SupplyGraph) that combines two views of the business: The dataset has 40 active SKUs, 9 plants, 21 product groups, 36 sub-groups and 13 storage locations. On average, each SKU has~41 edge connections, implying a densely connected graph where most SKUs are linked to many others through shared plants or product groups.. From a planning standpoint, this network encodes institutional knowledge that often lives only in planners’ heads: “If this SKU spikes, these others will feel it.”", "source": "TowardsDataScience", "date": "2026-01-21T11:55:48.445833", "author": null, "score": null}
{"title": "The Hidden Opportunity in AI Workflow Automation with n8n for Low-Tech Companies", "url": "https://towardsdatascience.com/the-hidden-opportunity-in-ai-workflow-automation-with-n8n-for-low-tech-companies/", "content": "The Hidden Opportunity in AI Workflow Automation with n8n for Low-Tech Companies. How to use n8n with multimodal AI and optimisation tools to help companies with low data maturity accelerate their digital transformation. Every day on professionalsocial media, someone claims their “AI agent” will run your entire business while you sleep.It is as if they can deploy AGI across factories, finance teams, and customer service using their “secret” n8n template. My reality check is that many companies are still struggling to collect and harmonise data to follow basic performance metrics. Logistics Director: “I don’t even know how many orders have been delivered late, what do you think your AI agent can do?” And these advertised AI workflows, which are often not ready for production, can unfortunately do nothing to help with that. Therefore, I adopt a more pragmatic approach for our supply chain projects. Instead of promising an AGI that will run your entire logistics operations, let us start with local issues hurting a specific process. Logistics Director: “I want our operators to get rid of papers and pens for order preparation and inventory cycle count.” Most of the time, it involves data extraction, repetitive data entry, and heavy admin work using manual processes that are inefficient and lack traceability. For example, a customer was using paper-based processes to organise inventory cycle counts in its warehouse.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:49.741785", "author": null, "score": null}
{"title": "Why Healthcare Leads in Knowledge Graphs", "url": "https://towardsdatascience.com/why-healthcare-leads-in-knowledge-graphs/", "content": "Why Healthcare Leads in Knowledge Graphs. How science, regulation, collaboration, and public funding shaped the world’s most mature semantic infrastructure Note 1: This post is part 2 of a three-part series on healthcare, knowledge graphs, and lessons for other industries. Part 1, “What Is a Knowledge Graph — and Why It Matters” is availablehere. Note 2: All images by author In Part 1, we described how structured knowledge enabled healthcare’s progress. This article examines why healthcare, more than any other industry, was able to build that structure at scale. Healthcare is the most mature industry in the use of knowledge graphs for a few fundamental reasons. At its core, medicine is grounded in empirical science (biology, chemistry, pharmacology) which makes it possible to establish a shared understanding of the types of things that exist, how they interact, and causality. In other words, healthcare lends itself naturally toontology. The industry also benefits from a deep culture of sharedcontrolled vocabularies. Scientists and clinicians are natural librarians. By necessity, they meticulously list and categorize everything they can find, from genes to diseases. This emphasis on classification is reinforced by a commitment to empirical, reproducibleobservation, where data must be comparable across institutions, studies, and time. Finally, there are structural forces that have accelerated maturity: strictregulation; strong pre-competitivecollaboration; sustainedpublic funding; andopen data standards. All of these factors incentivize shared standards and reusable knowledge rather than isolated, proprietary models. Together, these factors created the conditions for healthcare to build durable, shared semantic infrastructure—allowing knowledge to accumulate across institutions, generations, and technologies. Humans have always tried to understand how the world works. When we observe and report the same thing repeatedly, and agree that it is true, we develop a shared understanding of reality. This process is formalized in science using the scientific method. Scientists develop a hypothesis, conduct an experiment, and evaluate the results empirically. In this way, humans have been developing an implicit medical ontology for thousands of years. Otzi, the caveman discovered in 1991, who lived 5,300 years ago, was discovered with an antibacterial fungus in his leggings, likely to treat his whipworm infection (Kirsch and Ogas 4). Even cavemen had some understanding that plants could be used to treat ailments.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:50.997851", "author": null, "score": null}
{"title": "Data Poisoning in Machine Learning: Why and How People Manipulate Training Data", "url": "https://towardsdatascience.com/data-poisoning-in-machine-learning-why-and-how-people-manipulate-training-data/", "content": "Data Poisoning in Machine Learning: Why and How People Manipulate Training Data. Do you know where your data has been? Data is a sometimesoverlooked but hugely vital part of enabling machine learning and therefore AI to function. Generative AI companies are scouring the world for more data constantly because this raw material is required in great volumes for models to be built. Anyone who’s building or tuning a model must first collect a significant amount of data to even begin. Some conflicting incentives result from this reality, however. Protecting the quality and authenticity of your data is an important component of security, because these raw materials will make or break the machine learning models you are serving to users or customers. Bad actors can strategically insert, mutate, or remove data from your datasets in ways you may not even notice, but which will systematically alter the behavior of your models. Simultaneously, creators such as artists, musicians, and authors are fighting an ongoing battle againstrampant copyright violation and IP theft, primarily by generative AI companiesthat need to find more data to toss into the voracious maw of the training process. These creators are looking for action they can take to prevent or discourage this theft that doesn’t just require being at the mercy of often slow moving courts. Additionally, as companies do their darndest to replace traditional search engines with AI mediated search, companies whose businesses are founded on being surfaced through search are struggling. How do you access customers and present your desired brand identity to the public if the investments you made in search visibility over past decades are no longer relevant? All three of these cases point us to one concept — “data poisoning”. In short,data poisoning is changing the training data used to produce a machine learning model in some way so that the model behavior is altered.The impact is specific to the training process, so once a model artifact is created, the damage is done. The model will be irreparably biased, potentially to the point of being useless, and the only real solution is retraining with clean data. This phenomenon is a danger for automatic retraining, where human observation is minimal, but also for very well observed training becauseusually the changes to the training data are invisible to the average viewer. For example, in one study cited byHartle et al. (2025)in relation to poisoned medical misinformation data, “Fifteen clinicians were tasked with determining the poisoned response and the baseline response; the reviewers were unable to determine the difference between the two results… When the concept-specific data was poisoned, at 0.001%, there was a 4.8% increase in harmful content.” Attempting to reverse-engineer the poisoned data and remove it has largely not been successful. Techniques under the umbrella of“machine unlearning”have been attempted, but when we can’t detect the problematic data, it’s difficult for these efforts to make progress. Even when we can detect the data, researchers find thatremoving traces from a model’s architecture is not effective at undoing the damage. Data poisoning can take a lot of different forms, so I’m going to work backwards and discuss three specific motives for data poisoning, how they work, and what their results are:", "source": "TowardsDataScience", "date": "2026-01-21T11:55:52.359271", "author": null, "score": null}
{"title": "Terms of Use", "url": "https://towardsdatascience.com/website-terms-of-use/", "content": "Terms of Use. Last Modified: February 7, 2025 Acceptance of the Terms of Use These terms of use are entered into by and between you and Insight Media Group, LLC (“TDS,” “we,” or “us” or “our”). The following terms and conditions, together with any documents they expressly incorporate by reference (collectively, “Terms of Use“), govern your access to and use ofhttps://towardsdatascience.com, including any content, functionality, and services offered on or throughhttps://towardsdatascience.com(the “Website“), whether as a guest or a registered user. Please read these Terms of Use carefully before you start to use the Website. By using the Website or by clicking to accept or agree to the Terms of Use when this option is made available to you, you accept and agree to be bound and abide by these Terms of Use  and our Privacy Policy, found athttps://towardsdatascience.com/privacy-policy/,and incorporated herein by reference. In the event of a conflict between these Terms of Use and the Privacy Policy, the Privacy Policy shall govern. If you do not want to agree to these Terms of Use or the Privacy Policy, you must not access or use the Website. This Website is offered and available to users who are 13 years of age or older. If you are under the age of 13, you may not access or use the Website. Changes to the Terms of Use TDS may revise and update these Terms of Use from time to time in our sole discretion. All changes are effective immediately when we post them and apply to all access to and use of this Website thereafter. Your continued use of the Website following the posting of revised Terms of Use means that you accept and agree to the changes. You are expected to check this page from time to time so you are aware of any changes, as they are binding on you.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:53.584786", "author": null, "score": null}
{"title": "A Geometric Method to Spot Hallucinations Without an LLM Judge", "url": "https://towardsdatascience.com/the-red-bird/", "content": "A Geometric Method to Spot Hallucinations Without an LLM Judge. How geometry shows when LLMs are lying Imagine a flockof birds in flight. There’s no leader. No central command. Each bird aligns with its neighbors—matching direction, adjusting speed, maintaining coherence through purely local coordination. The result is global order emerging from local consistency. Now imagine one bird flying with the same conviction as the others. Its wingbeats are confident. Its speed is correct. But its direction doesn’t match its neighbors. It’s the red bird. It’s not lost. It’s not hesitating. It simply doesn’t belong to the flock. Hallucinations in LLMs are red birds. LLMs generate fluent, confident text that may contain fabricated information. They invent legal cases that don’t exist. They cite papers that were never written. They state facts with the same tone whether those facts are true or completely made up. The standard approach to detecting this is to ask another language model to check the output. LLM-as-judge. You can see the problem immediately: we’re using a system that hallucinates to detect hallucinations. It’s like asking someone who can’t distinguish colors to sort paint samples. They’ll give you an answer. It might even be right sometimes. But they’re not actually seeing what you need them to see. The question we asked was different:can we detect hallucinations from the geometric structure of the text itself, without needing another language model’s opinion? Before getting to the detection method, I want to step back and establish what we’re working with.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:57.549746", "author": null, "score": null}
{"title": "Cutting LLM Memory by 84%: A Deep Dive into Fused Kernels", "url": "https://towardsdatascience.com/cutting-llm-memory-by-84-a-deep-dive-into-fused-kernels/", "content": "Cutting LLM Memory by 84%: A Deep Dive into Fused Kernels. Why your final LLM layer is OOMing and how to fix it with a custom Triton kernel. If you’ve ever trainedor fine-tuned an LLM, you’ve likely hit a wall at the very last step: theCross-Entropy Loss. The culprit is thelogit bottleneck. To predict the next token, we project a hidden state into a massive vocabulary space. For Llama 3 (128,256 tokens), the weight matrix alone is over525 million parameters. While that’s only ~1GB inbfloat16, the intermediate logit tensor is the real issue. For large batches, it can easily exceed80GBof VRAM just to compute a single scalar loss. Optimising this layer is how libraries like Unsloth and Liger-Kernel achieve such massive memory reductions. In this article, we’ll build a fusedLinear + Cross Entropykernel from scratch in Triton. We will derive the math and implement a tiled forward and backward pass that slashes peak memory usage by84%. Note on Performance:This implementation is primarilyeducational. We prioritise mathematical clarity and readable Triton code by using global atomic operations. While it solves the memory bottleneck, matching production-grade speeds would require significantly more complex implementations which are out of scope for this article. This post is part of my Triton series. We’ll be using concepts liketilingandonline softmaxthat we’ve covered previously. If those sound unfamiliar, I recommend catching up there first! To get us started, let’s put some more numbers on the logit bottleneck. We consider an input matrixXwith shape[NxD], a weight matrixWwith shape[DxV]and a logit matrixY=X@Wwith shape[NxV]. In the context of an LLM,Nwould be the sequence length multiplied by the batch size (i.e. the total number of tokens in the batch),Dthe size of the hidden state andVthe vocabulary size. For a Llama3 8B model, we would have a context window of 8192 tokens, a hidden state with 4096 dimensions and a vocabulary size of 128,256 tokens. Using a modest batch size of 8, we getN = 8192x8 = 65,536. This results in theYmatrix having shape[NxV]=[65,536x128,256], or roughly8.4 billionelements. Inbfloat16, this would take up16.8GBof memory. However, if we follow best practices and usefloat32for the loss calculation to ensure numerical stability, the requirements double to33.6GB. To put this number in perspective, we would also need around16GBof memory to hold the weights of Llama3 8B in memory inbfloat16. One most GPUs, this leaves no space for the massive overhead of the optimiser states (e.g.Adam’s moments) and other activations, resulting in the infamous PyTorch OOM error.", "source": "TowardsDataScience", "date": "2026-01-21T11:55:58.792336", "author": null, "score": null}
{"title": "From RGB to Lab: Addressing Color Artifacts in AI Image Compositing", "url": "https://towardsdatascience.com/from-rgb-to-lab-addressing-color-artifacts-in-ai-image-compositing/", "content": "From RGB to Lab: Addressing Color Artifacts in AI Image Compositing. A multi-tier approach to segmentation, color correction, and domain-specific enhancement While backgroundreplacement is a staple of image editing, achieving production-grade results remains a significant challenge for developers. Many existing tools work like “black boxes,” which means we have little control over the balance between quality and speed needed for a real application. I ran into these difficulties while buildingVividFlow. The project is mainly focused on Image-to-Video generation, but it also provides a feature for users to swap backgrounds using AI prompts. To make the system more reliable across different types of images, I ended up focusing on three technical areas that made a significant difference in my results: These are the approaches that worked for me when I deployed the app on HuggingFace Spaces. In this article, I want to share the logic and some of the math behind these choices, and how they helped the system handle the messy variety of real-world images more consistently. Standard RGB alpha blending tends to leave a stubborn visual mess in background replacement. When you blend a portrait shot against a colored wall into a new background, the edge pixels usually hold onto some of that original color. This is most obvious when the original and new backgrounds have contrasting colors, like swapping a warm yellow wall for a cool blue sky. You often end up with an unnatural yellowish tint that immediately gives away the fact that the image is a composite.This is why even when your segmentation mask is pixel-perfect, the final composite still looks obviously fake — the color contamination betrays the edit. The issue is rooted in how RGB blending works. Standard alpha compositing treats each color channel independently, calculating weighted averages without considering how humans actually perceive color.To see this problem concretely, consider the example visualized in Figure 1 below.Take a dark hair pixel (RGB 80, 60, 40) captured against a yellow wall (RGB 200, 180, 120). During the photo shoot, light from that wall reflects onto the hair edges, creating a color cast. If you apply a 50% blend with a new blue background in RGB space, the pixel becomes a muddy average (RGB 140, 120, 80) that preserves obvious traces of the original yellow—exactly the yellowish tint problem we want to eliminate. Instead of a clean transition, this contamination breaks the illusion of natural integration. As demonstrated in the figure above, the middle panel shows how RGB blending produces a muddy result that retains the yellowish tint from the original wall. The rightmost panel reveals the solution: switching to Lab color space before the final blend allows surgical removal of this contamination. Lab space separates lightness (L channel) from chroma (a and b channels), enabling targeted corrections of color casts without disturbing the luminance that defines object edges. The corrected result (RGB 75, 55, 35) achieves natural hair darkness while eliminating yellow influence through vector operations in the ab plane, a mathematical process I’ll detail in Section 5. The background replacement pipeline orchestrates several specialized components in a carefully designed sequence that prioritizes both robustness and efficiency. The architecture ensures that even when individual models encounter challenging scenarios, the system gracefully degrades to alternative approaches while maintaining output quality without wasting GPU resources. Following the architecture diagram, the pipeline executes through six distinct stages: Image Preparation: The system resizes and normalizes input images to a maximum dimension of 1024 pixels, ensuring compatibility with diffusion model architectures while maintaining aspect ratio.", "source": "TowardsDataScience", "date": "2026-01-21T11:56:00.113684", "author": null, "score": null}
{"title": "The Great Data Closure: Why Databricks and Snowflake Are Hitting Their Ceiling", "url": "https://towardsdatascience.com/the-great-data-closure-why-databricks-and-snowflake-are-hitting-their-ceiling/", "content": "The Great Data Closure: Why Databricks and Snowflake Are Hitting Their Ceiling. Acquisitions, venture, and an increasingly competitive landscape all point to a market ceiling How big cana data company really grow? This week what would have been news a year ago was no longer news.Snowflake invested in AtScale, a provider of semantic layer services in a strategic investment in the waning company’s history. An odd move, given the commitment to theopen semantic interchangeor “OSI” (yet another acronym or .yaa) which appears to bemetricflow masqueradingas something else. Meanwhile, Databricks, the AI and Data company,invested in AI-winner and all-round VC paramore Loveable— the rapidly growing vibe-coding company from Sweden. Starting a venture arm is a tried-and-tested route for enterprises. Everybody from Walmart and Hitachi to banks like JPMorgan and Goldman Sachs, and of course the hyperscalers — MSFT, GOOG — themselves have venture arms (though strangelynot AWS). The benefits are clear. An investment into a round can give the right of first refusal. It offers both parties influence around complementary roadmap features as well as clear distribution advantages. “Synergy” is the word used in boardrooms, though it is the less insidious and friendly younger brother ofcentral cost cuttingso prevalent in PE rather than venture-backed businesses. It should therefore come as no surprise to see that Databricks are branching out outside of Data. After all (and Ali has been very open about this), the team understands the way to grow the company is through new use cases, most notably AI. WhileDolly was a flop, the jury is out on thepartnership with OpenAI. AI/BI, as well as Databricks Applications, are promising initiatives designed to bring more friends into the tent — outside of the core SYSADMIN cluster administrators. Snowflake meanwhile may be trying a similar tack but with differing levels of success. Aside from Streamlit, it is not clear what value its acquisitions are truly bringing.Openflow, Neolithic Nifi under-the-hood, is not well received. Rather, it is the internal developments such as the embedding of dbt core into the Snowflake platform that appear to be gaining more traction. In this article, we’ll dive into the different factors at play and make some predictions for 2026. Let’s get stuck in! Databricks has a problem. A big problem. And that is equity.", "source": "TowardsDataScience", "date": "2026-01-21T11:56:01.428459", "author": null, "score": null}
{"title": "TDS Newsletter: Is It Time to Revisit RAG?", "url": "https://towardsdatascience.com/tds-newsletter-is-it-time-to-revisit-rag/", "content": "TDS Newsletter: Is It Time to Revisit RAG?. Let's make sense of the current state of retrieval-augmented generation Never miss a new edition ofThe Variable, our weekly newsletter featuring a top-notch selection of editors’ picks, deep dives, community news, and more. It’s very difficult to tell what phase of the hype cycle we are in for any givenAI tool. Things are moving fast: a concept that just weeks ago seemed cutting edge can now appear stale, while an approach that was headed towards obsolescence might suddenly make a comeback.Retrieval-augmented generation is an interesting case in point. It dominated conversations a couple of years ago, quickly attracted a vocal crowd of skeptics, splintered into multiple types and flavors, and inspired a cottage industry of enhancements. These days, it seems to have landed somewhere midway between exciting and mundane. It’s a technique used by millions of practitioners, but no longer producing endless buzz. To help us make sense of the current state of RAG, we turn to our expert authors, who cover some of its current challenges, use cases, and recent innovations. We begin our exploration withSarah Schürch‘s enlightening and detailed look into chunking—the process of splitting longer documents into shorter, more easily digestible ones—and its potential effects on the retrieval step in your LLM pipelines. Can we apply the power of RAG beyond text? Sara Nobrega introduces us to the emerging idea of retrieval-augmented forecasting for time-series data. How complex should your RAG systemsactuallybe? Ida Silfverskiöld presents her latest testing, aiming to find the right balance between performance, latency, and cost. Catch up with three articles that resonated with a wide audience in the past few days. We hope you explore some of our other recent must-reads on a diverse range of topics.", "source": "TowardsDataScience", "date": "2026-01-21T11:56:02.745433", "author": null, "score": null}
{"title": "When Shapley Values Break: A Guide to Robust Model Explainability", "url": "https://towardsdatascience.com/when-shapley-values-break-a-guide-to-robust-model-explainability/", "content": "When Shapley Values Break: A Guide to Robust Model Explainability. Shapley Values are one of the most common methods for explainability, yet they can be misleading. Discover how to overcome these limitations to achieve better insights. Explainability in AI is essential for gaining trust in model predictions and is highly important for improving model robustness. Good explainability often acts as a debugging tool, revealing flaws in the model training process. While Shapley Values have become the industry standard for this task, we must ask: Do they always work? And critically, where do they fail? To understand where Shapley values fail, the best approach is to control the ground truth. We will start with a simple linear model, and then systematically break down the explanation. By observing how Shapley values react to these controlled changes, we can precisely identify exactly where they yield misleading results and how to fix them. We will start with a model with 100 uniform random variables. In this straightforward example, where all variables are independent, the calculation simplifies dramatically. Recall that the Shapley formula is based on themarginal contributionof each feature, the difference in the model’s output when a variable is added to a coalition of known features versus when it is absent. \\[ V(S∪{i}) – V(S)\\] Since the variables are independent, the specific combination of pre-selected features (S) does not influence the contribution of feature i. The effect of pre-selected and non-selected features cancel each other out during the subtraction, having no impact on the influence of feature i. Thus, the calculation reduces to measuring the marginal effect of feature i directly on the model output: \\[ W_i · X_i \\] The result is both intuitive and works as expected. Because there is no interference from other features, the contribution depends solely on the feature’s weight and its current value. Consequently, the feature with the largest combination of weight and value is the most contributing feature. In our case, feature index 0 has a weight of 10 and a value of 1.", "source": "TowardsDataScience", "date": "2026-01-21T11:56:03.953325", "author": null, "score": null}
{"title": "How to Run Coding Agents in Parallel", "url": "https://towardsdatascience.com/how-to-run-coding-agents-in-parallell/", "content": "How to Run Coding Agents in Parallel. Get the most out of Claude Code In the last few years, coding agents have become more and more prevalent. Initially, coding agents could only auto-complete specific lines of code. We then experienced how agents could interact with a single file and make changes to entire functions. After this, we started seeing agents capable of keeping track of and updating code in multiple files. Now, coding agents are extremely capable and can work across multiple code repositories, even implementing entire features with no need for human intervention. The capabilities of coding agents have opened up a whole new world of productivity for software engineers. In this article, I’ll highlight how coding agents have increased my productivity as an engineer, and how I leverage coding agents maximally by running multiple in parallel. I aim to create a high-level overview of what coding agents can do for you and the techniques I utilize to get the most out of my coding agents by running them in parallel. Just a year ago, it was almost unthinkable that you could be programming on multiple projects at the same time. Programming was known as a very high cognitive effort activity, where you had to minimize context switching. If you want to take full advantage of coding agents, you need to run them in paralell. And if you’re not taking full advantage of coding agents, you’re falling behind I still recommend minimizing context switching. However, the capabilities of coding agents have gotten so far that if you don’t run multiple in parallel, you’re falling behind. When spinning up a coding agent, you usually start it on a task by giving it some directions and asking a few questions. After this, however, the agents start working, and it can take 5-20 minutes before you need to interact with the agent again. Instead of waiting for this long, you spin up another coding agent. You can then continue this cycle of spinning up new agents until you have to interact with the first agent again. Simply put, the reason you should run multiple agents in parallel is that this is the way to achieve maximum effectiveness as a software engineer. You could, for example, look at the creator of Claude Code, Boris Cherny, on X.", "source": "TowardsDataScience", "date": "2026-01-21T11:56:05.201601", "author": null, "score": null}
{"title": "The 2026 Goal Tracker: How I Built a Data-Driven Vision Board Using Python, Streamlit, and Neon", "url": "https://towardsdatascience.com/the-2026-goal-tracker-how-i-built-a-data-driven-vision-board-using-python-streamlit-and-neon/", "content": "The 2026 Goal Tracker: How I Built a Data-Driven Vision Board Using Python, Streamlit, and Neon. Designing a centralized system to track daily habits and long-term goals Have you ever wonderedhow to actually stay consistent with your goals for 2026? This year, I’ve decided that I don’t just want a list of goals. I want a vision board backed by real metrics to track my progress month after month. The problem I’ve been facing these last few years is fragmentation. There are a million apps out there to help you track finance, training, or daily habits, but I could never find a single, centralized tracker. Even harder was finding something that could scale: a system that follows a goal whether it’s daily, weekly, monthly, quarterly or yearly. For this reason, I decided to build my own goal tracker. This app is just one example of what works well for me, but the intention goes beyond this specific implementation. The goal is to share the product thinking behind it: how to design a system that aligns metrics, visuals, and structure in a way that actually supports short and long-term goals. Before jumping into the code, it’s important to understand the design decisions behind the app. In reality, our ambition operates on different scales. Most trackers fail because they focus on a single resolution (often tracking daily habits). In my case, I needed a system that could support different frequencies of goals so i categorized my objectives into 2 categories: The app I designed was meant to capture all of these frequencies in a single system. This makes it possible to monitor execution on a daily basis, but also maintain an overview of progress throughout the whole year. When it came to the interface, I deliberately avoided complexity. I’m not a UI expert, and I didn’t want an app filled with buttons, menus, or unnecessary interactions. Instead, I chose a grid-based matrix. This allows to simply check boxes for habits or completed goals. In data visualization, an empty cell is just as informative as a filled one. Seeing gaps in the grid becomes a powerful and very concrete signal. It immediately shows where consistency is missing and helps adjusting. For this project, I had two important requirements for the architecture:", "source": "TowardsDataScience", "date": "2026-01-21T11:56:06.435624", "author": null, "score": null}
{"title": "Do You Smell That? Hidden Technical Debt in AI Development", "url": "https://towardsdatascience.com/do-you-smell-that-hidden-technical-debt-in-ai-development/", "content": "Do You Smell That? Hidden Technical Debt in AI Development. Why speed without standards creates fragile AI products Not everybody can“smell” them at first. In practice,code smellsare warning signs that suggest future problems. The code may work today, but its structure hints that it will become hard to maintain, test, scale, or secure. Smells arenot necessarily bugs; they’re indicators of design debt and long-term product risk. These smells typically manifest as slower delivery and higher change risk, more frequent regressions and production incidents, and less reliable AI/ML outcomes, often driven by leakage, bias, or drift that undermines evaluation and generalization. Most phases in the development of data/AI products can vary, but they usually follow a similar path. Typically, we start with a prototype: an idea first sketched, followed by a small implementation to demonstrate value. Tools likeStreamlit,Gradio, orn8ncan be used to present a very simple concept usingsynthetic data. In these cases, you avoid using sensitive real data andreduce privacy and security concerns, especially in large,privacy‑sensitive, orhighly regulated companies. Later, you move to the PoC, where you use a sample of real data and go deeper into the features while working closely with the business. After that, you move towardproductization, building anMVPthat evolves as you validate and capture business value. Most of the time, prototypes and PoCs are built quickly, and AI makes it even faster to deliver them. The problem is that this code rarely meets production standards. Before it can be robust, scalable, and secure, it usually needs refactoring acrossengineering(structure, readability, testing, maintainability),security(access control, data protection, compliance), andML/AI quality(evaluation, drift monitoring, reproducibility). This hidden technical debt (often visible as code smells) is easy to overlook when teams chasequick wins, and “vibe coding” can amplify it. As a result, you can run into issues such as: And the list goes on… and on. The problem isn’t that prototypes are bad. The problem is the gap between prototype speed and production responsibility,when teams, for one reason or another,don’t invest in the practices that make systems reliable, secure, and able to evolve. It’s also useful to extend the idea of “code smells” intomodel and pipeline smells: warning signs that the system may be producing confident but misleading results, even when aggregate metrics look great. Common examples includefairness gaps(subgroup error rates are consistently worse),spillover/leakage(evaluation accidentally includes future or relational information that won’t exist at decision time, generating dev/prod mismatch [7]), or/andmulticollinearity(correlated features that make coefficients and explanations unstable). These aren’t academic edge cases; they reliably predict downstream failures like weak generalization, unfair outcomes, untrustworthy interpretations, and painful production drops.", "source": "TowardsDataScience", "date": "2026-01-21T11:56:07.687736", "author": null, "score": null}
{"title": "Terms of Use", "url": "https://towardsdatascience.com/website-terms-of-use/", "content": "Terms of Use. Last Modified: February 7, 2025 Acceptance of the Terms of Use These terms of use are entered into by and between you and Insight Media Group, LLC (“TDS,” “we,” or “us” or “our”). The following terms and conditions, together with any documents they expressly incorporate by reference (collectively, “Terms of Use“), govern your access to and use ofhttps://towardsdatascience.com, including any content, functionality, and services offered on or throughhttps://towardsdatascience.com(the “Website“), whether as a guest or a registered user. Please read these Terms of Use carefully before you start to use the Website. By using the Website or by clicking to accept or agree to the Terms of Use when this option is made available to you, you accept and agree to be bound and abide by these Terms of Use  and our Privacy Policy, found athttps://towardsdatascience.com/privacy-policy/,and incorporated herein by reference. In the event of a conflict between these Terms of Use and the Privacy Policy, the Privacy Policy shall govern. If you do not want to agree to these Terms of Use or the Privacy Policy, you must not access or use the Website. This Website is offered and available to users who are 13 years of age or older. If you are under the age of 13, you may not access or use the Website. Changes to the Terms of Use TDS may revise and update these Terms of Use from time to time in our sole discretion. All changes are effective immediately when we post them and apply to all access to and use of this Website thereafter. Your continued use of the Website following the posting of revised Terms of Use means that you accept and agree to the changes. You are expected to check this page from time to time so you are aware of any changes, as they are binding on you.", "source": "TowardsDataScience", "date": "2026-01-21T11:56:08.996180", "author": null, "score": null}
