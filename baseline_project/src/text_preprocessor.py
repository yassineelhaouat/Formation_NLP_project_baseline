# src/text_preprocessor.py
"""
ğŸ”§ Module PrÃ©traitement NLP

Pipeline complet de nettoyage texte :
- Normalisation (HTML, URLs, whitespace, casse, accents)
- Tokenization (spaCy franÃ§ais)
- Cleaning (stopwords, lemmatization, filtrage tokens)
- Ã‰valuation qualitÃ©

Usage:
    preprocessor = TextPreprocessor(config)
    processed_articles = preprocessor.process_batch(articles)
    metrics = preprocessor.get_quality_metrics()
"""

import re
import spacy
import logging
from typing import List, Dict, Optional
import json
from collections import Counter
import unicodedata

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEXT PREPROCESSOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TextPreprocessor:
    """
    Pipeline NLP franÃ§ais avec justifications
    
    DESIGN DECISIONS (Ã  documenter) :
    
    1. Normalisation casse : minuscules OUI/NON?
       â†’ Choix : Minuscules (rÃ©duit vocabulaire, aide generalisation)
       â†’ MAIS : Conserver acronymes techniques (dÃ©tectÃ© avec regex)
    
    2. Tokenization : spaCy vs NLTK vs regex?
       â†’ Choix : spaCy (support franÃ§ais, POS tagging, speed)
    
    3. Lemmatization : appliquer?
       â†’ Choix : OUI (rÃ©duit bruit, aides clustering)
       â†’ Tradeoff : Perte info fine-grained (ex: "running" â†’ "run")
    
    4. Stopwords : supprimer?
       â†’ Choix : OUI (bruit pour classification)
       â†’ Mais : Documenter impact
    
    5. Accents franÃ§ais : normer?
       â†’ Choix : NON (unicodedata lossless)
    """
    
    def __init__(self, config: Dict):
        """
        Initialiser preprocessor
        
        Args:
            config: Dict de configuration (voir config.json)
        """
        self.config = config
        
        # Charger modÃ¨le spaCy
        lang = config.get('preprocessing', {}).get('language', 'fr')
        model_name = config.get('preprocessing', {}).get('spacy_model', 'fr_core_news_sm')
        
        try:
            self.nlp = spacy.load(model_name)
            logger.info(f"âœ… ModÃ¨le spaCy chargÃ©: {model_name}")
        except OSError:
            logger.error(f"âŒ ModÃ¨le {model_name} non trouvÃ©. Installez avec :")
            logger.error(f"   python -m spacy download {model_name}")
            raise
        
        # Stopwords franÃ§ais
        self.french_stopwords = self._load_french_stopwords()
        
        # MÃ©triques qualitÃ©
        self.quality_metrics = {}
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 1 : NORMALISATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def normalize(self, text: str) -> str:
        """
        Normaliser texte brut
        
        JUSTIFICATIONS :
        - Supprimer tags HTML (contenu non-pertinent)
        - Remplacer URLs (ne contribuent pas au sens)
        - Normaliser whitespace (aide tokenizer)
        - Lowercase (rÃ©duit vocabulaire, mais voir note casse)
        - PRESERVER accents franÃ§ais
        """
        if not text:
            return ""
        
        # 1. Supprimer tags HTML
        text = re.sub(r'<[^>]+>', '', text)
        
        # 2. Remplacer URLs par token
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text)
        
        # 3. Remplacer mentions Twitter
        text = re.sub(r'@\w+', '<MENTION>', text)
        
        # 4. Remplacer hashtags
        text = re.sub(r'#\w+', '<HASHTAG>', text)
        
        # 5. Normaliser whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # 6. Conversion minuscules (MAIS: conserver acronymes?)
        # Pour ce baseline, on garde minuscules simples
        # Les acronymes seront dÃ©tectÃ©s dans NER
        text = text.lower()
        
        # 7. Supprimer caractÃ¨res spÃ©ciaux extrÃªmes (mais pas accents)
        # Garder : lettres, chiffres, accents, espaces, ponctuation basique
        text = re.sub(r'[^\w\s\-\.\'Ã Ã¢Ã¤Ã¦Ã§Ã©Ã¨ÃªÃ«Ã¬Ã®Ã¯Ã²Ã´Ã¶Å“Ã¹Ã»Ã¼Å“Ã¿Ã±]', '', text)
        
        # 8. Supprimer espaces inutiles
        text = text.strip()
        
        return text
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 2 : TOKENIZATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def tokenize(self, text: str) -> List[str]:
        """
        Tokenizer avec spaCy
        
        JUSTIFICATIONS :
        - spaCy: Support franÃ§ais, POS tagging, pipeline modulaire
        - Alternatives:
          * NLTK: Plus flexible mais lent, moins support franÃ§ais
          * Regex: Rapide mais fragile sur edge cases
        """
        if not text:
            return []
        
        try:
            doc = self.nlp(text)
            tokens = [token.text for token in doc]
            return tokens
        except Exception as e:
            logger.warning(f"Erreur tokenization: {str(e)}")
            # Fallback: split simple
            return text.split()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 3 : CLEANING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def clean(self, text: str) -> List[str]:
        """
        Tokenize + clean
        
        Ã‰tapes :
        1. Tokenization
        2. Suppression stopwords franÃ§ais
        3. Suppression tokens trÃ¨s courts
        4. Lemmatization via spaCy
        
        JUSTIFICATIONS :
        - Stopwords: BERT/transformers gÃ¨rent bien, mais rÃ©duit bruit
        - Min length: Tokens 1-char peu informatifs
        - Lemmatization: RÃ©duit sparsitÃ© (runningâ†’run)
        """
        if not text:
            return []
        
        try:
            doc = self.nlp(text)
            
            tokens = []
            for token in doc:
                # 1. Skip stopwords
                if token.is_stop:
                    continue
                
                # 2. Skip tokens trop courts
                if len(token.text) < self.config.get('preprocessing', {}).get('min_token_length', 2):
                    continue
                
                # 3. Skip punctuation/numbers uniquement
                if token.is_punct:
                    continue
                
                # 4. Lemmatization
                lemma = token.lemma_
                tokens.append(lemma)
            
            return tokens
        
        except Exception as e:
            logger.warning(f"Erreur cleaning: {str(e)}")
            return []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ã‰TAPE 4 : PROCESS COMPLET
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def process(self, text: str, include_metrics: bool = False) -> Dict:
        """
        Pipeline complet
        
        Returns:
            {
                'original': texte original,
                'normalized': texte normalisÃ©,
                'tokens': liste tokens nettoyÃ©s,
                'num_tokens_original': compte tokens avant,
                'num_tokens_final': compte tokens aprÃ¨s,
                'token_loss_pct': % tokens perdus
            }
        """
        # Ã‰tape 1: Normalisation
        normalized = self.normalize(text)
        
        # Ã‰tape 2: Tokenization brut
        tokens_raw = self.tokenize(normalized)
        
        # Ã‰tape 3: Cleaning
        tokens_clean = self.clean(normalized)
        
        # Calculer metrics
        num_original = len(tokens_raw) if tokens_raw else 0
        num_final = len(tokens_clean) if tokens_clean else 0
        token_loss_pct = (1 - num_final / max(num_original, 1)) * 100
        
        return {
            'original': text[:100],  # Garder dÃ©but original
            'normalized': normalized[:100],
            'tokens': tokens_clean,
            'num_tokens_original': num_original,
            'num_tokens_final': num_final,
            'token_loss_pct': round(token_loss_pct, 2)
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PROCESS BATCH
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def process_batch(self, articles: List) -> List[Dict]:
        """
        Traiter batch d'articles
        
        OptimisÃ© avec nlp.pipe() pour speed
        """
        logger.info(f"ğŸ”§ Preprocessing {len(articles)} articles...")
        
        processed = []
        token_losses = []
        
        for i, article in enumerate(articles):
            try:
                result = self.process(article.content or article.title)
                
                # Enrichir article
                article_dict = article.to_dict()
                article_dict['tokens'] = result['tokens']
                article_dict['num_tokens'] = result['num_tokens_final']
                article_dict['token_loss_pct'] = result['token_loss_pct']
                article_dict['normalized_content'] = result['normalized']
                
                processed.append(article_dict)
                token_losses.append(result['token_loss_pct'])
                
                if (i + 1) % 20 == 0:
                    logger.info(f"  âœ“ {i + 1}/{len(articles)} articles")
            
            except Exception as e:
                logger.warning(f"  âœ— Article {i}: {str(e)}")
                continue
        
        # Sauvegarder metrics
        self.quality_metrics = {
            'num_articles_processed': len(processed),
            'avg_token_loss_pct': round(sum(token_losses) / len(token_losses), 2) if token_losses else 0,
            'token_loss_distribution': {
                'min': round(min(token_losses), 2) if token_losses else 0,
                'max': round(max(token_losses), 2) if token_losses else 0,
                'median': round(sorted(token_losses)[len(token_losses)//2], 2) if token_losses else 0,
            }
        }
        
        logger.info(f"âœ… Preprocessing terminÃ©: {len(processed)} articles")
        return processed
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TRIQUES QUALITÃ‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_quality_metrics(self) -> Dict:
        """Retourner mÃ©triques qualitÃ©"""
        return self.quality_metrics
    
    def print_quality_report(self):
        """Afficher rapport qualitÃ©"""
        if not self.quality_metrics:
            logger.warning("Aucune mÃ©trique disponible")
            return
        
        print("\n" + "="*70)
        print("ğŸ“Š RAPPORT QUALITÃ‰ PRÃ‰TRAITEMENT")
        print("="*70)
        print(f"Articles traitÃ©s: {self.quality_metrics['num_articles_processed']}")
        print(f"Perte tokens moyenne: {self.quality_metrics['avg_token_loss_pct']}%")
        print(f"  Min: {self.quality_metrics['token_loss_distribution']['min']}%")
        print(f"  Max: {self.quality_metrics['token_loss_distribution']['max']}%")
        print(f"  MÃ©diane: {self.quality_metrics['token_loss_distribution']['median']}%")
        print("="*70)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # UTILITAIRES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @staticmethod
    def _load_french_stopwords() -> set:
        """Charger stopwords franÃ§ais"""
        # Stopwords franÃ§ais courants
        stopwords = {
            'le', 'la', 'les', 'de', 'des', 'du', 'un', 'une', 'des',
            'et', 'ou', 'mais', 'donc', 'car', 'ni', 'soit',
            'Ã ', 'au', 'aux', 'par', 'pour', 'avec', 'sans', 'sous',
            'dans', 'sur', 'entre', 'vers', 'chez', 'depuis', 'jusqu',
            'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils', 'elles',
            'moi', 'toi', 'lui', 'elle', 'nous', 'vous', 'eux',
            'suis', 'es', 'est', 'sommes', 'Ãªtes', 'sont',
            'ce', 'cela', 'celui', 'celle', 'ceux', 'celles',
            'que', 'qui', 'quoi', 'quel', 'quelle', 'quels', 'quelles',
            'oÃ¹', 'quand', 'comment', 'pourquoi', 'combien',
            'trÃ¨s', 'trop', 'plus', 'moins', 'aussi', 'bien', 'mal',
            'ne', 'pas', 'rien', 'jamais', 'toujours', 'encore',
            'peu', 'beaucoup', 'assez', 'tout', 'autre'
        }
        return stopwords

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN - TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import json
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(name)s | %(levelname)s | %(message)s'
    )
    
    # Charger config
    with open('config.json', 'r') as f:
        config = json.load(f)
    
    # CrÃ©er preprocessor
    preprocessor = TextPreprocessor(config)
    
    # Test sur texte simple
    test_text = """
    <p>ğŸš€ DÃ©couvrez BERT 2.0 : L'rÃ©volution du NLP en 2025 !
    URL: https://example.com/article
    Auteur: @john_doe | 25 commentaires
    
    C'est incroyable, vraiment ! âœ¨ La nouvelle API HuggingFace...</p>
    """
    
    result = preprocessor.process(test_text)
    
    print("\nğŸ“ TEST PREPROCESSING")
    print(f"Original ({len(result['original'])} chars): {result['original']}")
    print(f"Normalized ({len(result['normalized'])} chars): {result['normalized']}")
    print(f"Tokens ({result['num_tokens_final']} tokens): {result['tokens']}")
    print(f"Token loss: {result['token_loss_pct']}%")
