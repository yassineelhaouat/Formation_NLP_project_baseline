# src/report_generator.py
"""
ğŸ“„ Module GÃ©nÃ©ration Rapport

SynthÃ©tise articles classifiÃ©s en rapport professionnel

Sections :
1. Trending Topics
2. Must-Read Articles
3. Thematic Analysis
4. Sentiment Distribution
5. Resources by Level

Usage:
    generator = ReportGenerator(config)
    report = generator.generate(classified_articles)
    generator.save_report(report, 'output/report.txt')
"""

import logging
from typing import List, Dict
from datetime import datetime
from collections import Counter
import json

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REPORT GENERATOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ReportGenerator:
    """GÃ©nÃ¨re rapport de veille professionnel"""
    
    def __init__(self, config: Dict):
        self.config = config
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 1 : TRENDING TOPICS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @staticmethod
    def extract_trending_topics(articles: List[Dict], num_topics: int = 3) -> List[Dict]:
        """
        Extraire trending topics
        
        CritÃ¨res :
        - Nombre articles sur le sujet
        - Mentions dans les titres
        - Sentiment dominant
        """
        logger.info("ğŸ“ˆ Extraction trending topics...")
        
        # Compter par topic
        topic_counts = Counter()
        topic_articles = {}
        
        for article in articles:
            if not article.get('is_duplicate', False):  # Ignorer doublons
                topic = article.get('topic_prediction', 'Other')
                topic_counts[topic] += 1
                
                if topic not in topic_articles:
                    topic_articles[topic] = []
                topic_articles[topic].append(article)
        
        # Top N topics
        trending = []
        for topic, count in topic_counts.most_common(num_topics):
            articles_topic = topic_articles.get(topic, [])
            
            # Sentiment moyen
            sentiments = [a.get('sentiment_label', 'Neutre') for a in articles_topic]
            sentiment_counts = Counter(sentiments)
            dominant_sentiment = sentiment_counts.most_common(1)[0][0] if sentiment_counts else 'Neutre'
            
            trending.append({
                'topic': topic,
                'count': count,
                'articles_sample': [a.get('title', '')[:50] for a in articles_topic[:3]],
                'dominant_sentiment': dominant_sentiment
            })
        
        logger.info(f"âœ… {len(trending)} trending topics trouvÃ©s")
        return trending
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 2 : MUST-READ ARTICLES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @staticmethod
    def extract_must_read_articles(articles: List[Dict], num_articles: int = 5) -> List[Dict]:
        """
        Extraire articles must-read
        
        CritÃ¨res de ranking :
        - Confiance classification Ã©levÃ©e
        - Score sentiment distinctif (pas neutre)
        - Pas doublon
        """
        logger.info("âœ¨ Extraction must-read articles...")
        
        # Filtrer
        candidates = [a for a in articles if not a.get('is_duplicate', False)]
        
        # Score ranking
        scored = []
        for article in candidates:
            # Score confiance
            confidence = article.get('topic_confidence', 0.0)
            
            # Score sentiment (intÃ©ressant si pas neutre)
            sentiment = article.get('sentiment_label', 'Neutre')
            sentiment_score = 0.5 if sentiment == 'Neutre' else 1.0
            
            # Score global
            score = (confidence * 0.7) + (sentiment_score * 0.3)
            
            scored.append({
                'article': article,
                'score': score
            })
        
        # Top N
        must_read = [s['article'] for s in sorted(scored, key=lambda x: x['score'], reverse=True)[:num_articles]]
        
        logger.info(f"âœ… {len(must_read)} must-read articles sÃ©lectionnÃ©s")
        return must_read
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 3 : THEMATIC ANALYSIS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @staticmethod
    def thematic_analysis(articles: List[Dict]) -> Dict:
        """
        Analyser thÃ¨mes principaux
        
        Extrait keywords des articles via tokens
        """
        logger.info("ğŸ“Š Analyse thÃ©matique...")
        
        # Compter tokens
        all_tokens = []
        for article in articles:
            tokens = article.get('tokens', [])
            all_tokens.extend(tokens)
        
        # Top keywords
        token_counts = Counter(all_tokens)
        top_keywords = token_counts.most_common(15)
        
        # Distribution par topic
        topics_dist = Counter(a.get('topic_prediction', 'Other') for a in articles 
                              if not a.get('is_duplicate', False))
        
        return {
            'top_keywords': [{'keyword': k, 'count': c} for k, c in top_keywords],
            'topic_distribution': dict(topics_dist),
            'num_unique_tokens': len(token_counts)
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECTION 4 : SENTIMENT ANALYSIS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    @staticmethod
    def sentiment_analysis(articles: List[Dict]) -> Dict:
        """Analyser distribution sentiments"""
        logger.info("ğŸ˜Š Analyse sentiments...")
        
        sentiments = Counter(a.get('sentiment_label', 'Neutre') for a in articles 
                            if not a.get('is_duplicate', False))
        
        total = sum(sentiments.values()) or 1
        
        return {
            'distribution': {
                sentiment: {
                    'count': count,
                    'percentage': round((count / total) * 100, 1)
                }
                for sentiment, count in sentiments.items()
            },
            'dominant_sentiment': sentiments.most_common(1)[0][0] if sentiments else 'Neutre'
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GÃ‰NÃ‰RATION RAPPORT COMPLET
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def generate(self, articles: List[Dict]) -> str:
        """
        GÃ©nÃ©rer rapport complet
        
        Format :
        - Header avec mÃ©tadonnÃ©es
        - Trending topics
        - Must-read articles
        - Thematic analysis
        - Sentiment analysis
        - Resources by level
        """
        logger.info("ğŸ“ GÃ©nÃ©ration rapport...")
        
        # Extraire sections
        trending = self.extract_trending_topics(articles)
        must_read = self.extract_must_read_articles(articles)
        thematic = self.thematic_analysis(articles)
        sentiments = self.sentiment_analysis(articles)
        
        # Compter articles
        num_total = len(articles)
        num_unique = sum(1 for a in articles if not a.get('is_duplicate', False))
        
        # Construire rapport
        report = []
        
        # HEADER
        report.append("=" * 75)
        report.append("ğŸ“° VEILLE AUTOMATIQUE : NLP & Python")
        report.append(f"GÃ©nÃ©rÃ© le {datetime.now().strftime('%d %B %Y Ã  %H:%M')}")
        report.append("=" * 75)
        report.append("")
        
        # RÃ‰SUMÃ‰ EXÃ‰CUTIF
        report.append("ğŸ“Š RÃ‰SUMÃ‰ EXÃ‰CUTIF")
        report.append("-" * 75)
        report.append(f"Articles collectÃ©s : {num_total}")
        report.append(f"Articles uniques : {num_unique} (dÃ©dupli rate: {((num_total-num_unique)/max(num_total,1)*100):.1f}%)")
        report.append("")
        
        # TRENDING TOPICS
        report.append("ğŸ”¥ TRENDING TOPICS (Sujets du moment)")
        report.append("-" * 75)
        for i, trend in enumerate(trending, 1):
            report.append(f"\n{i}. {trend['topic'].upper()}")
            report.append(f"   Articles: {trend['count']}")
            report.append(f"   Sentiment dominant: {trend['dominant_sentiment']}")
            report.append(f"   Exemples:")
            for article_title in trend['articles_sample']:
                report.append(f"     â€¢ {article_title}")
        report.append("")
        
        # MUST-READ
        report.append("\nâœ¨ ARTICLES Ã€ NE PAS MANQUER")
        report.append("-" * 75)
        for i, article in enumerate(must_read, 1):
            report.append(f"\nğŸ“Œ {i}. {article.get('title', 'No Title')}")
            report.append(f"   Source: {article.get('source', 'Unknown')}")
            report.append(f"   URL: {article.get('url', 'N/A')}")
            report.append(f"   Niveau: {article.get('topic_prediction', 'Unknown')}")
            report.append(f"   Confiance: {article.get('topic_confidence', 0):.1%}")
            report.append(f"   Sentiment: {article.get('sentiment_label', 'Neutre')}")
        report.append("")
        
        # ANALYSE THÃ‰MATIQUE
        report.append("\nğŸ“Š ANALYSE THÃ‰MATIQUE")
        report.append("-" * 75)
        report.append(f"\nTop Keywords ({thematic['num_unique_tokens']} tokens uniques):")
        for keyword_data in thematic['top_keywords'][:10]:
            keyword = keyword_data['keyword']
            count = keyword_data['count']
            report.append(f"  â€¢ {keyword:20s} ({count} mentions)")
        
        report.append(f"\nDistribution par Niveau:")
        for topic, count in sorted(thematic['topic_distribution'].items(), key=lambda x: x[1], reverse=True):
            pct = (count / max(num_unique, 1)) * 100
            report.append(f"  â€¢ {topic:20s}: {count:3d} ({pct:5.1f}%)")
        report.append("")
        
        # SENTIMENT ANALYSIS
        report.append("\nğŸ˜Š ANALYSE SENTIMENTS")
        report.append("-" * 75)
        for sentiment, data in sentiments['distribution'].items():
            report.append(f"{sentiment:15s}: {data['count']:3d} articles ({data['percentage']:5.1f}%)")
        report.append("")
        
        # FOOTER
        report.append("\n" + "=" * 75)
        report.append("Fin du rapport")
        report.append("=" * 75)
        
        return "\n".join(report)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SAUVEGARDE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def save_report(self, report: str, filepath: str):
        """Sauvegarder rapport en fichier"""
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"âœ… Rapport sauvegardÃ©: {filepath}")
        except Exception as e:
            logger.error(f"âŒ Erreur sauvegarde: {str(e)}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN - TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import json
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(name)s | %(levelname)s | %(message)s'
    )
    
    # Charger config
    with open('config.json', 'r') as f:
        config = json.load(f)
    
    # CrÃ©er gÃ©nÃ©rateur
    generator = ReportGenerator(config)
    
    # Test articles
    test_articles = [
        {
            'title': 'BERT Tutorial',
            'content': 'Learn BERT...',
            'topic_prediction': 'Beginner',
            'topic_confidence': 0.95,
            'sentiment_label': 'Positif',
            'tokens': ['bert', 'tutorial', 'nlp'],
            'source': 'Medium',
            'url': 'https://example.com/bert',
            'is_duplicate': False
        },
        {
            'title': 'Advanced Fine-tuning',
            'content': 'Advanced techniques...',
            'topic_prediction': 'Advanced',
            'topic_confidence': 0.88,
            'sentiment_label': 'Neutre',
            'tokens': ['fine', 'tuning', 'llm'],
            'source': 'ArXiv',
            'url': 'https://example.com/advanced',
            'is_duplicate': False
        }
    ]
    
    # GÃ©nÃ©rer rapport
    report = generator.generate(test_articles)
    print(report)
    
    # Sauvegarder
    generator.save_report(report, 'test_report.txt')
