{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Exploratory Data Analysis - Veille NLP\n",
    "\n",
    "This notebook performs basic Exploratory Data Analysis on the collected and processed articles.\n",
    "\n",
    "**Purpose:** Understand data distribution, quality, and patterns\n",
    "\n",
    "**Steps:**\n",
    "1. Load articles (raw, processed, classified)\n",
    "2. Basic statistics\n",
    "3. Distribution analysis\n",
    "4. Quality assessment\n",
    "5. Identify improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw articles\n",
    "raw_articles = []\n",
    "try:\n",
    "    with open('../data/articles_raw.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            raw_articles.append(json.loads(line))\n",
    "    print(f\"‚úì Loaded {len(raw_articles)} raw articles\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå articles_raw.jsonl not found. Run: python main.py\")\n",
    "\n",
    "# Load processed articles\n",
    "processed_articles = []\n",
    "try:\n",
    "    with open('../data/articles_processed.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            processed_articles.append(json.loads(line))\n",
    "    print(f\"‚úì Loaded {len(processed_articles)} processed articles\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå articles_processed.jsonl not found\")\n",
    "\n",
    "# Load classified articles\n",
    "classified_articles = []\n",
    "try:\n",
    "    with open('../data/articles_classified.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            classified_articles.append(json.loads(line))\n",
    "    print(f\"‚úì Loaded {len(classified_articles)} classified articles\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå articles_classified.jsonl not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Raw Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_articles:\n",
    "    # Convert to DataFrame\n",
    "    df_raw = pd.DataFrame(raw_articles)\n",
    "    \n",
    "    print(\"üìä RAW DATA STATISTICS\")\n",
    "    print(f\"Total articles: {len(df_raw)}\")\n",
    "    print(f\"\\nColumns: {list(df_raw.columns)}\")\n",
    "    print(f\"\\nData types:\\n{df_raw.dtypes}\")\n",
    "    print(f\"\\nMissing values:\\n{df_raw.isnull().sum()}\")\n",
    "    \n",
    "    # Sources\n",
    "    source_counts = df_raw['source'].value_counts()\n",
    "    print(f\"\\nArticles by source:\\n{source_counts}\")\n",
    "    \n",
    "    # Content length\n",
    "    if 'content' in df_raw.columns:\n",
    "        df_raw['content_length'] = df_raw['content'].str.len()\n",
    "        print(f\"\\nContent length stats:\\n{df_raw['content_length'].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processed_articles:\n",
    "    df_processed = pd.DataFrame(processed_articles)\n",
    "    \n",
    "    print(\"üîß PREPROCESSING QUALITY\")\n",
    "    print(f\"Total articles: {len(df_processed)}\")\n",
    "    \n",
    "    # Token statistics\n",
    "    if 'num_tokens' in df_processed.columns:\n",
    "        print(f\"\\nTokens per article:\\n{df_processed['num_tokens'].describe()}\")\n",
    "        print(f\"\\nToken loss:\\n{df_processed['token_loss_pct'].describe()}\")\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        axes[0].hist(df_processed['num_tokens'], bins=30, color='steelblue', edgecolor='black')\n",
    "        axes[0].set_title('Distribution of Tokens per Article')\n",
    "        axes[0].set_xlabel('Number of Tokens')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        \n",
    "        axes[1].hist(df_processed['token_loss_pct'], bins=30, color='coral', edgecolor='black')\n",
    "        axes[1].set_title('Distribution of Token Loss %')\n",
    "        axes[1].set_xlabel('Token Loss %')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if classified_articles:\n",
    "    df_classified = pd.DataFrame(classified_articles)\n",
    "    \n",
    "    print(\"ü§ñ CLASSIFICATION RESULTS\")\n",
    "    print(f\"Total articles: {len(df_classified)}\")\n",
    "    \n",
    "    # Topics\n",
    "    if 'topic_prediction' in df_classified.columns:\n",
    "        topic_counts = df_classified['topic_prediction'].value_counts()\n",
    "        print(f\"\\nTopic distribution:\\n{topic_counts}\")\n",
    "        print(f\"\\nTopic confidence stats:\\n{df_classified['topic_confidence'].describe()}\")\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        topic_counts.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "        axes[0].set_title('Topics Distribution')\n",
    "        axes[0].set_xlabel('Topic')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        axes[1].hist(df_classified['topic_confidence'], bins=20, color='coral', edgecolor='black')\n",
    "        axes[1].set_title('Topic Confidence Distribution')\n",
    "        axes[1].set_xlabel('Confidence')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Sentiments\n",
    "    if 'sentiment_label' in df_classified.columns:\n",
    "        sentiment_counts = df_classified['sentiment_label'].value_counts()\n",
    "        print(f\"\\nSentiment distribution:\\n{sentiment_counts}\")\n",
    "        \n",
    "        # Visualize\n",
    "        sentiment_counts.plot(kind='pie', autopct='%1.1f%%', figsize=(8, 6))\n",
    "        plt.title('Sentiment Distribution')\n",
    "        plt.ylabel('')\n",
    "        plt.show()\n",
    "    \n",
    "    # Duplicates\n",
    "    if 'is_duplicate' in df_classified.columns:\n",
    "        num_duplicates = df_classified['is_duplicate'].sum()\n",
    "        print(f\"\\nDuplicates: {num_duplicates} ({100*num_duplicates/len(df_classified):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if classified_articles:\n",
    "    print(\"üìÑ SAMPLE ARTICLES\")\n",
    "    \n",
    "    # Show 3 random articles with all details\n",
    "    import random\n",
    "    sample = random.sample(classified_articles, min(3, len(classified_articles)))\n",
    "    \n",
    "    for i, article in enumerate(sample, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Article {i}:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Title: {article.get('title', 'N/A')[:60]}...\")\n",
    "        print(f\"Source: {article.get('source', 'N/A')}\")\n",
    "        print(f\"Topic: {article.get('topic_prediction', 'N/A')} (confidence: {article.get('topic_confidence', 'N/A')})\")\n",
    "        print(f\"Sentiment: {article.get('sentiment_label', 'N/A')}\")\n",
    "        print(f\"Duplicate: {article.get('is_duplicate', False)}\")\n",
    "        print(f\"Tokens: {article.get('num_tokens', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations for Improvement\n",
    "\n",
    "Based on the EDA, here are potential improvements:\n",
    "\n",
    "### Collecte\n",
    "- [ ] Add more sources (GitHub, Medium, Twitter)\n",
    "- [ ] Implement caching to avoid re-scraping\n",
    "- [ ] Add retry logic for timeout handling\n",
    "\n",
    "### Preprocessing\n",
    "- [ ] Compare stemming vs lemmatization\n",
    "- [ ] Test impact of removing accents\n",
    "- [ ] Analyze token loss distribution\n",
    "\n",
    "### Classification\n",
    "- [ ] Fine-tune classifier on 50+ annotated examples\n",
    "- [ ] Implement custom NER for specific technologies\n",
    "- [ ] Calculate precision/recall/F1 metrics\n",
    "\n",
    "### Report\n",
    "- [ ] Add wordcloud visualization\n",
    "- [ ] Create trend analysis (week-over-week)\n",
    "- [ ] Export to markdown/JSON format\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
